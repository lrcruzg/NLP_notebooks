{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4c46f4c",
   "metadata": {},
   "source": [
    "###  Luis Ricardo Cruz Garc칤a\n",
    "#### Procesamiento de Lenguaje Natural\n",
    "\n",
    "#### Tarea 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61805f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import nltk\n",
    "import glob\n",
    "import numpy as np\n",
    "from decimal import Decimal\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from typing import Optional\n",
    "from sklearn.preprocessing import normalize\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LiaSqNfTAQNa"
   },
   "outputs": [],
   "source": [
    "def get_texts_from_file(path_corpus: str, path_label: str) -> tuple[list, list]:\n",
    "    \"\"\"Given the corpus and label paths, returns the list of docs and labels.\"\"\"\n",
    "    docs, labels = [], []\n",
    "\n",
    "    with open(path_corpus, \"r\") as f_corpus:\n",
    "        for doc in f_corpus:\n",
    "            docs.append(doc)\n",
    "\n",
    "    with open(path_label, \"r\") as f_labels:\n",
    "        for label in f_labels:\n",
    "            labels.append(label)\n",
    "\n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"Vocabulary class to store the ranking of the words using the \n",
    "    given corpus frequency distribution.\"\"\"\n",
    "    def __init__(self, \n",
    "                 corpus_freqdist: nltk.FreqDist, \n",
    "                 n_words: Optional[int] = None):\n",
    "        vocabulary_freq_desc = self._sort_FreqDist(corpus_freqdist)\n",
    "        \n",
    "        if n_words is not None: # restrict the max num of words\n",
    "            vocabulary_freq_desc = vocabulary_freq_desc[:n_words]\n",
    "\n",
    "        self.vocabulary = [word for word, freq in vocabulary_freq_desc]\n",
    "        \n",
    "        # dictionary of the rank (frequency) of words in the vocabulary, word: freq_ranking\n",
    "        self.word_to_index = {word:rank for rank, word in enumerate(self.vocabulary)}\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sort_FreqDist(fd: nltk.FreqDist) -> list:\n",
    "        \"\"\"Return the list of items (pairs of <word, freq>) sorted by frequency (desc).\"\"\"\n",
    "        aux = list(fd.items())\n",
    "        aux.sort(key=lambda x: x[1], reverse=True)\n",
    "        return aux\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocabulary)\n",
    "    \n",
    "    def __getitem__(self, key: str | int) -> int | str | None:\n",
    "        \"\"\"Depending of the type of the key, returns the word at \n",
    "        index key (if key is an integer) or the rank of the \n",
    "        word key (if key is a string).\n",
    "        \"\"\"\n",
    "        if not isinstance(key, int) and not isinstance(key, np.int64) and not isinstance(key, str):\n",
    "            raise ValueError(f\"Key must be an integer or a string, key = {key}\")\n",
    "        \n",
    "        if isinstance(key, int) or isinstance(key, np.int64):\n",
    "            return self.vocabulary[key]\n",
    "\n",
    "        if isinstance(key, str):\n",
    "            return self.word_to_index[key]\n",
    "\n",
    "    def __contains__(self, key: str) -> bool:\n",
    "        return key in self.word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus_Ngram:\n",
    "    \"\"\"Corpus class to preprocess and store the frequecy and vocabulary for a given list of documents.\"\"\"\n",
    "    def __init__(self, \n",
    "                 docs: list[str], \n",
    "                 n: int = 1, \n",
    "                 start_end_token: bool = False, \n",
    "                 n_words: Optional[int] = None):\n",
    "        self.n = n\n",
    "        self.docs = deepcopy(docs)\n",
    "        self.docs = [doc.lower() for doc in self.docs] # set to lowercase\n",
    "\n",
    "        # add start(<s>) and end(</s>) special tokens to each doc\n",
    "        if start_end_token:\n",
    "            self.docs = [\"<s>\" + doc + \"</s>\" for doc in self.docs]\n",
    "\n",
    "        tokenizer = nltk.TweetTokenizer()\n",
    "\n",
    "        self.tokens = []\n",
    "        for doc in self.docs:\n",
    "            self.tokens += tokenizer.tokenize(doc)\n",
    "\n",
    "        if n != 1:\n",
    "            self.tokens = nltk.ngrams(self.tokens, n)\n",
    "\n",
    "        self.freq = nltk.FreqDist(self.tokens)\n",
    "\n",
    "        self.vocabulary = Vocabulary(self.freq, n_words=n_words)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.docs)\n",
    "    \n",
    "    def __getitem__(self, key: int) -> str:\n",
    "        return self.docs[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(Corpus_Ngram):\n",
    "    \"\"\"Unigram corpus.\"\"\"\n",
    "    def __init__(self, \n",
    "                 docs: list[str], \n",
    "                 start_end_token: bool = False, \n",
    "                 n_words: Optional[int] = None):\n",
    "        super(Corpus, self).__init__(docs, n=1, start_end_token=start_end_token, n_words=n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f99a1c",
   "metadata": {},
   "source": [
    "### 2 Modelo de Lenguaje y Evaluaci칩n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27c5ad4",
   "metadata": {},
   "source": [
    "2.1. Preprocese todos los tuits de agresividad (positivos y negativos) seg칰n su intuici칩n para construir un buen corpus para un modelo de lenguaje (e.g., solo palabras en min칰scula, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a098980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training docs and labels\n",
    "train_docs, train_labels = get_texts_from_file(\"../../Data/mex_train.txt\", \"../../Data/mex_train_labels.txt\")\n",
    "train_labels = list(map(int, train_labels))  # cast to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_full_train = Corpus(train_docs, start_end_token=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e733882",
   "metadata": {},
   "source": [
    "Agregue tokens especiales de \"< s >\" y \"</ s >\" seg칰n usted considere (e.g., al inicio y final de cada tuit). Defina su vocabulario y enmascare con <unk> toda palabra que no est칠 en su vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8061548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_OOV_words(corpus: Corpus, \n",
    "                   n_terms: Optional[int]) -> list[str]:\n",
    "        \"\"\"Mask the words out of vocabulary (with an optional restriction on the \n",
    "        number of terms n_terms) with the \"<unk>\" token. \n",
    "        Returns the docs masked.\n",
    "        \"\"\"\n",
    "        docs = deepcopy(corpus.docs)\n",
    "        tokenizer = nltk.TweetTokenizer()\n",
    "\n",
    "        if n_terms is None: n_terms = len(corpus.vocabulary)\n",
    "\n",
    "        for i, doc in enumerate(docs):\n",
    "            doc_masked_words = []\n",
    "            for word in tokenizer.tokenize(doc):\n",
    "                if word in corpus.vocabulary and corpus.vocabulary[word] < n_terms:\n",
    "                    doc_masked_words.append(word)\n",
    "                else:\n",
    "                    doc_masked_words.append(\"<unk>\")\n",
    "            docs[i] = \" \".join(doc_masked_words)\n",
    "\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54bf847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask out of vocabulary words (restricted to the 5k most occurring words) of the train_docs\n",
    "train_docs = mask_OOV_words(corpus_full_train, n_terms=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c35a777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition of train_docs\n",
    "\n",
    "# train   -> 80% of train_docs\n",
    "# heldout -> 10% of train_docs\n",
    "# test    -> 10% of train_docs\n",
    "\n",
    "train   = train_docs[:4435]\n",
    "heldout = train_docs[4435:4989]\n",
    "test    = train_docs[4989:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13267d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct the train corpus from \"train\" (now train is smaller, \n",
    "# with start/end token and masked)\n",
    "corpus_train = Corpus(train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "878f9add",
   "metadata": {},
   "source": [
    "### 2.2. Entrene tres modelos de lenguaje sobre todos los tuits: $洧녞_{洧녹洧녵洧녰洧녮洧洧녩洧녴洧녩洧맣(洧녻_{1:洧녵})$, $洧녞_{洧녪洧녰洧녮洧洧녩洧녴洧녩洧맣(洧녻_{1:洧녵} )$, $洧녞_{洧노洧洧녰洧녮洧洧녩洧녴洧녩洧맣(洧녻_{1:洧녵})$. Para cada uno proporcione una interfaz (funci칩n) sencilla. Los modelos deben tener una estrategia com칰n para lidiar con secuencias no vistas. Puede optar por un suavizamiento Laplace o un Good-Turing discounting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39d06e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> puta madre quiero <unk> por la hdp de la <unk> </s>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose a tweet to apply the models\n",
    "test_tweet = train[19]\n",
    "test_tweet_tokenized = tokenizer.tokenize(test_tweet)\n",
    "test_tweet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos las estimaciones de una sucesi칩n de palabras usando un modelo de $n$-grams con Laplace smoothing de la siguiente manera.\n",
    "\n",
    "$$P_L(w_{1:k}) = \\prod_i P_L(w_i | w_{i-n:i-1})  = \\prod_i \\frac{C(w_{i-n:i}) + 1}{C(w_{i-n:i-1}) + V}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en el caso de no se quiera el \"suavizado\", entonces la f칩rmula se reduce a \n",
    "\n",
    "$$P(w_{1:k}) = \\prod_i P(w_i | w_{i-n:i-1})  = \\prod_i \\frac{C(w_{i-n:i})}{C(w_{i-n:i-1})}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ngram_Model:\n",
    "    def __init__(self, corpus: Corpus, n: int):\n",
    "        if corpus.n != 1:\n",
    "            raise ValueError(\"The corpus must be a unigram corpus.\")\n",
    "        \n",
    "        self.n = n\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        # create n-gram corpus for n and n-1\n",
    "        if self.n == 1:\n",
    "            self.corpus_n = self.corpus\n",
    "            self.corpus_n_minus_1 = self.corpus\n",
    "        elif self.n == 2:\n",
    "            self.corpus_n = Corpus_Ngram(self.corpus.docs, self.n)\n",
    "            self.corpus_n_minus_1 = self.corpus\n",
    "        else:\n",
    "            self.corpus_n = Corpus_Ngram(self.corpus.docs, self.n)\n",
    "            self.corpus_n_minus_1 = Corpus_Ngram(self.corpus.docs, self.n - 1)\n",
    "\n",
    "    def conditional_probability(self, \n",
    "                                word: str, \n",
    "                                prev_words: tuple[str], \n",
    "                                smoothing: bool = True) -> Decimal:\n",
    "        \"\"\"Computes the estimated conditional probability of a word given self.n - 1 previous words.\n",
    "\n",
    "            P(w_i | w_{i-n:i-1}) = C(w_{i-n:i} / C(w_{i-n:i-1}) \n",
    "\n",
    "        if smoothing is enabled, then it will use Laplace smoothing.\n",
    "\n",
    "            P_L(w_i | w_{i-n:i-1}) = (C(w_{i-n:i} + 1) / (C(w_{i-n:i-1}) + V) \n",
    "\n",
    "        where C(w_{i-n:i}) is the number occurrences of word + prev_words in the corpus and \n",
    "        C(w_{i-n:i-1}) is the number of occurrences of prev_words in the corpus.\n",
    "        \"\"\"\t\n",
    "        if not isinstance(prev_words, tuple):\n",
    "            raise ValueError(f\"The prev_words should be a tuple of strings.\")\n",
    "\n",
    "        if len(prev_words) != self.n - 1:\n",
    "            raise ValueError(f\"The tuple prev_words size should be self.n - 1. {len(prev_words) = }\")\n",
    "\n",
    "        if smoothing:\n",
    "            if self.n == 1:\n",
    "                P_L_w_given_prev = Decimal((self.corpus.freq[word] + 1) / (len(self.corpus.tokens) + len(self.corpus.vocabulary)))\n",
    "            elif self.n == 2:\n",
    "                P_L_w_given_prev = Decimal((self.corpus_n.freq[(*prev_words, word)] + 1) / (self.corpus_n_minus_1.freq[prev_words[0]] + len(self.corpus.vocabulary)))\n",
    "            else:\n",
    "                P_L_w_given_prev = Decimal((self.corpus_n.freq[(*prev_words, word)] + 1) / (self.corpus_n_minus_1.freq[prev_words] + len(self.corpus.vocabulary)))\n",
    "        else:\n",
    "            if self.n == 1:\n",
    "                P_L_w_given_prev = Decimal(self.corpus.freq[word] / len(self.corpus.tokens))\n",
    "            elif self.n == 2:\n",
    "                P_L_w_given_prev = Decimal(self.corpus_n.freq[(*prev_words, word)] / self.corpus_n_minus_1.freq[prev_words[0]])\n",
    "            else:\n",
    "                P_L_w_given_prev = Decimal(self.corpus_n.freq[(*prev_words, word)] / self.corpus_n_minus_1.freq[prev_words])\n",
    "\n",
    "        return P_L_w_given_prev\n",
    "\n",
    "    def probability(self, \n",
    "                    seq_words: list[str], \n",
    "                    smoothing: bool = True) -> Decimal:\n",
    "        \"\"\"Computes the estimate probability of sequence of words (i.e. tokenized doc) \n",
    "        using an n-gram model.\n",
    "\n",
    "            P(w_{1:k}) = \\prod_i P(w_i | w_{i-n:i-1})\n",
    "\n",
    "        If smoothing smoothing is enabled, then the probabilities will be computed with Laplace smoothing.\n",
    "            \n",
    "            P_L(w_{1:k}) = \\prod_i P_L(w_i | w_{i-n:i-1})\n",
    "        \n",
    "        where k = len(seq_words) - self.n and P_L(w_i | w_{i-n:i-1}) is estimated \n",
    "        with n-grams (see conditional_probability function).\n",
    "        \"\"\"\n",
    "        log_prob = Decimal(0.)\n",
    "        for ngram in list(nltk.ngrams(seq_words, self.n)):\n",
    "            P_L_w_given_prev = self.conditional_probability(ngram[-1], ngram[:-1], smoothing=smoothing)\n",
    "\n",
    "            # log_prob += ln( P_L(w_i | w_{i-n:i-1}) )\n",
    "            log_prob += P_L_w_given_prev.ln()\n",
    "\n",
    "        return log_prob.exp()\n",
    "\n",
    "    def perplexity(self, seq_words: list[str]):\n",
    "        return (1 / self.probability(seq_words)) ** (1 / Decimal(len(seq_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36e899",
   "metadata": {},
   "source": [
    "#### Modelo de unigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_model_train = Ngram_Model(corpus_train, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_1(test_tweet) = 5.004098532124699262146308101E-23\n"
     ]
    }
   ],
   "source": [
    "print(f\"P_1(test_tweet) = {unigram_model_train.probability(test_tweet_tokenized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04181764",
   "metadata": {},
   "source": [
    "#### Modelo de bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model_train = Ngram_Model(corpus_train, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_2(test_tweet) = 1.679099977175348530204536306E-25\n"
     ]
    }
   ],
   "source": [
    "print(f\"P_2(test_tweet) = {bigram_model_train.probability(test_tweet_tokenized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54ef7a1",
   "metadata": {},
   "source": [
    "#### Modelo Trigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_model_train = Ngram_Model(corpus_train, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_3(test_tweet) = 7.525659602589744277693384578E-31\n"
     ]
    }
   ],
   "source": [
    "print(f\"P_3(test_tweet) = {trigram_model_train.probability(test_tweet_tokenized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f6f7ee",
   "metadata": {},
   "source": [
    "Muestre un par de ejemplos de c칩mo funciona, al menos uno con una palabra\n",
    "fuera del vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67361068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_train.freq[\"topolog칤a\"]  # palabra OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc00092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hagamos un tweet (una oraci칩n con los tokens de inicio y fin) cuya 칰nica palabra sea \"topolog칤a\"\n",
    "tweet_oov = \"<s> topolog칤a </s>\"\n",
    "\n",
    "# lo tokenizo para enviarlo a los m칠todos de cada modelo\n",
    "tweet_oov_tokenized = tokenizer.tokenize(tweet_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_1(tweet_oov) = 2.305041643895685496422031688E-8\n"
     ]
    }
   ],
   "source": [
    "print(f\"P_1(tweet_oov) = {unigram_model_train.probability(tweet_oov_tokenized)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c25e1304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_2(tweet_oov) = 2.195755753439991671758321113E-8\n"
     ]
    }
   ],
   "source": [
    "print(f\"P_2(tweet_oov) = {bigram_model_train.probability(tweet_oov_tokenized)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcdeccfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_3(tweet_oov) = 0.0002046663937781416267146922250\n"
     ]
    }
   ],
   "source": [
    "print(f\"P_3(tweet_oov) = {trigram_model_train.probability(tweet_oov_tokenized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866447b",
   "metadata": {},
   "source": [
    "#### 2.3 Construya un modelo interpolado con valores $\\lambda$ fijos:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some $\\lambda = \\{ \\lambda_0, \\lambda_1, \\lambda_2 \\}$ such that $\\lambda_0 + \\lambda_1 + \\lambda_2 = 1$, the interpolated model using unigrams, bigrams and trigrams is given by\n",
    "\n",
    "$$\\hat{P}(w_{1:k}) = \\prod_i \\left( \\lambda_2 P_L(w_i | w_{i-2}w{i-1}) + \\lambda_1 P_L(w_i | w{i-1}) + \\lambda_0 P_L(w_i) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02bfec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interpolated_Model:\n",
    "    def __init__(self, \n",
    "                 corpus: Corpus):\n",
    "        self.corpus = corpus\n",
    "        self.unigram_model = Ngram_Model(corpus, n=1)\n",
    "        self.bigram_model  = Ngram_Model(corpus, n=2)\n",
    "        self.trigram_model = Ngram_Model(corpus, n=3)\n",
    "    \n",
    "    def conditional_probability(self, \n",
    "                                word: str, \n",
    "                                prev_words: tuple[str], \n",
    "                                lambda_values: list[float]) -> Decimal:\n",
    "        \"\"\"Computes the estimated conditional probability of a word given 2 previous words.\n",
    "\n",
    "            P(w_i | w_{i-2:i-1}) = \\lambda_2 P_L(w_i | w_{i-2}w{i-1}) + \\lambda_1 P_L(w_i | w{i-1}) + \\lambda_0 P_L(w_i)\n",
    "        \"\"\"\n",
    "        if not isinstance(prev_words, tuple):\n",
    "            raise ValueError(f\"The prev_words should be a tuple of strings.\")\n",
    "\n",
    "        if len(prev_words) != 2:\n",
    "            raise ValueError(f\"The tuple prev_words size should be 2. {len(prev_words) = }\")\n",
    "\n",
    "        P_L_w_given_prev = (\n",
    "                (self.unigram_model.conditional_probability(word, ()) * Decimal(lambda_values[0]))\n",
    "                + (self.bigram_model.conditional_probability(word, prev_words[:1]) * Decimal(lambda_values[1]))\n",
    "                + (self.trigram_model.conditional_probability(word, prev_words[:2]) * Decimal(lambda_values[2]))\n",
    "        )\n",
    "\n",
    "        return P_L_w_given_prev\n",
    "\n",
    "    def probability(self, \n",
    "                    seq_words: list[str], \n",
    "                    lambda_values: list[float]) -> Decimal:\n",
    "        \"\"\"Computes the estimate probability of sequence of words (i.e. tokenized doc) \n",
    "        using an interpolated trigram model.\n",
    "\n",
    "            P(w_{1:k}) = \\prod_i P(w_i | w_{i-2:i-1})\n",
    "        \"\"\"\n",
    "        log_prob = Decimal(0.)\n",
    "        for trigram in list(nltk.trigrams(seq_words)):\n",
    "            P_L_w_given_prev = self.conditional_probability(trigram[-1], trigram[:-1], lambda_values)\n",
    "\n",
    "            log_prob += P_L_w_given_prev.ln()\n",
    "\n",
    "        return log_prob.exp()\n",
    "\n",
    "    def perplexity(self, seq_words: list[str], lambda_values: list[float]) -> Decimal:\n",
    "        return (1 / self.probability(seq_words, lambda_values)) ** (1 / Decimal(len(seq_words)))\n",
    "    \n",
    "    def generate(self, lambda_values: list[float], max_len: int = 50):\n",
    "        \"\"\"Generate text using the interpolated model and the lambda_values.\n",
    "        \"\"\"\n",
    "        if len(lambda_values) != 3:\n",
    "            raise ValueError('The model needs 3 lambda values.')\n",
    "\n",
    "        generated_tokens = [\"<s>\"]\n",
    "        prev_prev_word, prev_word = \"<s>\", \"<s>\" # initial previous words\n",
    "\n",
    "        # generates tokens until the end of sentence token is generated or the length is 50 tokens\n",
    "        while prev_word != \"</s>\" and len(generated_tokens) < max_len:\n",
    "            # generates the probabilities of the word given prev_prev_word and prev_word\n",
    "            model_weights = [\n",
    "                float(self.conditional_probability(word, (prev_prev_word, prev_word), lambda_values)) for word in self.corpus.vocabulary\n",
    "            ]\n",
    "\n",
    "            # choose a word in the vocabulary (using the weights to bias the choice to the words with greater probability)\n",
    "            new_word = random.choices(self.corpus.vocabulary, weights=model_weights)[0]\n",
    "\n",
    "            if new_word == \"<s>\": continue\n",
    "\n",
    "            generated_tokens.append(new_word)\n",
    "\n",
    "            # updata previous words\n",
    "            prev_prev_word = prev_word\n",
    "            prev_word      = new_word\n",
    "\n",
    "        return \" \".join(generated_tokens)\n",
    "    \n",
    "    def Expectation_Maximization(self, seq_words: list[str], n_iterations: int = 5):\n",
    "        \"\"\"Expectation-maximization algorithm for interpolated language modeling. \n",
    "        Learns the lambda parameters for the interpolated language model.\n",
    "        See Jacob Eisenstein's book \"Natural Laguage Processing\". \"\"\"\n",
    "        n_max = 3\n",
    "        M = len(seq_words)\n",
    "        \n",
    "        trigrams_seq_words = list(nltk.trigrams(seq_words))\n",
    "\n",
    "        lambda_z = np.array([1/3, 1/3, 1/3], dtype=float)\n",
    "        q = np.zeros(shape=(M - 2, n_max), dtype=float)\n",
    "\n",
    "        print(f'\\tInitial lambdas = {lambda_z}')\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        for it in range(n_iterations):\n",
    "            for m, (w2, w1, w) in enumerate(trigrams_seq_words):\n",
    "                q[m][0] = lambda_z[0] * float(self.unigram_model.conditional_probability(w, ()))\n",
    "                q[m][1] = lambda_z[1] * float(self.bigram_model.conditional_probability(w, (w1,)))\n",
    "                q[m][2] = lambda_z[2] * float(self.trigram_model.conditional_probability(w, (w2, w1)))\n",
    "\n",
    "                q[m] =  q[m] * np.divide(1, np.sum(np.abs(q[m]))) # normalize (l1 norm) the m-th row\n",
    "\n",
    "            lambda_z = (1 / M) * np.sum(q, axis=0)\n",
    "\n",
    "            print(f'i = {it}: lambdas = {lambda_z}, ', end=\"\")\n",
    "            print(f\"PP(heldout) = {float(self.perplexity(seq_words, lambda_z)):.2f}\")\n",
    "\n",
    "        return lambda_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07e6eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_lambdas = [\n",
    "    [0.33, 0.33, 0.33], \n",
    "    [0.4, 0.4, 0.2], \n",
    "    [0.2, 0.4, 0.4], \n",
    "    [0.5, 0.4, 0.1], \n",
    "    [0.1, 0.4, 0.5]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1f2b794",
   "metadata": {},
   "outputs": [],
   "source": [
    "heldout_tokenized = tokenizer.tokenize(\" \".join(heldout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993aa634",
   "metadata": {},
   "source": [
    "Muestre c칩mo bajan o suben las perplejidades en el held-out, finalmente pruebe una vez en test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_model_train = Interpolated_Model(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24a7b8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33, 0.33, 0.33]: PP(heldout) = 511.67\n",
      "[0.4, 0.4, 0.2]: PP(heldout) = 472.52\n",
      "[0.2, 0.4, 0.4]: PP(heldout) = 626.09\n",
      "[0.5, 0.4, 0.1]: PP(heldout) = 434.30\n",
      "[0.1, 0.4, 0.5]: PP(heldout) = 813.17\n"
     ]
    }
   ],
   "source": [
    "for lambda_vals in possible_lambdas:\n",
    "    print(f\"{lambda_vals}: PP(heldout) = {float(interpolated_model_train.perplexity(heldout_tokenized, lambda_vals)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba98f36",
   "metadata": {},
   "source": [
    "De las perplejidades calculadas para las $\\lambda$ dadas, las que se desempe침aron mejor (pues la perplejidad obtenida en el Held-out fue la m칤nima) fueron $$\\lambda = [0.5, 0.4, 0.1]$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54b4bcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PP(test) = 410.5147955904772932987539322\n"
     ]
    }
   ],
   "source": [
    "# al evaluar en el test usando los valores lambda = [0.5, 0.4, 0.1] obtenemos la siguiente perplejidad\n",
    "test_tokenized = tokenizer.tokenize(\" \".join(test))\n",
    "print(f\"PP(test) = {interpolated_model_train.perplexity(test_tokenized, [0.5, 0.4, 0.1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8c257",
   "metadata": {},
   "source": [
    "### 3. Generaci칩n de Texto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7de5936c",
   "metadata": {},
   "source": [
    "#### 3.1. Proponga una estrategia con base en **Expectation Maximization** para encontrar buenos valores de interpolaci칩n en $\\hat{洧녞}$ usando todo el dataset de agresividad. Para ello experimente con el modelo en particiones de $80\\%$, $10\\%$ y $10\\%$ para entrenar (train), ajustar par치metros (val) y probar (test) respectivamente. Muestre como bajan las perplejidades en $5$ iteraciones que usted elija (de todas las que sean necesarias de acuerdo a su EM) en validaci칩n, y pruebe una vez en test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25971ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tInitial lambdas = [0.33333333 0.33333333 0.33333333]\n",
      "--------------------------------------------------------------------------------\n",
      "i = 0: lambdas = [0.51176631 0.33576714 0.15228981], PP(heldout) = 426.90\n",
      "i = 1: lambdas = [0.59594392 0.32821656 0.07566278], PP(heldout) = 404.63\n",
      "i = 2: lambdas = [0.63334677 0.32619773 0.04027876], PP(heldout) = 397.15\n",
      "i = 3: lambdas = [0.64961421 0.32794002 0.02226902], PP(heldout) = 394.54\n",
      "i = 4: lambdas = [0.65622659 0.33106038 0.01253629], PP(heldout) = 393.77\n"
     ]
    }
   ],
   "source": [
    "lambda_values_EM = interpolated_model_train.Expectation_Maximization(heldout_tokenized, n_iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65622659 0.33106038 0.01253629]\n"
     ]
    }
   ],
   "source": [
    "print(lambda_values_EM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68750f1b",
   "metadata": {},
   "source": [
    "3.2. Haga una funci칩n \"tuitear\" con base en su modelo de lenguaje $\\hat{洧녞}$ del 칰ltimo punto. El modelo deber치 poder parar autom치ticamente cuando genere el s칤mbolo de terminaci칩n de tuit al final (e.g., \"<\\/s>\"), o $50$ palabras. Proponga algo para que en los 칰ltimos tokens sea m치s probable generar el token \"<\\/s>\". Muestre al menos cinco ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> * vamos madre ptm educados contenido 游땥 tus de 30 </s>'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweet function\n",
    "interpolated_model_train.generate(list(lambda_values_EM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c99995",
   "metadata": {},
   "source": [
    "3.3. Use la intuici칩n que ha ganado en esta tarea y los datos de las ma침aneras para entrenar un modelo de lenguaje ``` AMLO``` . Haga una funci칩n ``` dar_conferencia()```. Genere un discurso de $300$ palabras y detenga al modelo de forma abrupta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2224a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "conferences = []\n",
    "\n",
    "for file in glob.glob(\"../../Data/estenograficas_limpias/*\"):\n",
    "\twith open(file, \"r\") as f_corpus:\n",
    "\t\tfor sentence in f_corpus:\n",
    "\t\t\tconferences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_conferences = Corpus(conferences, start_end_token=True)\n",
    "conferences_masked = mask_OOV_words(corpus_conferences, n_terms=5000)\n",
    "corpus_conferences = Corpus(conferences_masked) # corpus with masked conferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMLO = Interpolated_Model(corpus_conferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4cf021f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> asegurar abandonar social el papel 1췈 hoy corruptos acab칩 tomaron dec칤a estar칤an culiac치n movimientos fuera , ven fronteriza consultar'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AMLO.generate([0.1, 0.3, 0.6], max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc06f47b",
   "metadata": {},
   "source": [
    "3.4. Calcule el estimado de cada uno sus modelos de lenguaje (el de tuits y el de amlo) para las frases: \"si no gano me voy a la chingada\", \"ya se va a acabar la corrupci칩n\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c613f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = \"si no gano me voy a la chingada\"\n",
    "sentence_2 = \"ya se va a acabar la corrupci칩n\"\n",
    "\n",
    "sentence_1_tokenized = tokenizer.tokenize(sentence_1)\n",
    "sentence_2_tokenized = tokenizer.tokenize(sentence_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b578d1",
   "metadata": {},
   "source": [
    "Con los modelos de los tweets agresivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4885048b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_unigram(sentence_1) = 1.422737445556451821090663411E-20\n",
      "P_bigram(sentence_1)  = 8.717001740778747181481628152E-18\n",
      "P_trigram(sentence_1) = 3.466901356069048238502135543E-20\n"
     ]
    }
   ],
   "source": [
    "# evaluamos a sentence_1 = \"si no gano me voy a la chingada\"\n",
    "print(f\"P_unigram(sentence_1) = {unigram_model_train.probability(sentence_1_tokenized)}\")\n",
    "print(f\"P_bigram(sentence_1)  = {bigram_model_train.probability(sentence_1_tokenized)}\")\n",
    "print(f\"P_trigram(sentence_1) = {trigram_model_train.probability(sentence_1_tokenized)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84661de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_unigram(sentence_2) = 1.460573663588147904551449248E-19\n",
      "P_bigram(sentence_2)  = 7.270212752785908748956856244E-18\n",
      "P_trigram(sentence_2) = 1.404911774707322600534679050E-17\n"
     ]
    }
   ],
   "source": [
    "# evaluamos a sentence_2 = \"ya se va a acabar la corrupci칩n\"\n",
    "print(f\"P_unigram(sentence_2) = {unigram_model_train.probability(sentence_2_tokenized)}\")\n",
    "print(f\"P_bigram(sentence_2)  = {bigram_model_train.probability(sentence_2_tokenized)}\")\n",
    "print(f\"P_trigram(sentence_2) = {trigram_model_train.probability(sentence_2_tokenized)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b10d91a",
   "metadata": {},
   "source": [
    "Con los modelos de las conferencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bca4be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_model_conferences = Ngram_Model(corpus_conferences, n=1)\n",
    "bigram_model_conferences = Ngram_Model(corpus_conferences, n=2)\n",
    "trigram_model_conferences = Ngram_Model(corpus_conferences, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_unigram(sentence_1) = 4.532466486548684280014766525E-28\n",
      "P_bigram(sentence_1)  = 1.129882724757320311605002232E-18\n",
      "P_trigram(sentence_1) = 6.558164955988236444108646296E-20\n"
     ]
    }
   ],
   "source": [
    "# evaluamos a sentence_1 = \"si no gano me voy a la chingada\"\n",
    "print(f\"P_unigram(sentence_1) = {unigram_model_conferences.probability(sentence_1_tokenized)}\")\n",
    "print(f\"P_bigram(sentence_1)  = {bigram_model_conferences.probability(sentence_1_tokenized)}\")\n",
    "print(f\"P_trigram(sentence_1) = {trigram_model_conferences.probability(sentence_1_tokenized)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_unigram(sentence_2) = 7.950891199318519215067285481E-18\n",
      "P_bigram(sentence_2)  = 6.235694567159202024259164979E-10\n",
      "P_trigram(sentence_2) = 4.095999863770872298837284021E-9\n"
     ]
    }
   ],
   "source": [
    "# evaluamos a sentence_2 = \"ya se va a acabar la corrupci칩n\"\n",
    "print(f\"P_unigram(sentence_2) = {unigram_model_conferences.probability(sentence_2_tokenized)}\")\n",
    "print(f\"P_bigram(sentence_2)  = {bigram_model_conferences.probability(sentence_2_tokenized)}\")\n",
    "print(f\"P_trigram(sentence_2) = {trigram_model_conferences.probability(sentence_2_tokenized)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
