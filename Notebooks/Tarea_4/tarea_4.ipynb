{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4c46f4c",
   "metadata": {},
   "source": [
    "###  Luis Ricardo Cruz García\n",
    "#### Procesamiento de Lenguaje Natural\n",
    "\n",
    "#### Tarea 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61805f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import nltk\n",
    "import glob\n",
    "import numpy as np\n",
    "from decimal import Decimal\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from typing import Optional\n",
    "from sklearn.preprocessing import normalize\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LiaSqNfTAQNa"
   },
   "outputs": [],
   "source": [
    "def get_texts_from_file(path_corpus: str, path_label: str) -> tuple[list, list]:\n",
    "    \"\"\"Given the corpus and label paths, returns the list of docs and labels.\"\"\"\n",
    "    docs, labels = [], []\n",
    "\n",
    "    with open(path_corpus, \"r\") as f_corpus:\n",
    "        for doc in f_corpus:\n",
    "            docs.append(doc)\n",
    "\n",
    "    with open(path_label, \"r\") as f_labels:\n",
    "        for label in f_labels:\n",
    "            labels.append(label)\n",
    "\n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"Vocabulary class to store the ranking of the words using the \n",
    "    given corpus frequency distribution.\"\"\"\n",
    "    def __init__(self, \n",
    "                 corpus_freqdist: nltk.FreqDist, \n",
    "                 n_words: Optional[int] = None):\n",
    "        vocabulary_freq_desc = self._sort_FreqDist(corpus_freqdist)\n",
    "        \n",
    "        if n_words is not None: # restrict the max num of words\n",
    "            vocabulary_freq_desc = vocabulary_freq_desc[:n_words]\n",
    "\n",
    "        self.vocabulary = [word for word, freq in vocabulary_freq_desc]\n",
    "        \n",
    "        # dictionary of the rank (frequency) of words in the vocabulary, word: freq_ranking\n",
    "        self.word_to_index = {word:rank for rank, word in enumerate(self.vocabulary)}\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sort_FreqDist(fd: nltk.FreqDist) -> list:\n",
    "        \"\"\"Return the list of items (pairs of <word, freq>) sorted by frequency (desc).\"\"\"\n",
    "        aux = list(fd.items())\n",
    "        aux.sort(key=lambda x: x[1], reverse=True)\n",
    "        return aux\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocabulary)\n",
    "    \n",
    "    def __getitem__(self, key: str | int) -> int | str | None:\n",
    "        \"\"\"Depending of the type of the key, returns the word at \n",
    "        index key (if key is an integer) or the rank of the \n",
    "        word key (if key is a string).\n",
    "        \"\"\"\n",
    "        if not isinstance(key, int) and not isinstance(key, np.int64) and not isinstance(key, str):\n",
    "            raise ValueError(f\"Key must be an integer or a string, key = {key}\")\n",
    "        \n",
    "        if isinstance(key, int) or isinstance(key, np.int64):\n",
    "            return self.vocabulary[key]\n",
    "\n",
    "        if isinstance(key, str):\n",
    "            return self.word_to_index[key]\n",
    "\n",
    "    def __contains__(self, key: str) -> bool:\n",
    "        return key in self.word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus_Ngram:\n",
    "    \"\"\"Corpus class to preprocess and store the frequecy and vocabulary for a given list of documents.\"\"\"\n",
    "    def __init__(self, \n",
    "                 docs: list[str], \n",
    "                 n: int = 1, \n",
    "                 start_end_token: bool = False, \n",
    "                 n_words: Optional[int] = None):\n",
    "        self.n = n\n",
    "        self.docs = deepcopy(docs)\n",
    "        self.docs = [doc.lower() for doc in self.docs] # set to lowercase\n",
    "\n",
    "        # add start(<s>) and end(</s>) special tokens to each doc\n",
    "        if start_end_token:\n",
    "            self.docs = [\"<s>\" + doc + \"</s>\" for doc in self.docs]\n",
    "\n",
    "        tokenizer = nltk.TweetTokenizer()\n",
    "\n",
    "        self.tokens = []\n",
    "        for doc in self.docs:\n",
    "            self.tokens += tokenizer.tokenize(doc)\n",
    "\n",
    "        if n != 1:\n",
    "            self.tokens = nltk.ngrams(self.tokens, n)\n",
    "\n",
    "        self.freq = nltk.FreqDist(self.tokens)\n",
    "\n",
    "        self.vocabulary = Vocabulary(self.freq, n_words=n_words)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.docs)\n",
    "    \n",
    "    def __getitem__(self, key: int) -> str:\n",
    "        return self.docs[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(Corpus_Ngram):\n",
    "    \"\"\"Unigram corpus.\"\"\"\n",
    "    def __init__(self, \n",
    "                 docs: list[str], \n",
    "                 start_end_token: bool = False, \n",
    "                 n_words: Optional[int] = None):\n",
    "        super(Corpus, self).__init__(docs, n=1, start_end_token=start_end_token, n_words=n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f99a1c",
   "metadata": {},
   "source": [
    "### 2 Modelo de Lenguaje y Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27c5ad4",
   "metadata": {},
   "source": [
    "2.1. Preprocese todos los tuits de agresividad (positivos y negativos) según su intuición para construir un buen corpus para un modelo de lenguaje (e.g., solo palabras en minúscula, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a098980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training docs and labels\n",
    "train_docs, train_labels = get_texts_from_file(\"../../Data/mex_train.txt\", \"../../Data/mex_train_labels.txt\")\n",
    "train_labels = list(map(int, train_labels))  # cast to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_full_train = Corpus(train_docs, start_end_token=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e733882",
   "metadata": {},
   "source": [
    "Agregue tokens especiales de \"< s >\" y \"</ s >\" según usted considere (e.g., al inicio y final de cada tuit). Defina su vocabulario y enmascare con <unk> toda palabra que no esté en su vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8061548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_OOV_words(corpus: Corpus, \n",
    "                   n_terms: Optional[int]) -> list[str]:\n",
    "        \"\"\"Mask the words out of vocabulary (with an optional restriction on the \n",
    "        number of terms n_terms) with the \"<unk>\" token. \n",
    "        Returns the docs masked.\n",
    "        \"\"\"\n",
    "        docs = deepcopy(corpus.docs)\n",
    "        tokenizer = nltk.TweetTokenizer()\n",
    "\n",
    "        if n_terms is None: n_terms = len(corpus.vocabulary)\n",
    "\n",
    "        for i, doc in enumerate(docs):\n",
    "            doc_masked_words = []\n",
    "            for word in tokenizer.tokenize(doc):\n",
    "                if word in corpus.vocabulary and corpus.vocabulary[word] < n_terms:\n",
    "                    doc_masked_words.append(word)\n",
    "                else:\n",
    "                    doc_masked_words.append(\"<unk>\")\n",
    "            docs[i] = \" \".join(doc_masked_words)\n",
    "\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54bf847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask out of vocabulary words (restricted to the 5k most occurring words) of the train_docs\n",
    "train_docs = mask_OOV_words(corpus_full_train, n_terms=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c35a777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition of train_docs\n",
    "\n",
    "# train   -> 80% of train_docs\n",
    "# heldout -> 10% of train_docs\n",
    "# test    -> 10% of train_docs\n",
    "\n",
    "train   = train_docs[:4435]\n",
    "heldout = train_docs[4435:4989]\n",
    "test    = train_docs[4989:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13267d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct the train corpus from \"train\" (now train is smaller, \n",
    "# with start/end token and masked)\n",
    "corpus_train = Corpus(train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "878f9add",
   "metadata": {},
   "source": [
    "### 2.2. Entrene tres modelos de lenguaje sobre todos los tuits: $𝑃_{𝑢𝑛𝑖𝑔𝑟𝑎𝑚𝑎𝑠}(𝑤_{1:𝑛})$, $𝑃_{𝑏𝑖𝑔𝑟𝑎𝑚𝑎𝑠}(𝑤_{1:𝑛} )$, $𝑃_{𝑡𝑟𝑖𝑔𝑟𝑎𝑚𝑎𝑠}(𝑤_{1:𝑛})$. Para cada uno proporcione una interfaz (función) sencilla. Los modelos deben tener una estrategia común para lidiar con secuencias no vistas. Puede optar por un suavizamiento Laplace o un Good-Turing discounting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39d06e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> puta madre quiero <unk> por la hdp de la <unk> </s>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose a tweet to apply the models\n",
    "test_tweet = train[19]\n",
    "test_tweet_tokenized = tokenizer.tokenize(test_tweet)\n",
    "test_tweet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos las estimaciones de una sucesión de palabras usando un modelo de $n$-grams con Laplace smoothing de la siguiente manera.\n",
    "\n",
    "$$P_L(w_{1:k}) = \\prod_i P_L(w_i | w_{i-n:i-1})  = \\prod_i \\frac{C(w_{i-n:i}) + 1}{C(w_{i-n:i-1}) + V}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en el caso de no se quiera el \"suavizado\", entonces la fórmula se reduce a \n",
    "\n",
    "$$P(w_{1:k}) = \\prod_i P(w_i | w_{i-n:i-1})  = \\prod_i \\frac{C(w_{i-n:i})}{C(w_{i-n:i-1})}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ngram_Model:\n",
    "    def __init__(self, corpus: Corpus, n: int):\n",
    "        if corpus.n != 1:\n",
    "            raise ValueError(\"The corpus must be a unigram corpus.\")\n",
    "        \n",
    "        self.n = n\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        # create n-gram corpus for n and n-1\n",
    "        if self.n == 1:\n",
    "            self.corpus_n = self.corpus\n",
    "            self.corpus_n_minus_1 = self.corpus\n",
    "        elif self.n == 2:\n",
    "            self.corpus_n = Corpus_Ngram(self.corpus.docs, self.n)\n",
    "            self.corpus_n_minus_1 = self.corpus\n",
    "        else:\n",
    "            self.corpus_n = Corpus_Ngram(self.corpus.docs, self.n)\n",
    "            self.corpus_n_minus_1 = Corpus_Ngram(self.corpus.docs, self.n - 1)\n",
    "\n",
    "    def conditional_probability(self, \n",
    "                                word: str, \n",
    "                                prev_words: tuple[str], \n",
    "                                smoothing: bool = True) -> Decimal:\n",
    "        \"\"\"Computes the estimated conditional probability of a word given self.n - 1 previous words.\n",
    "\n",
    "            P(w_i | w_{i-n:i-1}) = C(w_{i-n:i} / C(w_{i-n:i-1}) \n",
    "\n",
    "        if smoothing is enabled, then it will use Laplace smoothing.\n",
    "\n",
    "            P_L(w_i | w_{i-n:i-1}) = (C(w_{i-n:i} + 1) / (C(w_{i-n:i-1}) + V) \n",
    "\n",
    "        where C(w_{i-n:i}) is the number occurrences of word + prev_words in the corpus and \n",
    "        C(w_{i-n:i-1}) is the number of occurrences of prev_words in the corpus.\n",
    "        \"\"\"\t\n",
    "        if not isinstance(prev_words, tuple):\n",
    "            raise ValueError(f\"The prev_words should be a tuple of strings.\")\n",
    "\n",
    "        if len(prev_words) != self.n - 1:\n",
    "            raise ValueError(f\"The tuple prev_words size should be self.n - 1. {len(prev_words) = }\")\n",
    "\n",
    "        if smoothing:\n",
    "            if self.n == 1:\n",
    "                P_L_w_given_prev = Decimal((self.corpus.freq[word] + 1) / (len(self.corpus.tokens) + len(self.corpus.vocabulary)))\n",
    "            elif self.n == 2:\n",
    "                P_L_w_given_prev = Decimal((self.corpus_n.freq[(*prev_words, word)] + 1) / (self.corpus_n_minus_1.freq[prev_words[0]] + len(self.corpus.vocabulary)))\n",
    "            else:\n",
    "                P_L_w_given_prev = Decimal((self.corpus_n.freq[(*prev_words, word)] + 1) / (self.corpus_n_minus_1.freq[prev_words] + len(self.corpus.vocabulary)))\n",
    "        else:\n",
    "            if self.n == 1:\n",
    "                P_L_w_given_prev = Decimal(self.corpus.freq[word] / len(self.corpus.tokens))\n",
    "            elif self.n == 2:\n",
    "                P_L_w_given_prev = Decimal(self.corpus_n.freq[(*prev_words, word)] / self.corpus_n_minus_1.freq[prev_words[0]])\n",
    "            else:\n",
    "                P_L_w_given_prev = Decimal(self.corpus_n.freq[(*prev_words, word)] / self.corpus_n_minus_1.freq[prev_words])\n",
    "\n",
    "        return P_L_w_given_prev\n",
    "\n",
    "    def probability(self, \n",
    "                    seq_words: list[str], \n",
    "                    smoothing: bool = True) -> Decimal:\n",
    "        \"\"\"Computes the estimate probability of sequence of words (i.e. tokenized doc) \n",
    "        using an n-gram model.\n",
    "\n",
    "            P(w_{1:k}) = \\prod_i P(w_i | w_{i-n:i-1})\n",
    "\n",
    "        If smoothing smoothing is enabled, then the probabilities will be computed with Laplace smoothing.\n",
    "            \n",
    "            P_L(w_{1:k}) = \\prod_i P_L(w_i | w_{i-n:i-1})\n",
    "        \n",
    "        where k = len(seq_words) - self.n and P_L(w_i | w_{i-n:i-1}) is estimated \n",
    "        with n-grams (see conditional_probability function).\n",
    "        \"\"\"\n",
    "        log_prob = Decimal(0.)\n",
    "        for ngram in list(nltk.ngrams(seq_words, self.n)):\n",
    "            P_L_w_given_prev = self.conditional_probability(ngram[-1], ngram[:-1], smoothing=smoothing)\n",
    "\n",
    "            # log_prob += ln( P_L(w_i | w_{i-n:i-1}) )\n",
    "            log_prob += P_L_w_given_prev.ln()\n",
    "\n",
    "        return log_prob.exp()\n",
    "\n",
    "    def perplexity(self, seq_words: list[str]):\n",
    "        return (1 / self.probability(seq_words)) ** (1 / Decimal(len(seq_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36e899",
   "metadata": {},
   "source": [
    "#### Modelo de unigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_model_train = Ngram_Model(corpus_train, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_1(test_tweet) = 5.004098532124699262146308101E-23\n"
     ]
    }
   ],
   "source": [
    "print(f\"P_1(test_tweet) = {unigram_model_train.probability(test_tweet_tokenized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04181764",
   "metadata": {},
   "source": [
    "#### Modelo de bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model_train = Ngram_Model(corpus_train, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_2(test_tweet) = 1.679099977175348530204536306E-25\n"
     ]
    }
   ],
   "source": [
    "print(f\"P_2(test_tweet) = {bigram_model_train.probability(test_tweet_tokenized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54ef7a1",
   "metadata": {},
   "source": [
    "#### Modelo Trigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_model_train = Ngram_Model(corpus_train, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_3(test_tweet) = 7.525659602589744277693384578E-31\n"
     ]
    }
   ],
   "source": [
    "print(f\"P_3(test_tweet) = {trigram_model_train.probability(test_tweet_tokenized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f6f7ee",
   "metadata": {},
   "source": [
    "Muestre un par de ejemplos de cómo funciona, al menos uno con una palabra\n",
    "fuera del vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67361068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_train.freq[\"topología\"]  # palabra OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc00092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hagamos un tweet (una oración con los tokens de inicio y fin) cuya única palabra sea \"topología\"\n",
    "tweet_oov = \"<s> topología </s>\"\n",
    "\n",
    "# lo tokenizo para enviarlo a los métodos de cada modelo\n",
    "tweet_oov_tokenized = tokenizer.tokenize(tweet_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_1(tweet_oov) = 2.305041643895685496422031688E-8\n"
     ]
    }
   ],
   "source": [
    "print(f\"P_1(tweet_oov) = {unigram_model_train.probability(tweet_oov_tokenized)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c25e1304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_2(tweet_oov) = 2.195755753439991671758321113E-8\n"
     ]
    }
   ],
   "source": [
    "print(f\"P_2(tweet_oov) = {bigram_model_train.probability(tweet_oov_tokenized)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcdeccfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_3(tweet_oov) = 0.0002046663937781416267146922250\n"
     ]
    }
   ],
   "source": [
    "print(f\"P_3(tweet_oov) = {trigram_model_train.probability(tweet_oov_tokenized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866447b",
   "metadata": {},
   "source": [
    "#### 2.3 Construya un modelo interpolado con valores $\\lambda$ fijos:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some $\\lambda = \\{ \\lambda_0, \\lambda_1, \\lambda_2 \\}$ such that $\\lambda_0 + \\lambda_1 + \\lambda_2 = 1$, the interpolated model using unigrams, bigrams and trigrams is given by\n",
    "\n",
    "$$\\hat{P}(w_{1:k}) = \\prod_i \\left( \\lambda_2 P_L(w_i | w_{i-2}w{i-1}) + \\lambda_1 P_L(w_i | w{i-1}) + \\lambda_0 P_L(w_i) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02bfec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interpolated_Model:\n",
    "    def __init__(self, \n",
    "                 corpus: Corpus):\n",
    "        self.corpus = corpus\n",
    "        self.unigram_model = Ngram_Model(corpus, n=1)\n",
    "        self.bigram_model  = Ngram_Model(corpus, n=2)\n",
    "        self.trigram_model = Ngram_Model(corpus, n=3)\n",
    "    \n",
    "    def conditional_probability(self, \n",
    "                                word: str, \n",
    "                                prev_words: tuple[str], \n",
    "                                lambda_values: list[float]) -> Decimal:\n",
    "        \"\"\"Computes the estimated conditional probability of a word given 2 previous words.\n",
    "\n",
    "            P(w_i | w_{i-2:i-1}) = \\lambda_2 P_L(w_i | w_{i-2}w{i-1}) + \\lambda_1 P_L(w_i | w{i-1}) + \\lambda_0 P_L(w_i)\n",
    "        \"\"\"\n",
    "        if not isinstance(prev_words, tuple):\n",
    "            raise ValueError(f\"The prev_words should be a tuple of strings.\")\n",
    "\n",
    "        if len(prev_words) != 2:\n",
    "            raise ValueError(f\"The tuple prev_words size should be 2. {len(prev_words) = }\")\n",
    "\n",
    "        P_L_w_given_prev = (\n",
    "                (self.unigram_model.conditional_probability(word, ()) * Decimal(lambda_values[0]))\n",
    "                + (self.bigram_model.conditional_probability(word, prev_words[:1]) * Decimal(lambda_values[1]))\n",
    "                + (self.trigram_model.conditional_probability(word, prev_words[:2]) * Decimal(lambda_values[2]))\n",
    "        )\n",
    "\n",
    "        return P_L_w_given_prev\n",
    "\n",
    "    def probability(self, \n",
    "                    seq_words: list[str], \n",
    "                    lambda_values: list[float]) -> Decimal:\n",
    "        \"\"\"Computes the estimate probability of sequence of words (i.e. tokenized doc) \n",
    "        using an interpolated trigram model.\n",
    "\n",
    "            P(w_{1:k}) = \\prod_i P(w_i | w_{i-2:i-1})\n",
    "        \"\"\"\n",
    "        log_prob = Decimal(0.)\n",
    "        for trigram in list(nltk.trigrams(seq_words)):\n",
    "            P_L_w_given_prev = self.conditional_probability(trigram[-1], trigram[:-1], lambda_values)\n",
    "\n",
    "            log_prob += P_L_w_given_prev.ln()\n",
    "\n",
    "        return log_prob.exp()\n",
    "\n",
    "    def perplexity(self, seq_words: list[str], lambda_values: list[float]) -> Decimal:\n",
    "        return (1 / self.probability(seq_words, lambda_values)) ** (1 / Decimal(len(seq_words)))\n",
    "    \n",
    "    def generate(self, lambda_values: list[float], max_len: int = 50):\n",
    "        \"\"\"Generate text using the interpolated model and the lambda_values.\n",
    "        \"\"\"\n",
    "        if len(lambda_values) != 3:\n",
    "            raise ValueError('The model needs 3 lambda values.')\n",
    "\n",
    "        generated_tokens = [\"<s>\"]\n",
    "        prev_prev_word, prev_word = \"<s>\", \"<s>\" # initial previous words\n",
    "\n",
    "        # generates tokens until the end of sentence token is generated or the length is 50 tokens\n",
    "        while prev_word != \"</s>\" and len(generated_tokens) < max_len:\n",
    "            # generates the probabilities of the word given prev_prev_word and prev_word\n",
    "            model_weights = [\n",
    "                float(self.conditional_probability(word, (prev_prev_word, prev_word), lambda_values)) for word in self.corpus.vocabulary\n",
    "            ]\n",
    "\n",
    "            # choose a word in the vocabulary (using the weights to bias the choice to the words with greater probability)\n",
    "            new_word = random.choices(self.corpus.vocabulary, weights=model_weights)[0]\n",
    "\n",
    "            if new_word == \"<s>\": continue\n",
    "\n",
    "            generated_tokens.append(new_word)\n",
    "\n",
    "            # updata previous words\n",
    "            prev_prev_word = prev_word\n",
    "            prev_word      = new_word\n",
    "\n",
    "        return \" \".join(generated_tokens)\n",
    "    \n",
    "    def Expectation_Maximization(self, seq_words: list[str], n_iterations: int = 5):\n",
    "        \"\"\"Expectation-maximization algorithm for interpolated language modeling. \n",
    "        Learns the lambda parameters for the interpolated language model.\n",
    "        See Jacob Eisenstein's book \"Natural Laguage Processing\". \"\"\"\n",
    "        n_max = 3\n",
    "        M = len(seq_words)\n",
    "        \n",
    "        trigrams_seq_words = list(nltk.trigrams(seq_words))\n",
    "\n",
    "        lambda_z = np.array([1/3, 1/3, 1/3], dtype=float)\n",
    "        q = np.zeros(shape=(M - 2, n_max), dtype=float)\n",
    "\n",
    "        print(f'\\tInitial lambdas = {lambda_z}')\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        for it in range(n_iterations):\n",
    "            for m, (w2, w1, w) in enumerate(trigrams_seq_words):\n",
    "                q[m][0] = lambda_z[0] * float(self.unigram_model.conditional_probability(w, ()))\n",
    "                q[m][1] = lambda_z[1] * float(self.bigram_model.conditional_probability(w, (w1,)))\n",
    "                q[m][2] = lambda_z[2] * float(self.trigram_model.conditional_probability(w, (w2, w1)))\n",
    "\n",
    "                q[m] =  q[m] * np.divide(1, np.sum(np.abs(q[m]))) # normalize (l1 norm) the m-th row\n",
    "\n",
    "            lambda_z = (1 / M) * np.sum(q, axis=0)\n",
    "\n",
    "            print(f'i = {it}: lambdas = {lambda_z}, ', end=\"\")\n",
    "            print(f\"PP(heldout) = {float(self.perplexity(seq_words, lambda_z)):.2f}\")\n",
    "\n",
    "        return lambda_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07e6eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_lambdas = [\n",
    "    [0.33, 0.33, 0.33], \n",
    "    [0.4, 0.4, 0.2], \n",
    "    [0.2, 0.4, 0.4], \n",
    "    [0.5, 0.4, 0.1], \n",
    "    [0.1, 0.4, 0.5]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1f2b794",
   "metadata": {},
   "outputs": [],
   "source": [
    "heldout_tokenized = tokenizer.tokenize(\" \".join(heldout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993aa634",
   "metadata": {},
   "source": [
    "Muestre cómo bajan o suben las perplejidades en el held-out, finalmente pruebe una vez en test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_model_train = Interpolated_Model(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24a7b8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33, 0.33, 0.33]: PP(heldout) = 511.67\n",
      "[0.4, 0.4, 0.2]: PP(heldout) = 472.52\n",
      "[0.2, 0.4, 0.4]: PP(heldout) = 626.09\n",
      "[0.5, 0.4, 0.1]: PP(heldout) = 434.30\n",
      "[0.1, 0.4, 0.5]: PP(heldout) = 813.17\n"
     ]
    }
   ],
   "source": [
    "for lambda_vals in possible_lambdas:\n",
    "    print(f\"{lambda_vals}: PP(heldout) = {float(interpolated_model_train.perplexity(heldout_tokenized, lambda_vals)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba98f36",
   "metadata": {},
   "source": [
    "De las perplejidades calculadas para las $\\lambda$ dadas, las que se desempeñaron mejor (pues la perplejidad obtenida en el Held-out fue la mínima) fueron $$\\lambda = [0.5, 0.4, 0.1]$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54b4bcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PP(test) = 410.5147955904772932987539322\n"
     ]
    }
   ],
   "source": [
    "# al evaluar en el test usando los valores lambda = [0.5, 0.4, 0.1] obtenemos la siguiente perplejidad\n",
    "test_tokenized = tokenizer.tokenize(\" \".join(test))\n",
    "print(f\"PP(test) = {interpolated_model_train.perplexity(test_tokenized, [0.5, 0.4, 0.1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8c257",
   "metadata": {},
   "source": [
    "### 3. Generación de Texto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7de5936c",
   "metadata": {},
   "source": [
    "#### 3.1. Proponga una estrategia con base en **Expectation Maximization** para encontrar buenos valores de interpolación en $\\hat{𝑃}$ usando todo el dataset de agresividad. Para ello experimente con el modelo en particiones de $80\\%$, $10\\%$ y $10\\%$ para entrenar (train), ajustar parámetros (val) y probar (test) respectivamente. Muestre como bajan las perplejidades en $5$ iteraciones que usted elija (de todas las que sean necesarias de acuerdo a su EM) en validación, y pruebe una vez en test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25971ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tInitial lambdas = [0.33333333 0.33333333 0.33333333]\n",
      "--------------------------------------------------------------------------------\n",
      "i = 0: lambdas = [0.51176631 0.33576714 0.15228981], PP(heldout) = 426.90\n",
      "i = 1: lambdas = [0.59594392 0.32821656 0.07566278], PP(heldout) = 404.63\n",
      "i = 2: lambdas = [0.63334677 0.32619773 0.04027876], PP(heldout) = 397.15\n",
      "i = 3: lambdas = [0.64961421 0.32794002 0.02226902], PP(heldout) = 394.54\n",
      "i = 4: lambdas = [0.65622659 0.33106038 0.01253629], PP(heldout) = 393.77\n"
     ]
    }
   ],
   "source": [
    "lambda_values_EM = interpolated_model_train.Expectation_Maximization(heldout_tokenized, n_iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65622659 0.33106038 0.01253629]\n"
     ]
    }
   ],
   "source": [
    "print(lambda_values_EM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68750f1b",
   "metadata": {},
   "source": [
    "3.2. Haga una función \"tuitear\" con base en su modelo de lenguaje $\\hat{𝑃}$ del último punto. El modelo deberá poder parar automáticamente cuando genere el símbolo de terminación de tuit al final (e.g., \"<\\/s>\"), o $50$ palabras. Proponga algo para que en los últimos tokens sea más probable generar el token \"<\\/s>\". Muestre al menos cinco ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> * vamos madre ptm educados contenido 😛 tus de 30 </s>'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweet function\n",
    "interpolated_model_train.generate(list(lambda_values_EM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c99995",
   "metadata": {},
   "source": [
    "3.3. Use la intuición que ha ganado en esta tarea y los datos de las mañaneras para entrenar un modelo de lenguaje ``` AMLO``` . Haga una función ``` dar_conferencia()```. Genere un discurso de $300$ palabras y detenga al modelo de forma abrupta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2224a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "conferences = []\n",
    "\n",
    "for file in glob.glob(\"../../Data/estenograficas_limpias/*\"):\n",
    "\twith open(file, \"r\") as f_corpus:\n",
    "\t\tfor sentence in f_corpus:\n",
    "\t\t\tconferences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_conferences = Corpus(conferences, start_end_token=True)\n",
    "conferences_masked = mask_OOV_words(corpus_conferences, n_terms=5000)\n",
    "corpus_conferences = Corpus(conferences_masked) # corpus with masked conferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMLO = Interpolated_Model(corpus_conferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4cf021f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> asegurar abandonar social el papel 1º hoy corruptos acabó tomaron decía estarían culiacán movimientos fuera , ven fronteriza consultar'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AMLO.generate([0.1, 0.3, 0.6], max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc06f47b",
   "metadata": {},
   "source": [
    "3.4. Calcule el estimado de cada uno sus modelos de lenguaje (el de tuits y el de amlo) para las frases: \"si no gano me voy a la chingada\", \"ya se va a acabar la corrupción\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c613f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = \"si no gano me voy a la chingada\"\n",
    "sentence_2 = \"ya se va a acabar la corrupción\"\n",
    "\n",
    "sentence_1_tokenized = tokenizer.tokenize(sentence_1)\n",
    "sentence_2_tokenized = tokenizer.tokenize(sentence_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b578d1",
   "metadata": {},
   "source": [
    "Con los modelos de los tweets agresivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4885048b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_unigram(sentence_1) = 1.422737445556451821090663411E-20\n",
      "P_bigram(sentence_1)  = 8.717001740778747181481628152E-18\n",
      "P_trigram(sentence_1) = 3.466901356069048238502135543E-20\n"
     ]
    }
   ],
   "source": [
    "# evaluamos a sentence_1 = \"si no gano me voy a la chingada\"\n",
    "print(f\"P_unigram(sentence_1) = {unigram_model_train.probability(sentence_1_tokenized)}\")\n",
    "print(f\"P_bigram(sentence_1)  = {bigram_model_train.probability(sentence_1_tokenized)}\")\n",
    "print(f\"P_trigram(sentence_1) = {trigram_model_train.probability(sentence_1_tokenized)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84661de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_unigram(sentence_2) = 1.460573663588147904551449248E-19\n",
      "P_bigram(sentence_2)  = 7.270212752785908748956856244E-18\n",
      "P_trigram(sentence_2) = 1.404911774707322600534679050E-17\n"
     ]
    }
   ],
   "source": [
    "# evaluamos a sentence_2 = \"ya se va a acabar la corrupción\"\n",
    "print(f\"P_unigram(sentence_2) = {unigram_model_train.probability(sentence_2_tokenized)}\")\n",
    "print(f\"P_bigram(sentence_2)  = {bigram_model_train.probability(sentence_2_tokenized)}\")\n",
    "print(f\"P_trigram(sentence_2) = {trigram_model_train.probability(sentence_2_tokenized)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b10d91a",
   "metadata": {},
   "source": [
    "Con los modelos de las conferencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bca4be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_model_conferences = Ngram_Model(corpus_conferences, n=1)\n",
    "bigram_model_conferences = Ngram_Model(corpus_conferences, n=2)\n",
    "trigram_model_conferences = Ngram_Model(corpus_conferences, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_unigram(sentence_1) = 4.532466486548684280014766525E-28\n",
      "P_bigram(sentence_1)  = 1.129882724757320311605002232E-18\n",
      "P_trigram(sentence_1) = 6.558164955988236444108646296E-20\n"
     ]
    }
   ],
   "source": [
    "# evaluamos a sentence_1 = \"si no gano me voy a la chingada\"\n",
    "print(f\"P_unigram(sentence_1) = {unigram_model_conferences.probability(sentence_1_tokenized)}\")\n",
    "print(f\"P_bigram(sentence_1)  = {bigram_model_conferences.probability(sentence_1_tokenized)}\")\n",
    "print(f\"P_trigram(sentence_1) = {trigram_model_conferences.probability(sentence_1_tokenized)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_unigram(sentence_2) = 7.950891199318519215067285481E-18\n",
      "P_bigram(sentence_2)  = 6.235694567159202024259164979E-10\n",
      "P_trigram(sentence_2) = 4.095999863770872298837284021E-9\n"
     ]
    }
   ],
   "source": [
    "# evaluamos a sentence_2 = \"ya se va a acabar la corrupción\"\n",
    "print(f\"P_unigram(sentence_2) = {unigram_model_conferences.probability(sentence_2_tokenized)}\")\n",
    "print(f\"P_bigram(sentence_2)  = {bigram_model_conferences.probability(sentence_2_tokenized)}\")\n",
    "print(f\"P_trigram(sentence_2) = {trigram_model_conferences.probability(sentence_2_tokenized)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
