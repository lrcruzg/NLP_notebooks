{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7323a8e",
   "metadata": {},
   "source": [
    "###  Luis Ricardo Cruz Garc√≠a\n",
    "#### Procesamiento de Lenguaje Natural\n",
    "\n",
    "#### Tarea 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb9725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from typing import Tuple\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams, FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd5ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25c28508",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(\"mex_train.txt\", sep=\"\\r\\n\", engine=\"python\", header=None).loc[:, 0].values.tolist()\n",
    "x_val   = pd.read_csv(\"mex_val.txt\"  , sep=\"\\r\\n\", engine=\"python\", header=None).loc[:, 0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "247466d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# esta class es escencialmente id√©ntica la vista en clase, s√≥lo le agregu√© unos peque√±os cambios para que \n",
    "# pudiera ser utilizada en todos los casos que se pide en la tarea\n",
    "class NgramData():\n",
    "\tdef __init__(self, \n",
    "\t\t\t\tN: int,                          # n√∫mero del N-grama\n",
    "\t\t\t\tmax_vocabulary_size: int = 5000, # tama√±o m√°ximo del vocabulario\n",
    "\t\t\t\ttokenizer = None,                # tokenizador\n",
    "\t\t\t\tnumeric = False,                 # acepta o no tokens num√©ricos\n",
    "\t\t\t\tembeddings_model = None,         # array de word-representations\n",
    "\t\t\t\tembedding_words = None,          # palabras de las cuales tenemos un embedding\n",
    "\t\t\t\tword_to_emb_id = None):          # diccionario de la palabra al √≠ndice en el array \"embeddings_model\"\n",
    "\n",
    "\t\tself.tokenizer = tokenizer if tokenizer else self.default_tokenizer()\n",
    "\t\tself.punct = set([\".\", \",\", \";\", \":\", \"^\", \"!\", \"¬°\", \"¬ø\", \"?\", \"\\'\", \"*\", \"<url>\", \"@usuario\"])\n",
    "\t\tself.embeddings_model = embeddings_model\n",
    "\t\tself.embedding_words = embedding_words\n",
    "\t\tself.word_to_emb_id = word_to_emb_id\n",
    "\t\tself.max_vocabulary_size = max_vocabulary_size\n",
    "\t\tself.numeric = numeric\n",
    "\t\tself.N = N\n",
    "\t\tself.UNK = \"<unk>\"\n",
    "\t\tself.SOS = \"<s>\"\n",
    "\t\tself.EOS = \"</s>\"\n",
    "\n",
    "\tdef remove_word(self, word:str) -> bool:\n",
    "\t\tlo_word = word.lower()\n",
    "\t\t# se pueden aceptar o no tokens num√©ricos\n",
    "\t\tif not self.numeric:\n",
    "\t\t\treturn lo_word in self.punct or lo_word.isnumeric()\n",
    "\t\telse:\n",
    "\t\t\treturn lo_word in self.punct\n",
    "\n",
    "\tdef get_vocabulary(self, corpus: list) -> set:\n",
    "\t\tfreq_dist = FreqDist([word.lower() for sentence in corpus for word in self.tokenizer(sentence) if not self.remove_word(word)])\n",
    "\t\tsorted_words = self.sortFreqDict(freq_dist)[:self.max_vocabulary_size - 3]\n",
    "\t\treturn set(sorted_words)\n",
    "\n",
    "\tdef build_embedding_matrix(self):\n",
    "\t\tembedding_dimension = self.embeddings_model.shape[1]\n",
    "\t\tself.embedding_matrix = np.zeros((len(self.vocabulary), embedding_dimension))\n",
    "\t\t\n",
    "\t\tfor i, word in enumerate(self.vocabulary):\n",
    "\t\t\tif word in embedding_words:\n",
    "\t\t\t\tself.embedding_matrix[i] = self.embeddings_model[self.word_to_emb_id[word]]\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dimension, ))\n",
    "\n",
    "\tdef fit(self, corpus:list) -> None:\n",
    "\t\tself.vocabulary = self.get_vocabulary(corpus)\n",
    "\t\tself.vocabulary.add(self.UNK)\n",
    "\t\tself.vocabulary.add(self.SOS)\n",
    "\t\tself.vocabulary.add(self.EOS)\n",
    "\n",
    "\t\tself.w2id = {}\n",
    "\t\tself.id2w = {}\n",
    "\n",
    "\t\tid_word = 0\n",
    "\t\tfor doc in corpus:\n",
    "\t\t\tfor word in self.tokenizer(doc):\n",
    "\t\t\t\tword_lower = word.lower()\n",
    "\t\t\t\tif word_lower in self.vocabulary and word_lower not in self.w2id:\n",
    "\t\t\t\t\tself.w2id[word_lower] = id_word\n",
    "\t\t\t\t\tself.id2w[id_word]    = word_lower\n",
    "\t\t\t\t\tid_word += 1\n",
    "\n",
    "\t\t# agregar tokens especiales\n",
    "\t\tself.w2id.update(\n",
    "\t\t\t{\n",
    "\t\t\t\tself.UNK: id_word,\n",
    "\t\t\t\tself.SOS: id_word + 1,\n",
    "\t\t\t\tself.EOS: id_word + 2\n",
    "\n",
    "\t\t\t}\n",
    "\t\t)\n",
    "\n",
    "\t\tself.id2w.update(\n",
    "\t\t\t{\n",
    "\t\t\t\tid_word: self.UNK,\n",
    "\t\t\t\tid_word + 1: self.SOS,\n",
    "\t\t\t\tid_word + 2: self.EOS \n",
    "\n",
    "\t\t\t}\n",
    "\t\t)\n",
    "\n",
    "\t\t# si nos pasan un embeddings_model, creamos la matriz de embeddings\n",
    "\t\tif self.embeddings_model is not None:\n",
    "\t\t\tself.build_embedding_matrix()\n",
    "\n",
    "\tdef transform(self, corpus : list) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\t\tx_ngrams = []\n",
    "\t\ty = []\n",
    "\t\tfor doc in corpus:\n",
    "\t\t\tdoc_ngram = self.get_ngram_doc(doc)\n",
    "\t\t\tfor words_window in doc_ngram:\n",
    "\t\t\t\twords_window_ids = [self.w2id[word] for word in words_window]\n",
    "\t\t\t\tx_ngrams.append(list(words_window_ids[:-1]))\n",
    "\t\t\t\ty.append(words_window_ids[-1])\n",
    "\n",
    "\t\treturn array(x_ngrams), array(y)\n",
    "\n",
    "\tdef get_ngram_doc(self, doc : str) -> list:\n",
    "\t\tdoc_tokens = self.tokenizer(doc)\n",
    "\t\tdoc_tokens = [word.lower() for word in doc_tokens]\n",
    "\t\tdoc_tokens = self.replace_unk(doc_tokens)\n",
    "\t\tdoc_tokens = [self.SOS] * (self.N - 1) + doc_tokens + [self.EOS]\n",
    "\t\treturn list(ngrams(doc_tokens, self.N))\n",
    "\n",
    "\tdef replace_unk(self, doc_tokens):\n",
    "\t\tfor i, token in enumerate(doc_tokens):\n",
    "\t\t\tif token.lower() not in self.vocabulary:\n",
    "\t\t\t\tdoc_tokens[i] = self.UNK\n",
    "\n",
    "\t\treturn doc_tokens\n",
    "\n",
    "\tdef sortFreqDict(self, fdist_dict):\n",
    "\t\taux = list(fdist_dict.keys())\n",
    "\t\taux.sort(key=lambda char : fdist_dict[char], reverse=True)\n",
    "\t\treturn aux\n",
    "\n",
    "\tdef default_tokenizer(self, doc : str) -> list:\n",
    "\t\treturn doc.split(\" \")\n",
    "\n",
    "\tdef get_vocabulary_size(self) -> int:\n",
    "\t\treturn len(self.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4ef2a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se agrega la opci√≥n de que se tome la conexi√≥n directa entre la capa de embeddings y la capa de salida\n",
    "# y se agrega la opci√≥n de usar una matriz de embeddings pre-entrenados\n",
    "class NeuralLM(nn.Module):\n",
    "\tdef __init__(self, args, embedding_matrix=None, direct_emb_to_output=False):\n",
    "\t\tsuper(NeuralLM, self).__init__()\n",
    "\n",
    "\t\tself.window_size = args.N - 1\n",
    "\t\tself.embedding_size = args.d\n",
    "\t\tself.direct_emb_to_output = direct_emb_to_output\n",
    "\n",
    "\t\tself.emb = nn.Embedding(args.vocabulary_size, args.d)\n",
    "\n",
    "\t\tif embedding_matrix is not None:\n",
    "\t\t\tself.emb.load_state_dict({'weight': torch.Tensor(embedding_matrix)})\n",
    "\t\t\tself.emb.weight.requires_grad = False\n",
    "\n",
    "\t\tself.fc1 = nn.Linear(args.d * (args.N - 1), args.d_h)\n",
    "\t\tself.drop1 = nn.Dropout(p=args.dropout)\n",
    "\n",
    "\t\tif self.direct_emb_to_output:\n",
    "\t\t\tself.fc2 = nn.Linear(args.d_h + (args.d * (args.N - 1)), args.vocabulary_size, bias=False)\n",
    "\t\telse:\n",
    "\t\t\tself.fc2 = nn.Linear(args.d_h, args.vocabulary_size, bias=False)\n",
    "\n",
    "\t# x = lista de ids de las palabras en un n-grama\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.emb(x)\n",
    "\t\tx = x.view(-1, self.window_size * self.embedding_size)\n",
    "\n",
    "\t\th = F.relu(self.fc1(x))\n",
    "\t\th = self.drop1(h)\n",
    "\n",
    "\t\t# si se quiere que exista la conexi√≥n directa entre la capa de embeddings y la capa de salida\n",
    "\t\t# entonces se agrega a los argumentos que opera la capa de salida (fc2)\n",
    "\t\tif self.direct_emb_to_output:\n",
    "\t\t\th = torch.cat((x, h), dim=1)\n",
    "\t\treturn self.fc2(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f3a9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(raw_logits):\n",
    "\tprobs = F.softmax(raw_logits.detach(), dim=1)\n",
    "\ty_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "\treturn y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b10aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(data, model, gpu=False):\n",
    "\twith torch.no_grad():\n",
    "\t\tpreds, tgts = [], []\n",
    "\t\tfor window_words, labels in data:\n",
    "\t\t\tif gpu:\n",
    "\t\t\t\twindow_words = window_words.cuda()\n",
    "\n",
    "\t\t\toutputs = model(window_words)\n",
    "\n",
    "\t\t\ty_pred = get_preds(outputs)\n",
    "\n",
    "\t\t\ttgt = labels.numpy()\n",
    "\t\t\ttgts.append(tgt)\n",
    "\t\t\tpreds.append(y_pred)\n",
    "\n",
    "\ttgts  = [e for l in tgts  for e in l]\n",
    "\tpreds = [e for l in preds for e in l]\n",
    "\n",
    "\treturn accuracy_score(tgts, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e351aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint_path, filename=\"checkpoint.pt\"): \n",
    "\tfilename = os.path.join(checkpoint_path, filename)\n",
    "\ttorch.save(state, filename)\n",
    "\tif is_best:\n",
    "\t\tshutil.copyfile(filename, os.path.join(checkpoint_path, \"model_best.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16ec70",
   "metadata": {},
   "source": [
    "### 1. Con base en la implementaci√≥n mostrada en clase, construya un modelo de lenguaje neuronal a nivel de car√°cter. Tom√© en cuenta secuencias de tama√±o 6 o m√°s para el modelo, es decir hasta 5 caracteres o m√°s en el contexto. Ponga al modelo a generar texto 3 veces, con un m√°ximo de 300 caracteres. Escriba 5 ejemplos de oraciones y m√≠dales el likelihood. Escriba un ejemplo de estructura morfol√≥gica (permutaciones con caracteres) similar al de estructura sint√°ctica del profesor con 5 o m√°s caracteres de su gusto (e.g., \"ando \"). Calcule la perplejidad del modelo sobre los datos val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9b3acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 6\n",
    "\n",
    "# el \"tokenizer\" es la funci√≥n list, as√≠ hacemos que tokenize por caracter a los strings\n",
    "ngram_data_chars = NgramData(args.N, 5000, list, numeric=False)\n",
    "ngram_data_chars.fit(x_train)\n",
    "\n",
    "x_ngram_train, y_ngram_train = ngram_data_chars.transform(x_train)\n",
    "x_ngram_val  , y_ngram_val   = ngram_data_chars.transform(x_val)\n",
    "\n",
    "args.batch_size = 64\n",
    "args.num_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d1f77a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(x_ngram_train, dtype=torch.int64), torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=args.num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "978b2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = TensorDataset(torch.tensor(x_ngram_val, dtype=torch.int64), torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, num_workers=args.num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e0f0441",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Model hyperparameters \n",
    "\n",
    "args.vocabulary_size = ngram_data_chars.get_vocabulary_size()\n",
    "args.d = 100\n",
    "args.d_h = 200\n",
    "args.dropout = 0.1\n",
    "\n",
    "# Train hyperparameters\n",
    "\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db8768c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.savedir = \"model_chars\"\n",
    "os.makedirs(args.savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b5f8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_chars = NeuralLM(args)\n",
    "\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "\tmodel_chars.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_chars.parameters(), lr=args.lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=args.lr_patience, verbose=True, factor=args.lr_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a65c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# esta part√© s√≠ la corr√≠, pero en un colab (porque toma muucho tiempo en mi computadora), s√≥lo copi√© el archivo \n",
    "# \"best_model.pt\" (que generaba colab) a la carpeta correspondiente y segu√≠ con lo dem√°s\n",
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "\tepoch_start_time =  time.time()\n",
    "\tloss_epoch = []\n",
    "\ttraining_metric = []\n",
    "\tmodel_chars.train()\n",
    "\n",
    "\tfor window_words, labels in train_loader:\n",
    "\n",
    "\t\t# if gpu available\n",
    "\t\tif args.use_gpu:\n",
    "\t\t\twindow_words = window_words.cuda()\n",
    "\t\t\tlabels = labels.cuda()\n",
    "\n",
    "\t\t# forward pass\n",
    "\t\toutputs = model_chars(window_words)\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\tloss_epoch.append(loss.item())\n",
    "\n",
    "\t\t# get_training metrics\n",
    "\t\ty_pred = get_preds(outputs)\n",
    "\t\ttgt = labels.cpu().numpy()\n",
    "\t\ttraining_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "\t\t# posteriormente, hacemos el backward y optimizamos\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t# get metric n training dataset\n",
    "\tmean_epoch_metric = np.mean(training_metric)\n",
    "\ttrain_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "\t# get metric in validation dataset\n",
    "\tmodel_chars.eval()\n",
    "\ttuning_metric = model_eval(val_loader, model_chars, gpu=args.use_gpu)\n",
    "\tmetric_history.append(mean_epoch_metric)\n",
    "\n",
    "\t# update scheduler\n",
    "\tscheduler.step(tuning_metric)\n",
    "\n",
    "\t# chech for metric improvement\n",
    "\tis_improvement = tuning_metric > best_metric\n",
    "\tif is_improvement:\n",
    "\t\tvest_metric = tuning_metric\n",
    "\t\tn_no_improve = 0\n",
    "\telse:\n",
    "\t\tn_no_improve += 1\n",
    "\n",
    "\tsave_checkpoint(\n",
    "\t\t{\n",
    "\t\t\"epoch\" : epoch + 1, \n",
    "\t\t\"state_dict\" : model_chars.state_dict(), \n",
    "\t\t\"optimizer\" : optimizer.state_dict(),\n",
    "\t\t\"scheduler\" : scheduler.state_dict(), \n",
    "\t\t\"best_metric\" : best_metric\n",
    "\t\t}, \n",
    "\t\tis_improvement, \n",
    "\t\targs.savedir\n",
    "\t)\n",
    "\n",
    "\t# detener el modelo si no hay mejora\n",
    "\tif n_no_improve >= args.patience:\n",
    "\t\tprint(\"No improvement. Breaking out of loop\")\n",
    "\t\tbreak\n",
    "\n",
    "\tprint(\"Train acc: {}\".format(mean_epoch_metric))\n",
    "\tprint(\"Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time : {:.2f}\".format(epoch + 1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time() - epoch_start_time)))\n",
    "\n",
    "print(\"--- %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72e35f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text, tokenizer, ngram_data):\n",
    "\tall_tokens = [w.lower() if w in ngram_data.w2id else \"<unk>\" for w in tokenizer(text)]\n",
    "\ttoken_ids = [ngram_data.w2id[word.lower()] for word in all_tokens]\n",
    "\treturn all_tokens, token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8316be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_word(logits, temperature=1.0):\n",
    "\tlogits = np.asarray(logits).astype(\"float64\")\n",
    "\tpreds = logits / temperature\n",
    "\texp_preds = np.exp(preds)\n",
    "\tpreds = exp_preds / np.sum(exp_preds)\n",
    "\tprobas = np.random.multinomial(1, preds)\n",
    "\treturn np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbabecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(model, token_ids):\n",
    "\tword_ids_tensor = torch.LongTensor(token_ids).unsqueeze(0)\n",
    "\ty_raw_pred = model(word_ids_tensor).squeeze(0).detach().numpy()\n",
    "\t\n",
    "\ty_pred = sample_next_word(y_raw_pred, 1.0)\n",
    "\treturn y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c04e056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, initial_text, tokenizer, ngram_data, max_gen_tokens : int = 100, join_char : str = \" \"):\n",
    "\tall_tokens, window_word_ids = parse_text(initial_text, tokenizer, ngram_data)\n",
    "\tfor i in range(max_gen_tokens):\n",
    "\t\ty_pred = predict_next_token(model, window_word_ids)\n",
    "\t\tnext_word = ngram_data.id2w[y_pred]\n",
    "\t\tall_tokens.append(next_word)\n",
    "\n",
    "\t\tif next_word == \"</s>\":\n",
    "\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\twindow_word_ids.pop(0)\n",
    "\t\t\twindow_word_ids.append(y_pred)\n",
    "\n",
    "\treturn join_char.join(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34e8b8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(348, 100)\n",
       "  (fc1): Linear(in_features=500, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=348, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_chars = NeuralLM(args)\n",
    "best_model_chars.load_state_dict(torch.load(\"model_chars/model_best.pt\", map_location=torch.device(\"cpu\"))[\"state_dict\"])\n",
    "best_model_chars.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b771f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ando en la verga moso pero se quieren ex√° mi bendiendo sus una interischino de inundo tu plocura que le dos de inventy est√° soy <unk> (su vas a la madre de est√© como una no mamar mis√≥tarde la pela y no haya pinche recercial empezar la cera fojos de bendilda<unk>  üòê</s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"ando \"\n",
    "\n",
    "# el tokenizer es la funci√≥n \"list\"\n",
    "print(generate_sentence(best_model_chars, initial_tokens, list, ngram_data_chars, max_gen_tokens=300, join_char=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba05303a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola del ni√±os y una hdp<unk><unk></s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"hola \"\n",
    "\n",
    "# el tokenizer es la funci√≥n \"list\" pues seguimos usando chars\n",
    "print(generate_sentence(best_model_chars, initial_tokens, list, ngram_data_chars, max_gen_tokens=300, join_char=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a377a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llevando para daba comer√≠a wala</s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"lleva\"\n",
    "\n",
    "# el tokenizer es la funci√≥n \"list\" pues seguimos usando chars\n",
    "print(generate_sentence(best_model_chars, initial_tokens, list, ngram_data_chars, max_gen_tokens=300, join_char=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bcaa3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(model, text, ngram_data):\n",
    "\t# generate n-gram window from input text and the respective label y\n",
    "\tx, y = ngram_data.transform([text])\n",
    "\n",
    "\t# no tomar en cuenta los dos primeros n-gram windows pues son \"<s>\"\n",
    "\tx, y = x[2:], y[2:]\n",
    "\n",
    "\tx = torch.LongTensor(x).unsqueeze(0)\n",
    "\n",
    "\tlogits = model(x).detach()\n",
    "\tprobs = F.softmax(logits, dim=1).numpy()\n",
    "\n",
    "\treturn np.sum([np.log(probs[i][w]) for i, w in enumerate(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "316e31aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, text, ngram_data):\n",
    "\tlen_text = len(text)\n",
    "\tx, y = ngram_data.transform([text])\n",
    "\tx, y = x[2:], y[2:]\n",
    "\n",
    "\tx = torch.LongTensor(x).unsqueeze(0)\n",
    "\n",
    "\tlogits = model(x).detach()\n",
    "\n",
    "\tprobs = F.softmax(logits, dim=1).numpy()\n",
    "\n",
    "\tpartial_prod = 1\n",
    "\n",
    "\tfor i, w in enumerate(y):\n",
    "\t\tpartial_prod *= probs[i][w]\n",
    "\n",
    "\treturn Decimal(partial_prod) ** Decimal(1 / len_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "88e46282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -45.579407\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model_chars, \"clase de lenguage natural\", ngram_data_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "672c1660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -38.200916\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model_chars, \"messi es el mejor jugador\", ngram_data_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8ad47b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -46.30466\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model_chars, \"ronaldo es el mejor jugador\", ngram_data_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5880c1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -37.17421\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model_chars, \"amlo es un mal presidente\", ngram_data_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "851dd6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -43.010994\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model_chars, \"mexico le va a ganar a argentina\", ngram_data_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6a2df57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.891865  ->  rateros\n",
      "-11.700338  ->  ratores\n",
      "-11.899188  ->  rreatos\n",
      "-11.925676  ->  rroetas\n",
      "-11.952572  ->  sratero\n",
      "--------------------------------------------------\n",
      "-53.344097  ->  seoatrr\n",
      "-53.891838  ->  trsroea\n",
      "-54.5099  ->  trsreoa\n",
      "-55.720245  ->  trsroae\n",
      "-56.395157  ->  trsraoe\n"
     ]
    }
   ],
   "source": [
    "char_list = \"arresto\"\n",
    "perms = [\"\".join(perm) for perm in list(set(permutations(list(char_list))))]\n",
    "\n",
    "likelihood_word = [(log_likelihood(best_model_chars, text, ngram_data_chars), text) for text in perms]\n",
    "likelihood_word = sorted(likelihood_word, reverse=True)\n",
    "\n",
    "for likelihood, permutation in likelihood_word[:5]:\n",
    "\tprint(likelihood, \" -> \", permutation)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for likelihood, permutation in likelihood_word[-5:]:\n",
    "\tprint(likelihood, \" -> \", permutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ccdac9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P = 0\n"
     ]
    }
   ],
   "source": [
    "# Tengo un problema de underflow con la perplejidad, no lo puede solucionar, pero creo que lo hace bien\n",
    "print(\"P = {}\".format(perplexity(best_model_chars, \" \".join(x_val), ngram_data_chars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e1950e",
   "metadata": {},
   "source": [
    "### 2. Con base en la implementaci√≥n mostrada en clase, construya un modelo de lenguaje neuronal a nivel de palabra, pero preinicializado con los embeddings proporcionados. Tom√© en cuenta secuencias de tama√±o 4 para el modelo, es decir hasta 3 palabras en el contexto. Despu√©s de haber entrenado el modelo, recupere las 10 palabras m√°s similares a tres palabras de su gusto dadas. Ponga al modelo a generar texto a partir de tres secuencias de inicio de su gusto. Escriba 5 ejemplos de oraciones y m√≠dales el likelihood. Proponga un ejemplo para ver estructuras sint√°cticas (permutaciones de palabras de alguna oraci√≥n) buenas usando el likelihood a partir de una oraci√≥n que usted proponga. Calcule la perplejidad del modelo sobre los datos val. Comp√°relo con la perplejidad del modelo de lenguaje sin embeddings preentrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c2d3532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "26b5ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos los embeddings del archivo\n",
    "embedding_words  = set()\n",
    "word_to_emb_id   = {} \n",
    "vector_embeddings = []\n",
    "\n",
    "with open('word2vec_col.txt', 'r') as f:\n",
    "\tnext(f) # ignoramos la primer l√≠nea \n",
    "\tfor index, line in enumerate(f):\n",
    "\t\ttokens = line.split()\n",
    "\t\tword = tokens[0]\n",
    "\t\tembedding_words.add(word)\n",
    "\t\tword_to_emb_id[word] = index\n",
    "\t\tword_embedding = np.array(tokens[1:]).astype(float)\n",
    "\t\tvector_embeddings.append(word_embedding)\n",
    "\n",
    "vector_embeddings = np.array(vector_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d24b38ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "args2 = Namespace()\n",
    "args2.N = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b8e41159",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_data_w2v = NgramData(args2.N, \n",
    "\t\t\t\t\t\t\t5000, \n",
    "\t\t\t\t\t\t\ttokenizer.tokenize, \n",
    "\t\t\t\t\t\t\tembeddings_model=vector_embeddings, \n",
    "\t\t\t\t\t\t\tembedding_words=embedding_words,\n",
    "\t\t\t\t\t\t\tword_to_emb_id=word_to_emb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dc524a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_data_w2v.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d84dbaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ngram_train, y_ngram_train = ngram_data_w2v.transform(x_train)\n",
    "x_ngram_val  , y_ngram_val   = ngram_data_w2v.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "030b70ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "args2.batch_size = 64\n",
    "args2.num_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d9955fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_w2v = TensorDataset(torch.tensor(x_ngram_train, dtype=torch.int64), torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "train_loader_w2v = DataLoader(train_dataset_w2v, batch_size=args2.batch_size, num_workers=args2.num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a6e3f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_w2v = TensorDataset(torch.tensor(x_ngram_val, dtype=torch.int64), torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "val_loader_w2v = DataLoader(val_dataset_w2v, batch_size=args2.batch_size, num_workers=args2.num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d7ab67f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "05139c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters \n",
    "\n",
    "args2.vocabulary_size = ngram_data_w2v.get_vocabulary_size()\n",
    "args2.d = 100\n",
    "args2.d_h = 200\n",
    "args2.dropout = 0.1\n",
    "\n",
    "# Train hyperparameters\n",
    "\n",
    "args2.lr = 2.3e-1\n",
    "args2.num_epochs = 100\n",
    "args2.patience = 20\n",
    "\n",
    "args2.lr_patience = 10\n",
    "args2.lr_factor = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "42ff5d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args2.savedir = \"model_w2v\"\n",
    "os.makedirs(args2.savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0a4f822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = NeuralLM(args2, embedding_matrix=ngram_data_w2v.embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6f334879",
   "metadata": {},
   "outputs": [],
   "source": [
    "args2.use_gpu = torch.cuda.is_available()\n",
    "if args2.use_gpu:\n",
    "\tmodel_w2v.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_w2v.parameters(), lr=args2.lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=args2.lr_patience, verbose=True, factor=args2.lr_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66e15d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.11865841806220095\n",
      "Epoch [1/100], Loss: 5.9706 - Val accuracy: 0.1691 - Epoch time : 13.97\n",
      "Train acc: 0.13365916566985647\n",
      "Epoch [2/100], Loss: 5.5267 - Val accuracy: 0.1762 - Epoch time : 13.89\n",
      "Train acc: 0.14068667763157894\n",
      "Epoch [3/100], Loss: 5.2930 - Val accuracy: 0.1792 - Epoch time : 13.79\n",
      "Train acc: 0.1450956937799043\n",
      "Epoch [4/100], Loss: 5.1120 - Val accuracy: 0.1571 - Epoch time : 14.04\n",
      "Train acc: 0.14803566088516745\n",
      "Epoch [5/100], Loss: 4.9449 - Val accuracy: 0.1317 - Epoch time : 13.96\n",
      "Train acc: 0.15063733552631578\n",
      "Epoch [6/100], Loss: 4.8028 - Val accuracy: 0.1528 - Epoch time : 13.93\n",
      "Train acc: 0.15492673444976077\n",
      "Epoch [7/100], Loss: 4.6760 - Val accuracy: 0.1408 - Epoch time : 13.72\n",
      "Train acc: 0.1594011662679426\n",
      "Epoch [8/100], Loss: 4.5592 - Val accuracy: 0.1764 - Epoch time : 14.24\n",
      "Train acc: 0.16508298444976077\n",
      "Epoch [9/100], Loss: 4.4591 - Val accuracy: 0.1240 - Epoch time : 13.97\n",
      "Train acc: 0.1712301883971292\n",
      "Epoch [10/100], Loss: 4.3756 - Val accuracy: 0.1241 - Epoch time : 13.96\n",
      "Train acc: 0.17708582535885167\n",
      "Epoch [11/100], Loss: 4.2985 - Val accuracy: 0.1246 - Epoch time : 13.99\n",
      "Train acc: 0.18104627691387562\n",
      "Epoch [12/100], Loss: 4.2308 - Val accuracy: 0.1766 - Epoch time : 14.11\n",
      "Train acc: 0.1872271232057416\n",
      "Epoch [13/100], Loss: 4.1806 - Val accuracy: 0.1611 - Epoch time : 14.24\n",
      "Train acc: 0.19436677631578947\n",
      "Epoch [14/100], Loss: 4.1190 - Val accuracy: 0.1258 - Epoch time : 13.86\n",
      "Train acc: 0.2003102571770335\n",
      "Epoch [15/100], Loss: 4.0690 - Val accuracy: 0.1139 - Epoch time : 13.97\n",
      "Train acc: 0.20498841208133972\n",
      "Epoch [16/100], Loss: 4.0187 - Val accuracy: 0.1220 - Epoch time : 13.98\n",
      "Train acc: 0.20802183014354067\n",
      "Epoch [17/100], Loss: 3.9815 - Val accuracy: 0.1535 - Epoch time : 13.94\n",
      "Train acc: 0.2143634120813397\n",
      "Epoch [18/100], Loss: 3.9486 - Val accuracy: 0.1700 - Epoch time : 13.96\n",
      "Train acc: 0.21719871411483255\n",
      "Epoch [19/100], Loss: 3.9193 - Val accuracy: 0.1603 - Epoch time : 13.85\n",
      "Train acc: 0.21987328050239235\n",
      "Epoch [20/100], Loss: 3.8917 - Val accuracy: 0.1540 - Epoch time : 13.90\n",
      "Train acc: 0.22355898624401913\n",
      "Epoch [21/100], Loss: 3.8611 - Val accuracy: 0.1238 - Epoch time : 13.93\n",
      "Train acc: 0.2271306818181818\n",
      "Epoch [22/100], Loss: 3.8363 - Val accuracy: 0.1721 - Epoch time : 14.00\n",
      "Train acc: 0.2299416866028708\n",
      "Epoch [23/100], Loss: 3.8154 - Val accuracy: 0.1435 - Epoch time : 14.01\n",
      "Train acc: 0.23345544258373205\n",
      "Epoch [24/100], Loss: 3.7925 - Val accuracy: 0.1741 - Epoch time : 13.95\n",
      "Train acc: 0.23202938098086123\n",
      "Epoch [25/100], Loss: 3.7850 - Val accuracy: 0.1223 - Epoch time : 14.24\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.1500e-01.\n",
      "Train acc: 0.23646456339712918\n",
      "Epoch [26/100], Loss: 3.7611 - Val accuracy: 0.1271 - Epoch time : 13.95\n",
      "Train acc: 0.2897727272727273\n",
      "Epoch [27/100], Loss: 3.3019 - Val accuracy: 0.1648 - Epoch time : 13.89\n",
      "Train acc: 0.2978076405502392\n",
      "Epoch [28/100], Loss: 3.2239 - Val accuracy: 0.1885 - Epoch time : 13.93\n",
      "Train acc: 0.3029885616028708\n",
      "Epoch [29/100], Loss: 3.2024 - Val accuracy: 0.1594 - Epoch time : 14.00\n",
      "Train acc: 0.3015251196172249\n",
      "Epoch [30/100], Loss: 3.1948 - Val accuracy: 0.1614 - Epoch time : 14.00\n",
      "Train acc: 0.30486692583732056\n",
      "Epoch [31/100], Loss: 3.1755 - Val accuracy: 0.1762 - Epoch time : 14.06\n",
      "Train acc: 0.30565004485645936\n",
      "Epoch [32/100], Loss: 3.1709 - Val accuracy: 0.1676 - Epoch time : 13.98\n",
      "Train acc: 0.3071377840909091\n",
      "Epoch [33/100], Loss: 3.1627 - Val accuracy: 0.1659 - Epoch time : 13.99\n",
      "Train acc: 0.3056201405502392\n",
      "Epoch [34/100], Loss: 3.1550 - Val accuracy: 0.1741 - Epoch time : 14.02\n",
      "Train acc: 0.3068275269138756\n",
      "Epoch [35/100], Loss: 3.1447 - Val accuracy: 0.1407 - Epoch time : 14.05\n",
      "Train acc: 0.30753588516746416\n",
      "Epoch [36/100], Loss: 3.1358 - Val accuracy: 0.1343 - Epoch time : 13.97\n",
      "Epoch 00037: reducing learning rate of group 0 to 5.7500e-02.\n",
      "Train acc: 0.31066836124401914\n",
      "Epoch [37/100], Loss: 3.1281 - Val accuracy: 0.1691 - Epoch time : 13.90\n",
      "Train acc: 0.3428173594497607\n",
      "Epoch [38/100], Loss: 2.9117 - Val accuracy: 0.1656 - Epoch time : 13.96\n",
      "Train acc: 0.3457834928229665\n",
      "Epoch [39/100], Loss: 2.8842 - Val accuracy: 0.1674 - Epoch time : 14.07\n",
      "Train acc: 0.34592180023923447\n",
      "Epoch [40/100], Loss: 2.8760 - Val accuracy: 0.1599 - Epoch time : 14.09\n",
      "Train acc: 0.3460731907894737\n",
      "Epoch [41/100], Loss: 2.8696 - Val accuracy: 0.1782 - Epoch time : 14.09\n",
      "Train acc: 0.3472936602870813\n",
      "Epoch [42/100], Loss: 2.8648 - Val accuracy: 0.1770 - Epoch time : 14.28\n",
      "Train acc: 0.3486823415071771\n",
      "Epoch [43/100], Loss: 2.8609 - Val accuracy: 0.1790 - Epoch time : 13.92\n",
      "Train acc: 0.34939069976076553\n",
      "Epoch [44/100], Loss: 2.8538 - Val accuracy: 0.1705 - Epoch time : 14.12\n",
      "Train acc: 0.349379485645933\n",
      "Epoch [45/100], Loss: 2.8508 - Val accuracy: 0.1649 - Epoch time : 14.06\n",
      "Train acc: 0.34875336423444975\n",
      "Epoch [46/100], Loss: 2.8517 - Val accuracy: 0.1771 - Epoch time : 13.93\n",
      "Train acc: 0.3491907147129186\n",
      "Epoch [47/100], Loss: 2.8488 - Val accuracy: 0.1888 - Epoch time : 14.21\n",
      "Epoch 00048: reducing learning rate of group 0 to 2.8750e-02.\n",
      "Train acc: 0.350390625\n",
      "Epoch [48/100], Loss: 2.8438 - Val accuracy: 0.1609 - Epoch time : 14.02\n",
      "Train acc: 0.3699928977272727\n",
      "Epoch [49/100], Loss: 2.7333 - Val accuracy: 0.1777 - Epoch time : 14.03\n",
      "Train acc: 0.37118906997607654\n",
      "Epoch [50/100], Loss: 2.7227 - Val accuracy: 0.1777 - Epoch time : 14.08\n",
      "Train acc: 0.3695275119617225\n",
      "Epoch [51/100], Loss: 2.7171 - Val accuracy: 0.1815 - Epoch time : 14.09\n",
      "Train acc: 0.37149932715311007\n",
      "Epoch [52/100], Loss: 2.7133 - Val accuracy: 0.1742 - Epoch time : 14.28\n",
      "Train acc: 0.37150867224880385\n",
      "Epoch [53/100], Loss: 2.7083 - Val accuracy: 0.1893 - Epoch time : 13.96\n",
      "Train acc: 0.371783418062201\n",
      "Epoch [54/100], Loss: 2.7094 - Val accuracy: 0.1850 - Epoch time : 14.15\n",
      "Train acc: 0.37432902212918656\n",
      "Epoch [55/100], Loss: 2.7005 - Val accuracy: 0.1911 - Epoch time : 14.20\n",
      "Train acc: 0.3715049342105263\n",
      "Epoch [56/100], Loss: 2.7066 - Val accuracy: 0.1914 - Epoch time : 14.23\n",
      "Train acc: 0.37185630980861245\n",
      "Epoch [57/100], Loss: 2.7028 - Val accuracy: 0.1789 - Epoch time : 14.01\n",
      "Train acc: 0.3741608104066985\n",
      "Epoch [58/100], Loss: 2.6936 - Val accuracy: 0.1804 - Epoch time : 14.00\n",
      "Epoch 00059: reducing learning rate of group 0 to 1.4375e-02.\n",
      "Train acc: 0.3743963068181818\n",
      "Epoch [59/100], Loss: 2.6968 - Val accuracy: 0.1830 - Epoch time : 14.15\n",
      "Train acc: 0.38431332236842103\n",
      "Epoch [60/100], Loss: 2.6402 - Val accuracy: 0.1870 - Epoch time : 13.96\n",
      "Train acc: 0.3862627093301435\n",
      "Epoch [61/100], Loss: 2.6317 - Val accuracy: 0.1822 - Epoch time : 14.04\n",
      "Train acc: 0.3852459629186603\n",
      "Epoch [62/100], Loss: 2.6355 - Val accuracy: 0.1843 - Epoch time : 14.02\n",
      "Train acc: 0.3856253738038278\n",
      "Epoch [63/100], Loss: 2.6313 - Val accuracy: 0.1803 - Epoch time : 14.01\n",
      "Train acc: 0.38529642643540674\n",
      "Epoch [64/100], Loss: 2.6306 - Val accuracy: 0.1812 - Epoch time : 14.03\n",
      "Train acc: 0.3862851375598086\n",
      "Epoch [65/100], Loss: 2.6296 - Val accuracy: 0.1779 - Epoch time : 14.04\n",
      "Train acc: 0.38675799940191385\n",
      "Epoch [66/100], Loss: 2.6264 - Val accuracy: 0.1811 - Epoch time : 14.11\n",
      "Train acc: 0.3858776913875598\n",
      "Epoch [67/100], Loss: 2.6270 - Val accuracy: 0.1824 - Epoch time : 14.00\n",
      "Train acc: 0.38727384868421055\n",
      "Epoch [68/100], Loss: 2.6264 - Val accuracy: 0.1838 - Epoch time : 13.98\n",
      "Train acc: 0.3853263307416268\n",
      "Epoch [69/100], Loss: 2.6305 - Val accuracy: 0.1769 - Epoch time : 13.91\n",
      "Epoch 00070: reducing learning rate of group 0 to 7.1875e-03.\n",
      "Train acc: 0.3867991178229665\n",
      "Epoch [70/100], Loss: 2.6211 - Val accuracy: 0.1791 - Epoch time : 14.17\n",
      "Train acc: 0.3929893092105263\n",
      "Epoch [71/100], Loss: 2.5932 - Val accuracy: 0.1836 - Epoch time : 14.16\n",
      "Train acc: 0.3926827900717703\n",
      "Epoch [72/100], Loss: 2.5923 - Val accuracy: 0.1882 - Epoch time : 14.11\n",
      "Train acc: 0.39292389354066987\n",
      "Epoch [73/100], Loss: 2.5910 - Val accuracy: 0.1885 - Epoch time : 13.86\n",
      "Train acc: 0.3927538127990431\n",
      "Epoch [74/100], Loss: 2.5896 - Val accuracy: 0.1912 - Epoch time : 14.08\n",
      "Train acc: 0.394761139354067\n",
      "Epoch [75/100], Loss: 2.5829 - Val accuracy: 0.1862 - Epoch time : 13.99\n",
      "Train acc: 0.3930435107655503\n",
      "Epoch [76/100], Loss: 2.5912 - Val accuracy: 0.1883 - Epoch time : 14.07\n",
      "Train acc: 0.394293884569378\n",
      "Epoch [77/100], Loss: 2.5879 - Val accuracy: 0.1859 - Epoch time : 14.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.3948938397129187\n",
      "Epoch [78/100], Loss: 2.5832 - Val accuracy: 0.1887 - Epoch time : 12.76\n",
      "Train acc: 0.39453872607655505\n",
      "Epoch [79/100], Loss: 2.5830 - Val accuracy: 0.1880 - Epoch time : 12.65\n",
      "Train acc: 0.39366589413875597\n",
      "Epoch [80/100], Loss: 2.5854 - Val accuracy: 0.1838 - Epoch time : 12.72\n",
      "Epoch 00081: reducing learning rate of group 0 to 3.5938e-03.\n",
      "Train acc: 0.3936434659090909\n",
      "Epoch [81/100], Loss: 2.5821 - Val accuracy: 0.1870 - Epoch time : 12.83\n",
      "Train acc: 0.3968376196172249\n",
      "Epoch [82/100], Loss: 2.5698 - Val accuracy: 0.1825 - Epoch time : 12.70\n",
      "Train acc: 0.397517942583732\n",
      "Epoch [83/100], Loss: 2.5692 - Val accuracy: 0.1843 - Epoch time : 12.76\n",
      "Train acc: 0.3983216208133971\n",
      "Epoch [84/100], Loss: 2.5663 - Val accuracy: 0.1839 - Epoch time : 12.81\n",
      "Train acc: 0.3977515699760765\n",
      "Epoch [85/100], Loss: 2.5689 - Val accuracy: 0.1891 - Epoch time : 12.70\n",
      "Train acc: 0.39917576255980863\n",
      "Epoch [86/100], Loss: 2.5668 - Val accuracy: 0.1856 - Epoch time : 12.89\n",
      "Train acc: 0.3970095693779904\n",
      "Epoch [87/100], Loss: 2.5687 - Val accuracy: 0.1868 - Epoch time : 12.71\n",
      "Train acc: 0.39764877392344494\n",
      "Epoch [88/100], Loss: 2.5637 - Val accuracy: 0.1869 - Epoch time : 13.12\n",
      "Train acc: 0.3975908343301435\n",
      "Epoch [89/100], Loss: 2.5689 - Val accuracy: 0.1872 - Epoch time : 12.75\n",
      "Train acc: 0.398123504784689\n",
      "Epoch [90/100], Loss: 2.5619 - Val accuracy: 0.1882 - Epoch time : 12.87\n",
      "Train acc: 0.39786931818181814\n",
      "Epoch [91/100], Loss: 2.5659 - Val accuracy: 0.1848 - Epoch time : 12.82\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.7969e-03.\n",
      "Train acc: 0.3977366178229665\n",
      "Epoch [92/100], Loss: 2.5645 - Val accuracy: 0.1885 - Epoch time : 12.80\n",
      "Train acc: 0.4004840759569378\n",
      "Epoch [93/100], Loss: 2.5523 - Val accuracy: 0.1839 - Epoch time : 12.70\n",
      "Train acc: 0.3989757775119617\n",
      "Epoch [94/100], Loss: 2.5593 - Val accuracy: 0.1858 - Epoch time : 12.86\n",
      "Train acc: 0.3996710526315789\n",
      "Epoch [95/100], Loss: 2.5545 - Val accuracy: 0.1873 - Epoch time : 12.95\n",
      "Train acc: 0.4004728618421053\n",
      "Epoch [96/100], Loss: 2.5545 - Val accuracy: 0.1909 - Epoch time : 12.89\n",
      "Train acc: 0.39934584330143535\n",
      "Epoch [97/100], Loss: 2.5553 - Val accuracy: 0.1866 - Epoch time : 12.79\n",
      "Train acc: 0.4008204994019139\n",
      "Epoch [98/100], Loss: 2.5509 - Val accuracy: 0.1854 - Epoch time : 12.89\n",
      "Train acc: 0.40170828349282295\n",
      "Epoch [99/100], Loss: 2.5508 - Val accuracy: 0.1895 - Epoch time : 12.86\n",
      "Train acc: 0.399923370215311\n",
      "Epoch [100/100], Loss: 2.5550 - Val accuracy: 0.1855 - Epoch time : 12.63\n",
      "--- 1374.5437705516815 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "for epoch in range(args2.num_epochs):\n",
    "\tepoch_start_time =  time.time()\n",
    "\tloss_epoch = []\n",
    "\ttraining_metric = []\n",
    "\tmodel_w2v.train()\n",
    "\n",
    "\tfor window_words, labels in train_loader_w2v:\n",
    "\n",
    "\t\t# if gpu available\n",
    "\t\tif args2.use_gpu:\n",
    "\t\t\twindow_words = window_words.cuda()\n",
    "\t\t\tlabels = labels.cuda()\n",
    "\n",
    "\t\t# forward pass\n",
    "\t\toutputs = model_w2v(window_words)\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\tloss_epoch.append(loss.item())\n",
    "\n",
    "\t\t# get_training metrics\n",
    "\t\ty_pred = get_preds(outputs)\n",
    "\t\ttgt = labels.cpu().numpy()\n",
    "\t\ttraining_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "\t\t# posteriormente, hacemos el backward y optimizamos\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t# get metric n training dataset\n",
    "\tmean_epoch_metric = np.mean(training_metric)\n",
    "\ttrain_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "\t# get metric in validation dataset\n",
    "\tmodel_w2v.eval()\n",
    "\ttuning_metric = model_eval(val_loader_w2v, model_w2v, gpu=args2.use_gpu)\n",
    "\tmetric_history.append(mean_epoch_metric)\n",
    "\n",
    "\t# update scheduler\n",
    "\tscheduler.step(tuning_metric)\n",
    "\n",
    "\t# chech for metric improvement\n",
    "\tis_improvement = tuning_metric > best_metric\n",
    "\tif is_improvement:\n",
    "\t\tvest_metric = tuning_metric\n",
    "\t\tn_no_improve = 0\n",
    "\telse:\n",
    "\t\tn_no_improve += 1\n",
    "\n",
    "\tsave_checkpoint(\n",
    "\t\t{\n",
    "\t\t\"epoch\" : epoch + 1, \n",
    "\t\t\"state_dict\" : model_w2v.state_dict(), \n",
    "\t\t\"optimizer\" : optimizer.state_dict(),\n",
    "\t\t\"scheduler\" : scheduler.state_dict(), \n",
    "\t\t\"best_metric\" : best_metric\n",
    "\t\t}, \n",
    "\t\tis_improvement, \n",
    "\t\targs2.savedir\n",
    "\t)\n",
    "\n",
    "\t# detener el modelo si no hay mejora\n",
    "\tif n_no_improve >= args2.patience:\n",
    "\t\tprint(\"No improvement. Breaking out of loop\")\n",
    "\t\tbreak\n",
    "\n",
    "\tprint(\"Train acc: {}\".format(mean_epoch_metric))\n",
    "\tprint(\"Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time : {:.2f}\".format(epoch + 1, args2.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time() - epoch_start_time)))\n",
    "\n",
    "print(\"--- %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "76659ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(5000, 100)\n",
       "  (fc1): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=5000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_w2v = NeuralLM(args2)\n",
    "best_model_w2v.load_state_dict(torch.load(\"model_w2v/model_best.pt\", map_location=torch.device(\"cpu\"))[\"state_dict\"])\n",
    "best_model_w2v.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "302eb27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> <s> porque <unk> <unk> no valgo de <unk> los <unk> <unk> <unk> lo vuelvo a subir pinche <unk> ac√° </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"<s> <s> <s>\"\n",
    "\n",
    "# el tokenizer es la funci√≥n \"list\"\n",
    "print(generate_sentence(best_model_w2v, initial_tokens, tokenizer.tokenize, ngram_data_w2v, max_gen_tokens=100, join_char=\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a79c2ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> hola cuando <unk> <unk> pero como hoy un <unk> <unk> ‚Äù <unk> que holanda no le <unk> no quiero pedo con <unk> un <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"<s> <s> hola\"\n",
    "\n",
    "print(generate_sentence(best_model_w2v, initial_tokens, tokenizer.tokenize, ngram_data_w2v, max_gen_tokens=100, join_char=\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0a805304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -5.535452\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model_w2v, \"clase de lenguage natural\", ngram_data_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "24767efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -29.040985\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model_w2v, \"messi es el mejor jugador\", ngram_data_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "38caa27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -30.505259\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model_w2v, \"ronaldo es el mejor jugador\", ngram_data_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4f0e7751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -31.871273\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model_w2v, \"amlo es un mal presidente\", ngram_data_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bcd1a06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -52.662636\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model_w2v, \"mexico le va a ganar a argentina\", ngram_data_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f0b3551a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-29.469181 ganar le va a argentina a mexico\n",
      "-29.469181 ganar le va a argentina a mexico\n",
      "-29.807985 argentina ganar le va a a mexico\n",
      "-29.807985 argentina ganar le va a a mexico\n",
      "-33.405617 le ganar va a argentina a mexico\n",
      "--------------------------------------------------\n",
      "-124.671486 le a va mexico ganar a argentina\n",
      "-126.85434 le a va mexico argentina ganar a\n",
      "-126.85434 le a va mexico argentina ganar a\n",
      "-128.51643 le a va mexico ganar argentina a\n",
      "-128.51643 le a va mexico ganar argentina a\n"
     ]
    }
   ],
   "source": [
    "word_list = \"mexico le va a ganar a argentina\"\n",
    "perms = [\" \".join(perm) for perm in permutations(word_list.split(\" \"))]\n",
    "\n",
    "for p, t in sorted([(log_likelihood(best_model_w2v, text, ngram_data_w2v), text) for text in perms], reverse=True)[:5]:\n",
    "\tprint(p, t)\n",
    "\n",
    "    \n",
    "print(\"-\" * 50)\n",
    "\n",
    "for p, t in sorted([(log_likelihood(best_model_w2v, text, ngram_data_w2v), text) for text in perms], reverse=True)[-5:]:\n",
    "\tprint(p, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d8ead2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P = 0\n"
     ]
    }
   ],
   "source": [
    "# Tengo un problema de underflow con la perplejidad, no lo puede solucionar, pero creo que lo hace bien\n",
    "print(\"P = {}\".format(perplexity(best_model_w2v, \" \".join(x_val), ngram_data_w2v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401e0ea2",
   "metadata": {},
   "source": [
    "### 3 A partir del modelo anterior haga un modelo de lenguaje que integre una conexi√≥n directa de la capa de embeddings hac√≠a la salida, justo como lo propon√≠a Bengio. Discuta sobre las diferencias en el proceso de entrenamiento y la perplejidad respecto al modelo anterior y el visto en clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0fab172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args3 = Namespace()\n",
    "\n",
    "# (Hyper)par√°metros\n",
    "\n",
    "args3.N               = args2.N\n",
    "args3.batch_size      = args2.batch_size\n",
    "args3.num_workers     = args2.num_workers\n",
    "args3.vocabulary_size = args2.vocabulary_size\n",
    "args3.d               = args2.d\n",
    "args3.d_h             = args2.d_h\n",
    "args3.dropout         = args2.dropout\n",
    "args3.lr              = args2.lr\n",
    "args3.num_epochs      = args2.num_epochs\n",
    "args3.patience        = args2.patience\n",
    "args3.lr_patience     = args2.lr_patience\n",
    "args3.lr_factor       = args2.lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "50d04899",
   "metadata": {},
   "outputs": [],
   "source": [
    "args3.savedir = \"model_w2v_with_direct_conxn\"\n",
    "os.makedirs(args3.savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "67fc3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_2 = NeuralLM(args3, embedding_matrix=ngram_data_w2v.embedding_matrix, direct_emb_to_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "70a81d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args3.use_gpu = torch.cuda.is_available()\n",
    "if args3.use_gpu:\n",
    "\tmodel_w2v_2.cuda()\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_w2v_2.parameters(), lr=args3.lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=args3.lr_patience, verbose=True, factor=args3.lr_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4473ddaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.0852833433014354\n",
      "Epoch [1/100], Loss: 8.7656 - Val accuracy: 0.0685 - Epoch time : 24.51\n",
      "Train acc: 0.09734973086124403\n",
      "Epoch [2/100], Loss: 8.2779 - Val accuracy: 0.0698 - Epoch time : 24.52\n",
      "Train acc: 0.11349992523923445\n",
      "Epoch [3/100], Loss: 7.9537 - Val accuracy: 0.1039 - Epoch time : 24.58\n",
      "Train acc: 0.13167052930622009\n",
      "Epoch [4/100], Loss: 7.7698 - Val accuracy: 0.0935 - Epoch time : 24.99\n",
      "Train acc: 0.14615542763157893\n",
      "Epoch [5/100], Loss: 7.5895 - Val accuracy: 0.0772 - Epoch time : 24.62\n",
      "Train acc: 0.15960115131578945\n",
      "Epoch [6/100], Loss: 7.4453 - Val accuracy: 0.0841 - Epoch time : 24.73\n",
      "Train acc: 0.16911072069377991\n",
      "Epoch [7/100], Loss: 7.3871 - Val accuracy: 0.1472 - Epoch time : 24.95\n",
      "Train acc: 0.18089675538277514\n",
      "Epoch [8/100], Loss: 7.3039 - Val accuracy: 0.0573 - Epoch time : 24.87\n",
      "Train acc: 0.1897203947368421\n",
      "Epoch [9/100], Loss: 7.1717 - Val accuracy: 0.0934 - Epoch time : 24.64\n",
      "Train acc: 0.19794781698564592\n",
      "Epoch [10/100], Loss: 7.1044 - Val accuracy: 0.0863 - Epoch time : 24.70\n",
      "Train acc: 0.20416230562200957\n",
      "Epoch [11/100], Loss: 7.0396 - Val accuracy: 0.1133 - Epoch time : 24.70\n",
      "Train acc: 0.2125336423444976\n",
      "Epoch [12/100], Loss: 6.9383 - Val accuracy: 0.0666 - Epoch time : 24.48\n",
      "Train acc: 0.2172267494019139\n",
      "Epoch [13/100], Loss: 6.9790 - Val accuracy: 0.1017 - Epoch time : 24.88\n",
      "Train acc: 0.2205293062200957\n",
      "Epoch [14/100], Loss: 6.9659 - Val accuracy: 0.0675 - Epoch time : 25.05\n",
      "Train acc: 0.2269998504784689\n",
      "Epoch [15/100], Loss: 6.8483 - Val accuracy: 0.0793 - Epoch time : 24.93\n",
      "Train acc: 0.23214712918660285\n",
      "Epoch [16/100], Loss: 6.8234 - Val accuracy: 0.0582 - Epoch time : 24.67\n",
      "Train acc: 0.23567770633971294\n",
      "Epoch [17/100], Loss: 6.8163 - Val accuracy: 0.0587 - Epoch time : 24.64\n",
      "Train acc: 0.24093338815789472\n",
      "Epoch [18/100], Loss: 6.8023 - Val accuracy: 0.0925 - Epoch time : 24.69\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.1500e-01.\n",
      "Train acc: 0.24426584928229667\n",
      "Epoch [19/100], Loss: 6.7373 - Val accuracy: 0.0777 - Epoch time : 25.06\n",
      "Train acc: 0.31969946172248803\n",
      "Epoch [20/100], Loss: 4.0290 - Val accuracy: 0.1171 - Epoch time : 24.91\n",
      "Train acc: 0.33216955741626797\n",
      "Epoch [21/100], Loss: 3.8183 - Val accuracy: 0.1038 - Epoch time : 24.89\n",
      "Train acc: 0.3383074162679426\n",
      "Epoch [22/100], Loss: 3.7553 - Val accuracy: 0.0843 - Epoch time : 24.91\n",
      "Train acc: 0.3367804276315789\n",
      "Epoch [23/100], Loss: 3.7500 - Val accuracy: 0.1297 - Epoch time : 24.88\n",
      "Train acc: 0.3408567583732058\n",
      "Epoch [24/100], Loss: 3.7169 - Val accuracy: 0.0953 - Epoch time : 24.94\n",
      "Train acc: 0.34143054724880384\n",
      "Epoch [25/100], Loss: 3.6966 - Val accuracy: 0.1370 - Epoch time : 24.74\n",
      "Train acc: 0.3442265998803828\n",
      "Epoch [26/100], Loss: 3.6928 - Val accuracy: 0.1043 - Epoch time : 25.09\n",
      "Train acc: 0.34572181519138756\n",
      "Epoch [27/100], Loss: 3.6678 - Val accuracy: 0.0822 - Epoch time : 24.76\n",
      "Train acc: 0.3462862589712919\n",
      "Epoch [28/100], Loss: 3.6779 - Val accuracy: 0.0978 - Epoch time : 25.11\n",
      "Train acc: 0.3483309659090909\n",
      "Epoch [29/100], Loss: 3.6477 - Val accuracy: 0.1013 - Epoch time : 25.33\n",
      "Epoch 00030: reducing learning rate of group 0 to 5.7500e-02.\n",
      "Train acc: 0.35107842404306216\n",
      "Epoch [30/100], Loss: 3.6248 - Val accuracy: 0.0936 - Epoch time : 24.53\n",
      "Train acc: 0.4092722039473684\n",
      "Epoch [31/100], Loss: 2.7531 - Val accuracy: 0.1325 - Epoch time : 24.83\n",
      "Train acc: 0.4142624850478469\n",
      "Epoch [32/100], Loss: 2.6893 - Val accuracy: 0.0977 - Epoch time : 24.60\n",
      "Train acc: 0.41678752990430623\n",
      "Epoch [33/100], Loss: 2.6661 - Val accuracy: 0.1115 - Epoch time : 24.94\n",
      "Train acc: 0.4164567135167464\n",
      "Epoch [34/100], Loss: 2.6546 - Val accuracy: 0.1567 - Epoch time : 24.79\n",
      "Train acc: 0.41745290071770336\n",
      "Epoch [35/100], Loss: 2.6474 - Val accuracy: 0.1668 - Epoch time : 24.93\n",
      "Train acc: 0.41849768241626795\n",
      "Epoch [36/100], Loss: 2.6442 - Val accuracy: 0.1242 - Epoch time : 24.85\n",
      "Train acc: 0.4197013307416268\n",
      "Epoch [37/100], Loss: 2.6339 - Val accuracy: 0.1315 - Epoch time : 25.42\n",
      "Train acc: 0.4199648624401914\n",
      "Epoch [38/100], Loss: 2.6348 - Val accuracy: 0.1169 - Epoch time : 26.61\n",
      "Train acc: 0.4212114982057416\n",
      "Epoch [39/100], Loss: 2.6284 - Val accuracy: 0.1115 - Epoch time : 25.53\n",
      "Train acc: 0.42034614234449763\n",
      "Epoch [40/100], Loss: 2.6322 - Val accuracy: 0.1441 - Epoch time : 25.16\n",
      "Epoch 00041: reducing learning rate of group 0 to 2.8750e-02.\n",
      "Train acc: 0.4220861991626794\n",
      "Epoch [41/100], Loss: 2.6158 - Val accuracy: 0.1170 - Epoch time : 25.20\n",
      "Train acc: 0.4610272129186603\n",
      "Epoch [42/100], Loss: 2.3316 - Val accuracy: 0.1380 - Epoch time : 24.85\n",
      "Train acc: 0.46145521830143543\n",
      "Epoch [43/100], Loss: 2.3129 - Val accuracy: 0.1530 - Epoch time : 25.11\n",
      "Train acc: 0.46166641746411485\n",
      "Epoch [44/100], Loss: 2.3082 - Val accuracy: 0.1553 - Epoch time : 24.96\n",
      "Train acc: 0.46433911483253587\n",
      "Epoch [45/100], Loss: 2.3011 - Val accuracy: 0.1255 - Epoch time : 25.22\n",
      "Train acc: 0.46444191088516745\n",
      "Epoch [46/100], Loss: 2.2949 - Val accuracy: 0.1333 - Epoch time : 25.03\n",
      "Train acc: 0.46442135167464116\n",
      "Epoch [47/100], Loss: 2.2948 - Val accuracy: 0.1473 - Epoch time : 24.92\n",
      "Train acc: 0.4655577153110048\n",
      "Epoch [48/100], Loss: 2.2939 - Val accuracy: 0.1412 - Epoch time : 24.94\n",
      "Train acc: 0.46537642045454547\n",
      "Epoch [49/100], Loss: 2.2935 - Val accuracy: 0.1477 - Epoch time : 25.04\n",
      "Train acc: 0.465157745215311\n",
      "Epoch [50/100], Loss: 2.2900 - Val accuracy: 0.1615 - Epoch time : 24.91\n",
      "Train acc: 0.46583433014354064\n",
      "Epoch [51/100], Loss: 2.2899 - Val accuracy: 0.1553 - Epoch time : 25.07\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.4375e-02.\n",
      "Train acc: 0.4640232505980861\n",
      "Epoch [52/100], Loss: 2.2847 - Val accuracy: 0.1528 - Epoch time : 24.84\n",
      "Train acc: 0.4859711423444976\n",
      "Epoch [53/100], Loss: 2.1783 - Val accuracy: 0.1420 - Epoch time : 24.70\n",
      "Train acc: 0.48725889653110044\n",
      "Epoch [54/100], Loss: 2.1690 - Val accuracy: 0.1529 - Epoch time : 25.21\n",
      "Train acc: 0.4883784389952153\n",
      "Epoch [55/100], Loss: 2.1667 - Val accuracy: 0.1512 - Epoch time : 25.17\n",
      "Train acc: 0.4889223235645933\n",
      "Epoch [56/100], Loss: 2.1644 - Val accuracy: 0.1626 - Epoch time : 24.81\n",
      "Train acc: 0.4898867374401914\n",
      "Epoch [57/100], Loss: 2.1614 - Val accuracy: 0.1598 - Epoch time : 24.84\n",
      "Train acc: 0.4886531848086125\n",
      "Epoch [58/100], Loss: 2.1619 - Val accuracy: 0.1566 - Epoch time : 24.92\n",
      "Train acc: 0.4898942135167464\n",
      "Epoch [59/100], Loss: 2.1615 - Val accuracy: 0.1471 - Epoch time : 24.84\n",
      "Train acc: 0.4897559061004785\n",
      "Epoch [60/100], Loss: 2.1603 - Val accuracy: 0.1511 - Epoch time : 25.02\n",
      "Train acc: 0.48827564294258374\n",
      "Epoch [61/100], Loss: 2.1605 - Val accuracy: 0.1450 - Epoch time : 24.57\n",
      "Train acc: 0.49036707535885166\n",
      "Epoch [62/100], Loss: 2.1557 - Val accuracy: 0.1572 - Epoch time : 24.61\n",
      "Epoch 00063: reducing learning rate of group 0 to 7.1875e-03.\n",
      "Train acc: 0.488886812200957\n",
      "Epoch [63/100], Loss: 2.1576 - Val accuracy: 0.1602 - Epoch time : 24.99\n",
      "Train acc: 0.49977384868421054\n",
      "Epoch [64/100], Loss: 2.1080 - Val accuracy: 0.1654 - Epoch time : 24.63\n",
      "Train acc: 0.5008485346889953\n",
      "Epoch [65/100], Loss: 2.1058 - Val accuracy: 0.1676 - Epoch time : 24.88\n",
      "Train acc: 0.5017400568181818\n",
      "Epoch [66/100], Loss: 2.1017 - Val accuracy: 0.1645 - Epoch time : 24.71\n",
      "Train acc: 0.5024297248803827\n",
      "Epoch [67/100], Loss: 2.1001 - Val accuracy: 0.1562 - Epoch time : 24.91\n",
      "Train acc: 0.5015213815789474\n",
      "Epoch [68/100], Loss: 2.1007 - Val accuracy: 0.1606 - Epoch time : 25.19\n",
      "Train acc: 0.501814817583732\n",
      "Epoch [69/100], Loss: 2.0977 - Val accuracy: 0.1636 - Epoch time : 25.16\n",
      "Train acc: 0.5019923744019138\n",
      "Epoch [70/100], Loss: 2.0966 - Val accuracy: 0.1644 - Epoch time : 25.29\n",
      "Train acc: 0.5018951854066986\n",
      "Epoch [71/100], Loss: 2.1006 - Val accuracy: 0.1694 - Epoch time : 24.74\n",
      "Train acc: 0.5013998953349282\n",
      "Epoch [72/100], Loss: 2.0948 - Val accuracy: 0.1647 - Epoch time : 26.00\n",
      "Train acc: 0.5021026465311005\n",
      "Epoch [73/100], Loss: 2.0981 - Val accuracy: 0.1672 - Epoch time : 25.16\n",
      "Epoch 00074: reducing learning rate of group 0 to 3.5938e-03.\n",
      "Train acc: 0.5017475328947368\n",
      "Epoch [74/100], Loss: 2.0968 - Val accuracy: 0.1632 - Epoch time : 24.93\n",
      "Train acc: 0.507728394138756\n",
      "Epoch [75/100], Loss: 2.0716 - Val accuracy: 0.1658 - Epoch time : 24.71\n",
      "Train acc: 0.5073975777511962\n",
      "Epoch [76/100], Loss: 2.0719 - Val accuracy: 0.1695 - Epoch time : 24.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.5084741327751197\n",
      "Epoch [77/100], Loss: 2.0722 - Val accuracy: 0.1693 - Epoch time : 24.96\n",
      "Train acc: 0.5069882625598087\n",
      "Epoch [78/100], Loss: 2.0703 - Val accuracy: 0.1565 - Epoch time : 24.98\n",
      "Train acc: 0.5107562051435407\n",
      "Epoch [79/100], Loss: 2.0656 - Val accuracy: 0.1668 - Epoch time : 25.05\n",
      "Train acc: 0.5091918361244019\n",
      "Epoch [80/100], Loss: 2.0684 - Val accuracy: 0.1652 - Epoch time : 24.72\n",
      "Train acc: 0.5082834928229665\n",
      "Epoch [81/100], Loss: 2.0675 - Val accuracy: 0.1631 - Epoch time : 24.94\n",
      "Train acc: 0.5082984449760766\n",
      "Epoch [82/100], Loss: 2.0698 - Val accuracy: 0.1650 - Epoch time : 25.10\n",
      "Train acc: 0.5095020933014355\n",
      "Epoch [83/100], Loss: 2.0679 - Val accuracy: 0.1602 - Epoch time : 25.00\n",
      "Train acc: 0.5082292912679426\n",
      "Epoch [84/100], Loss: 2.0699 - Val accuracy: 0.1683 - Epoch time : 25.11\n",
      "Epoch 00085: reducing learning rate of group 0 to 1.7969e-03.\n",
      "Train acc: 0.5081208881578947\n",
      "Epoch [85/100], Loss: 2.0679 - Val accuracy: 0.1714 - Epoch time : 24.70\n",
      "Train acc: 0.5124551435406699\n",
      "Epoch [86/100], Loss: 2.0558 - Val accuracy: 0.1670 - Epoch time : 25.52\n",
      "Train acc: 0.5125747607655502\n",
      "Epoch [87/100], Loss: 2.0539 - Val accuracy: 0.1662 - Epoch time : 24.93\n",
      "Train acc: 0.5136457087320574\n",
      "Epoch [88/100], Loss: 2.0518 - Val accuracy: 0.1672 - Epoch time : 24.99\n",
      "Train acc: 0.5118963068181818\n",
      "Epoch [89/100], Loss: 2.0542 - Val accuracy: 0.1684 - Epoch time : 25.11\n",
      "Train acc: 0.5125953199760765\n",
      "Epoch [90/100], Loss: 2.0512 - Val accuracy: 0.1673 - Epoch time : 25.08\n",
      "Train acc: 0.5108963815789473\n",
      "Epoch [91/100], Loss: 2.0546 - Val accuracy: 0.1672 - Epoch time : 24.75\n",
      "Train acc: 0.5125093450956938\n",
      "Epoch [92/100], Loss: 2.0539 - Val accuracy: 0.1664 - Epoch time : 25.43\n",
      "Train acc: 0.5122159090909091\n",
      "Epoch [93/100], Loss: 2.0523 - Val accuracy: 0.1691 - Epoch time : 25.13\n",
      "Train acc: 0.5117411782296651\n",
      "Epoch [94/100], Loss: 2.0537 - Val accuracy: 0.1692 - Epoch time : 24.88\n",
      "Train acc: 0.5116122159090909\n",
      "Epoch [95/100], Loss: 2.0557 - Val accuracy: 0.1694 - Epoch time : 24.99\n",
      "Epoch 00096: reducing learning rate of group 0 to 8.9844e-04.\n",
      "Train acc: 0.5124327153110048\n",
      "Epoch [96/100], Loss: 2.0517 - Val accuracy: 0.1665 - Epoch time : 24.94\n",
      "Train acc: 0.5147502990430622\n",
      "Epoch [97/100], Loss: 2.0459 - Val accuracy: 0.1678 - Epoch time : 25.22\n",
      "Train acc: 0.5136382326555025\n",
      "Epoch [98/100], Loss: 2.0448 - Val accuracy: 0.1663 - Epoch time : 24.94\n",
      "Train acc: 0.5128270783492823\n",
      "Epoch [99/100], Loss: 2.0487 - Val accuracy: 0.1680 - Epoch time : 25.13\n",
      "Train acc: 0.5144830293062201\n",
      "Epoch [100/100], Loss: 2.0442 - Val accuracy: 0.1649 - Epoch time : 25.89\n",
      "--- 2496.4375262260437 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "for epoch in range(args3.num_epochs):\n",
    "\tepoch_start_time = time.time()\n",
    "\tloss_epoch = []\n",
    "\ttraining_metric = []\n",
    "\tmodel_w2v_2.train()\n",
    "\n",
    "\tfor window_words, labels in train_loader_w2v:\n",
    "\n",
    "\t\t# if gpu available\n",
    "\t\tif args3.use_gpu:\n",
    "\t\t\twindow_words = window_words.cuda()\n",
    "\t\t\tlabels = labels.cuda()\n",
    "\n",
    "\t\t# forward pass\n",
    "\t\toutputs = model_w2v_2(window_words)\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\tloss_epoch.append(loss.item())\n",
    "\n",
    "\t\t# get_training metrics\n",
    "\t\ty_pred = get_preds(outputs)\n",
    "\t\ttgt = labels.cpu().numpy()\n",
    "\t\ttraining_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "\t\t# posteriormente, hacemos el backward y optimizamos\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t# get metric n training dataset\n",
    "\tmean_epoch_metric = np.mean(training_metric)\n",
    "\ttrain_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "\t# get metric in validation dataset\n",
    "\tmodel_w2v_2.eval()\n",
    "\ttuning_metric = model_eval(val_loader_w2v, model_w2v_2, gpu=args3.use_gpu)\n",
    "\tmetric_history.append(mean_epoch_metric)\n",
    "\n",
    "\t# update scheduler\n",
    "\tscheduler.step(tuning_metric)\n",
    "\n",
    "\t# chech for metric improvement\n",
    "\tis_improvement = tuning_metric > best_metric\n",
    "\tif is_improvement:\n",
    "\t\tvest_metric = tuning_metric\n",
    "\t\tn_no_improve = 0\n",
    "\telse:\n",
    "\t\tn_no_improve += 1\n",
    "\n",
    "\tsave_checkpoint(\n",
    "\t\t{\n",
    "\t\t\"epoch\" : epoch + 1, \n",
    "\t\t\"state_dict\" : model_w2v_2.state_dict(), \n",
    "\t\t\"optimizer\" : optimizer.state_dict(),\n",
    "\t\t\"scheduler\" : scheduler.state_dict(), \n",
    "\t\t\"best_metric\" : best_metric\n",
    "\t\t}, \n",
    "\t\tis_improvement, \n",
    "\t\targs3.savedir\n",
    "\t)\n",
    "\n",
    "\t# detener el modelo si no hay mejora\n",
    "\tif n_no_improve >= args3.patience:\n",
    "\t\tprint(\"No improvement. Breaking out of loop\")\n",
    "\t\tbreak\n",
    "\n",
    "\tprint(\"Train acc: {}\".format(mean_epoch_metric))\n",
    "\tprint(\"Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time : {:.2f}\".format(epoch + 1, args3.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time() - epoch_start_time)))\n",
    "\n",
    "print(\"--- %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f940aa9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(5000, 100)\n",
       "  (fc1): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=500, out_features=5000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_w2v_2 = NeuralLM(args3, direct_emb_to_output=True)\n",
    "best_model_w2v_2.load_state_dict(torch.load(\"model_w2v_with_direct_conxn/model_best.pt\", map_location=torch.device(\"cpu\"))[\"state_dict\"])\n",
    "best_model_w2v_2.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3dd96a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amlo es un pendejo maric√≥n <unk> quien vergas hace ra√∫l mejor ‚Ä¶ vas a ir a dormir un <unk> <unk> por qu√© pala ver a este par de semana era como de sopa de letras <unk> las ratas y putas como <unk> ‚òπ Ô∏è </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"amlo es un\"\n",
    "\n",
    "# el tokenizer es la funci√≥n \"list\"\n",
    "print(generate_sentence(best_model_w2v_2, initial_tokens, tokenizer.tokenize, ngram_data_w2v, max_gen_tokens=100, join_char=\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "01444a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P = 0\n"
     ]
    }
   ],
   "source": [
    "# Tengo un problema de underflow con la perplejidad, no lo puede solucionar, pero creo que lo hace bien\n",
    "print(\"P = {}\".format(perplexity(best_model_w2v_2, \" \".join(x_val), ngram_data_w2v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bcf2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe6e7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (main, Nov 24 2022, 19:45:47) [GCC 12.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
