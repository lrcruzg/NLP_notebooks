{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7323a8e",
   "metadata": {},
   "source": [
    "###  Luis Ricardo Cruz García\n",
    "#### Procesamiento de Lenguaje Natural\n",
    "\n",
    "#### Tarea 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb9725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from typing import Tuple\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams, FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd5ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25c28508",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(\"mex_train.txt\", sep=\"\\r\\n\", engine=\"python\", header=None).loc[:, 0].values.tolist()\n",
    "x_val   = pd.read_csv(\"mex_val.txt\"  , sep=\"\\r\\n\", engine=\"python\", header=None).loc[:, 0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "247466d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# esta class es escencialmente idéntica la vista en clase, sólo le agregué unos pequeños cambios para que \n",
    "# pudiera ser utilizada en todos los casos que se pide en la tarea\n",
    "class NgramData():\n",
    "\tdef __init__(self, \n",
    "\t\t\t\tN: int,                          # número del N-grama\n",
    "\t\t\t\tmax_vocabulary_size: int = 5000, # tamaño máximo del vocabulario\n",
    "\t\t\t\ttokenizer = None,                # tokenizador\n",
    "\t\t\t\tnumeric = False,                 # acepta o no tokens numéricos\n",
    "\t\t\t\tembeddings_model = None,         # array de word-representations\n",
    "\t\t\t\tembedding_words = None,          # palabras de las cuales tenemos un embedding\n",
    "\t\t\t\tword_to_emb_id = None):          # diccionario de la palabra al índice en el array \"embeddings_model\"\n",
    "\n",
    "\t\tself.tokenizer = tokenizer if tokenizer else self.default_tokenizer()\n",
    "\t\tself.punct = set([\".\", \",\", \";\", \":\", \"^\", \"!\", \"¡\", \"¿\", \"?\", \"\\'\", \"*\", \"<url>\", \"@usuario\"])\n",
    "\t\tself.embeddings_model = embeddings_model\n",
    "\t\tself.embedding_words = embedding_words\n",
    "\t\tself.word_to_emb_id = word_to_emb_id\n",
    "\t\tself.max_vocabulary_size = max_vocabulary_size\n",
    "\t\tself.numeric = numeric\n",
    "\t\tself.N = N\n",
    "\t\tself.UNK = \"<unk>\"\n",
    "\t\tself.SOS = \"<s>\"\n",
    "\t\tself.EOS = \"</s>\"\n",
    "\n",
    "\tdef remove_word(self, word:str) -> bool:\n",
    "\t\tlo_word = word.lower()\n",
    "\t\t# se pueden aceptar o no tokens numéricos\n",
    "\t\tif not self.numeric:\n",
    "\t\t\treturn lo_word in self.punct or lo_word.isnumeric()\n",
    "\t\telse:\n",
    "\t\t\treturn lo_word in self.punct\n",
    "\n",
    "\tdef get_vocabulary(self, corpus: list) -> set:\n",
    "\t\tfreq_dist = FreqDist([word.lower() for sentence in corpus for word in self.tokenizer(sentence) if not self.remove_word(word)])\n",
    "\t\tsorted_words = self.sortFreqDict(freq_dist)[:self.max_vocabulary_size - 3]\n",
    "\t\treturn set(sorted_words)\n",
    "\n",
    "\tdef build_embedding_matrix(self):\n",
    "\t\tembedding_dimension = self.embeddings_model.shape[1]\n",
    "\t\tself.embedding_matrix = np.zeros((len(self.vocabulary), embedding_dimension))\n",
    "\t\t\n",
    "\t\tfor i, word in enumerate(self.vocabulary):\n",
    "\t\t\tif word in embedding_words:\n",
    "\t\t\t\tself.embedding_matrix[i] = self.embeddings_model[self.word_to_emb_id[word]]\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dimension, ))\n",
    "\n",
    "\tdef fit(self, corpus:list) -> None:\n",
    "\t\tself.vocabulary = self.get_vocabulary(corpus)\n",
    "\t\tself.vocabulary.add(self.UNK)\n",
    "\t\tself.vocabulary.add(self.SOS)\n",
    "\t\tself.vocabulary.add(self.EOS)\n",
    "\n",
    "\t\tself.w2id = {}\n",
    "\t\tself.id2w = {}\n",
    "\n",
    "\t\tid_word = 0\n",
    "\t\tfor doc in corpus:\n",
    "\t\t\tfor word in self.tokenizer(doc):\n",
    "\t\t\t\tword_lower = word.lower()\n",
    "\t\t\t\tif word_lower in self.vocabulary and word_lower not in self.w2id:\n",
    "\t\t\t\t\tself.w2id[word_lower] = id_word\n",
    "\t\t\t\t\tself.id2w[id_word]    = word_lower\n",
    "\t\t\t\t\tid_word += 1\n",
    "\n",
    "\t\t# agregar tokens especiales\n",
    "\t\tself.w2id.update(\n",
    "\t\t\t{\n",
    "\t\t\t\tself.UNK: id_word,\n",
    "\t\t\t\tself.SOS: id_word + 1,\n",
    "\t\t\t\tself.EOS: id_word + 2\n",
    "\n",
    "\t\t\t}\n",
    "\t\t)\n",
    "\n",
    "\t\tself.id2w.update(\n",
    "\t\t\t{\n",
    "\t\t\t\tid_word: self.UNK,\n",
    "\t\t\t\tid_word + 1: self.SOS,\n",
    "\t\t\t\tid_word + 2: self.EOS \n",
    "\n",
    "\t\t\t}\n",
    "\t\t)\n",
    "\n",
    "\t\t# si nos pasan un embeddings_model, creamos la matriz de embeddings\n",
    "\t\tif self.embeddings_model is not None:\n",
    "\t\t\tself.build_embedding_matrix()\n",
    "\n",
    "\tdef transform(self, corpus : list) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\t\tx_ngrams = []\n",
    "\t\ty = []\n",
    "\t\tfor doc in corpus:\n",
    "\t\t\tdoc_ngram = self.get_ngram_doc(doc)\n",
    "\t\t\tfor words_window in doc_ngram:\n",
    "\t\t\t\twords_window_ids = [self.w2id[word] for word in words_window]\n",
    "\t\t\t\tx_ngrams.append(list(words_window_ids[:-1]))\n",
    "\t\t\t\ty.append(words_window_ids[-1])\n",
    "\n",
    "\t\treturn array(x_ngrams), array(y)\n",
    "\n",
    "\tdef get_ngram_doc(self, doc : str) -> list:\n",
    "\t\tdoc_tokens = self.tokenizer(doc)\n",
    "\t\tdoc_tokens = [word.lower() for word in doc_tokens]\n",
    "\t\tdoc_tokens = self.replace_unk(doc_tokens)\n",
    "\t\tdoc_tokens = [self.SOS] * (self.N - 1) + doc_tokens + [self.EOS]\n",
    "\t\treturn list(ngrams(doc_tokens, self.N))\n",
    "\n",
    "\tdef replace_unk(self, doc_tokens):\n",
    "\t\tfor i, token in enumerate(doc_tokens):\n",
    "\t\t\tif token.lower() not in self.vocabulary:\n",
    "\t\t\t\tdoc_tokens[i] = self.UNK\n",
    "\n",
    "\t\treturn doc_tokens\n",
    "\n",
    "\tdef sortFreqDict(self, fdist_dict):\n",
    "\t\taux = list(fdist_dict.keys())\n",
    "\t\taux.sort(key=lambda char : fdist_dict[char], reverse=True)\n",
    "\t\treturn aux\n",
    "\n",
    "\tdef default_tokenizer(self, doc : str) -> list:\n",
    "\t\treturn doc.split(\" \")\n",
    "\n",
    "\tdef get_vocabulary_size(self) -> int:\n",
    "\t\treturn len(self.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4ef2a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se agrega la opción de que se tome la conexión directa entre la capa de embeddings y la capa de salida\n",
    "# y se agrega la opción de usar una matriz de embeddings pre-entrenados\n",
    "class NeuralLM(nn.Module):\n",
    "\tdef __init__(self, args, embedding_matrix=None, direct_emb_to_output=False):\n",
    "\t\tsuper(NeuralLM, self).__init__()\n",
    "\n",
    "\t\tself.window_size = args.N - 1\n",
    "\t\tself.embedding_size = args.d\n",
    "\t\tself.direct_emb_to_output = direct_emb_to_output\n",
    "\n",
    "\t\tself.emb = nn.Embedding(args.vocabulary_size, args.d)\n",
    "\n",
    "\t\tif embedding_matrix is not None:\n",
    "\t\t\tself.emb.load_state_dict({'weight': torch.Tensor(embedding_matrix)})\n",
    "\t\t\tself.emb.weight.requires_grad = False\n",
    "\n",
    "\t\tself.fc1 = nn.Linear(args.d * (args.N - 1), args.d_h)\n",
    "\t\tself.drop1 = nn.Dropout(p=args.dropout)\n",
    "\n",
    "\t\tif self.direct_emb_to_output:\n",
    "\t\t\tself.fc2 = nn.Linear(args.d_h + (args.d * (args.N - 1)), args.vocabulary_size, bias=False)\n",
    "\t\telse:\n",
    "\t\t\tself.fc2 = nn.Linear(args.d_h, args.vocabulary_size, bias=False)\n",
    "\n",
    "\t# x = lista de ids de las palabras en un n-grama\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.emb(x)\n",
    "\t\tx = x.view(-1, self.window_size * self.embedding_size)\n",
    "\n",
    "\t\th = F.relu(self.fc1(x))\n",
    "\t\th = self.drop1(h)\n",
    "\n",
    "\t\t# si se quiere que exista la conexión directa entre la capa de embeddings y la capa de salida\n",
    "\t\t# entonces se agrega a los argumentos que opera la capa de salida (fc2)\n",
    "\t\tif self.direct_emb_to_output:\n",
    "\t\t\th = torch.cat((x, h), dim=1)\n",
    "\t\treturn self.fc2(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f3a9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(raw_logits):\n",
    "\tprobs = F.softmax(raw_logits.detach(), dim=1)\n",
    "\ty_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "\treturn y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b10aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(data, model, gpu=False):\n",
    "\twith torch.no_grad():\n",
    "\t\tpreds, tgts = [], []\n",
    "\t\tfor window_words, labels in data:\n",
    "\t\t\tif gpu:\n",
    "\t\t\t\twindow_words = window_words.cuda()\n",
    "\n",
    "\t\t\toutputs = model(window_words)\n",
    "\n",
    "\t\t\ty_pred = get_preds(outputs)\n",
    "\n",
    "\t\t\ttgt = labels.numpy()\n",
    "\t\t\ttgts.append(tgt)\n",
    "\t\t\tpreds.append(y_pred)\n",
    "\n",
    "\ttgts  = [e for l in tgts  for e in l]\n",
    "\tpreds = [e for l in preds for e in l]\n",
    "\n",
    "\treturn accuracy_score(tgts, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e351aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint_path, filename=\"checkpoint.pt\"): \n",
    "\tfilename = os.path.join(checkpoint_path, filename)\n",
    "\ttorch.save(state, filename)\n",
    "\tif is_best:\n",
    "\t\tshutil.copyfile(filename, os.path.join(checkpoint_path, \"model_best.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16ec70",
   "metadata": {},
   "source": [
    "### 1. Con base en la implementación mostrada en clase, construya un modelo de lenguaje neuronal a nivel de carácter. Tomé en cuenta secuencias de tamaño 6 o más para el modelo, es decir hasta 5 caracteres o más en el contexto. Ponga al modelo a generar texto 3 veces, con un máximo de 300 caracteres. Escriba 5 ejemplos de oraciones y mídales el likelihood. Escriba un ejemplo de estructura morfológica (permutaciones con caracteres) similar al de estructura sintáctica del profesor con 5 o más caracteres de su gusto (e.g., \"ando \"). Calcule la perplejidad del modelo sobre los datos val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9b3acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 6\n",
    "\n",
    "# el \"tokenizer\" es la función list, así hacemos que tokenize por caracter a los strings\n",
    "ngram_data_chars = NgramData(args.N, 5000, list, numeric=False)\n",
    "ngram_data_chars.fit(x_train)\n",
    "\n",
    "x_ngram_train, y_ngram_train = ngram_data_chars.transform(x_train)\n",
    "x_ngram_val  , y_ngram_val   = ngram_data_chars.transform(x_val)\n",
    "\n",
    "args.batch_size = 64\n",
    "args.num_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d1f77a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(x_ngram_train, dtype=torch.int64), torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=args.num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "978b2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = TensorDataset(torch.tensor(x_ngram_val, dtype=torch.int64), torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, num_workers=args.num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e0f0441",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Model hyperparameters \n",
    "\n",
    "args.vocabulary_size = ngram_data_chars.get_vocabulary_size()\n",
    "args.d = 100\n",
    "args.d_h = 200\n",
    "args.dropout = 0.1\n",
    "\n",
    "# Train hyperparameters\n",
    "\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db8768c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.savedir = \"model_chars\"\n",
    "os.makedirs(args.savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b5f8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_chars = NeuralLM(args)\n",
    "\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "\tmodel_chars.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_chars.parameters(), lr=args.lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=args.lr_patience, verbose=True, factor=args.lr_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a65c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# esta parté sí la corrí, pero en un colab (porque toma muucho tiempo en mi computadora), sólo copié el archivo \n",
    "# \"best_model.pt\" (que generaba colab) a la carpeta correspondiente y seguí con lo demás\n",
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "\tepoch_start_time =  time.time()\n",
    "\tloss_epoch = []\n",
    "\ttraining_metric = []\n",
    "\tmodel_chars.train()\n",
    "\n",
    "\tfor window_words, labels in train_loader:\n",
    "\n",
    "\t\t# if gpu available\n",
    "\t\tif args.use_gpu:\n",
    "\t\t\twindow_words = window_words.cuda()\n",
    "\t\t\tlabels = labels.cuda()\n",
    "\n",
    "\t\t# forward pass\n",
    "\t\toutputs = model_chars(window_words)\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\tloss_epoch.append(loss.item())\n",
    "\n",
    "\t\t# get_training metrics\n",
    "\t\ty_pred = get_preds(outputs)\n",
    "\t\ttgt = labels.cpu().numpy()\n",
    "\t\ttraining_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "\t\t# posteriormente, hacemos el backward y optimizamos\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t# get metric n training dataset\n",
    "\tmean_epoch_metric = np.mean(training_metric)\n",
    "\ttrain_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "\t# get metric in validation dataset\n",
    "\tmodel_chars.eval()\n",
    "\ttuning_metric = model_eval(val_loader, model_chars, gpu=args.use_gpu)\n",
    "\tmetric_history.append(mean_epoch_metric)\n",
    "\n",
    "\t# update scheduler\n",
    "\tscheduler.step(tuning_metric)\n",
    "\n",
    "\t# chech for metric improvement\n",
    "\tis_improvement = tuning_metric > best_metric\n",
    "\tif is_improvement:\n",
    "\t\tvest_metric = tuning_metric\n",
    "\t\tn_no_improve = 0\n",
    "\telse:\n",
    "\t\tn_no_improve += 1\n",
    "\n",
    "\tsave_checkpoint(\n",
    "\t\t{\n",
    "\t\t\"epoch\" : epoch + 1, \n",
    "\t\t\"state_dict\" : model_chars.state_dict(), \n",
    "\t\t\"optimizer\" : optimizer.state_dict(),\n",
    "\t\t\"scheduler\" : scheduler.state_dict(), \n",
    "\t\t\"best_metric\" : best_metric\n",
    "\t\t}, \n",
    "\t\tis_improvement, \n",
    "\t\targs.savedir\n",
    "\t)\n",
    "\n",
    "\t# detener el modelo si no hay mejora\n",
    "\tif n_no_improve >= args.patience:\n",
    "\t\tprint(\"No improvement. Breaking out of loop\")\n",
    "\t\tbreak\n",
    "\n",
    "\tprint(\"Train acc: {}\".format(mean_epoch_metric))\n",
    "\tprint(\"Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time : {:.2f}\".format(epoch + 1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time() - epoch_start_time)))\n",
    "\n",
    "print(\"--- %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72e35f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text, tokenizer, ngram_data):\n",
    "\tall_tokens = [w.lower() if w in ngram_data.w2id else \"<unk>\" for w in tokenizer(text)]\n",
    "\ttoken_ids = [ngram_data.w2id[word.lower()] for word in all_tokens]\n",
    "\treturn all_tokens, token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8316be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_word(logits, temperature=1.0):\n",
    "\tlogits = np.asarray(logits).astype(\"float64\")\n",
    "\tpreds = logits / temperature\n",
    "\texp_preds = np.exp(preds)\n",
    "\tpreds = exp_preds / np.sum(exp_preds)\n",
    "\tprobas = np.random.multinomial(1, preds)\n",
    "\treturn np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbabecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(model, token_ids):\n",
    "\tword_ids_tensor = torch.LongTensor(token_ids).unsqueeze(0)\n",
    "\ty_raw_pred = model(word_ids_tensor).squeeze(0).detach().numpy()\n",
    "\t\n",
    "\ty_pred = sample_next_word(y_raw_pred, 1.0)\n",
    "\treturn y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c04e056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, initial_text, tokenizer, ngram_data, max_gen_tokens : int = 100, join_char : str = \" \"):\n",
    "\tall_tokens, window_word_ids = parse_text(initial_text, tokenizer, ngram_data)\n",
    "\tfor i in range(max_gen_tokens):\n",
    "\t\ty_pred = predict_next_token(model, window_word_ids)\n",
    "\t\tnext_word = ngram_data.id2w[y_pred]\n",
    "\t\tall_tokens.append(next_word)\n",
    "\n",
    "\t\tif next_word == \"</s>\":\n",
    "\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\twindow_word_ids.pop(0)\n",
    "\t\t\twindow_word_ids.append(y_pred)\n",
    "\n",
    "\treturn join_char.join(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34e8b8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(348, 100)\n",
       "  (fc1): Linear(in_features=500, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=348, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_chars = NeuralLM(args)\n",
    "best_model_chars.load_state_dict(torch.load(\"model_chars/model_best.pt\", map_location=torch.device(\"cpu\"))[\"state_dict\"])\n",
    "best_model_chars.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b771f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ando en la verga moso pero se quieren exá mi bendiendo sus una interischino de inundo tu plocura que le dos de inventy está soy <unk> (su vas a la madre de esté como una no mamar misótarde la pela y no haya pinche recercial empezar la cera fojos de bendilda<unk>  😐</s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"ando \"\n",
    "\n",
    "# el tokenizer es la función \"list\"\n",
    "print(generate_sentence(best_model_chars, initial_tokens, list, ngram_data_chars, max_gen_tokens=300, join_char=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba05303a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola del niños y una hdp<unk><unk></s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"hola \"\n",
    "\n",
    "# el tokenizer es la función \"list\" pues seguimos usando chars\n",
    "print(generate_sentence(best_model_chars, initial_tokens, list, ngram_data_chars, max_gen_tokens=300, join_char=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a377a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llevando para daba comería wala</s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"lleva\"\n",
    "\n",
    "# el tokenizer es la función \"list\" pues seguimos usando chars\n",
    "print(generate_sentence(best_model_chars, initial_tokens, list, ngram_data_chars, max_gen_tokens=300, join_char=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bcaa3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(model, text, ngram_data):\n",
    "\t# generate n-gram window from input text and the respective label y\n",
    "\tx, y = ngram_data.transform([text])\n",
    "\n",
    "\t# no tomar en cuenta los dos primeros n-gram windows pues son \"<s>\"\n",
    "\tx, y = x[2:], y[2:]\n",
    "\n",
    "\tx = torch.LongTensor(x).unsqueeze(0)\n",
    "\n",
    "\tlogits = model(x).detach()\n",
    "\tprobs = F.softmax(logits, dim=1).numpy()\n",
    "\n",
    "\treturn np.sum([np.log(probs[i][w]) for i, w in enumerate(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "316e31aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, text, ngram_data):\n",
    "\tlen_text = len(text)\n",
    "\tx, y = ngram_data.transform([text])\n",
    "\tx, y = x[2:], y[2:]\n",
    "\n",
    "\tx = torch.LongTensor(x).unsqueeze(0)\n",
    "\n",
    "\tlogits = model(x).detach()\n",
    "\n",
    "\tprobs = F.softmax(logits, dim=1).numpy()\n",
    "\n",
    "\tpartial_prod = 1\n",
    "\n",
    "\tfor i, w in enumerate(y):\n",
    "\t\tpartial_prod *= probs[i][w]\n",
    "\n",
    "\treturn Decimal(partial_prod) ** Decimal(1 / len_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "88e46282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -45.579407\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model_chars, \"clase de lenguage natural\", ngram_data_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "672c1660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -38.200916\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model_chars, \"messi es el mejor jugador\", ngram_data_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8ad47b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -46.30466\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model_chars, \"ronaldo es el mejor jugador\", ngram_data_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5880c1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -37.17421\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model_chars, \"amlo es un mal presidente\", ngram_data_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "851dd6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -43.010994\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model_chars, \"mexico le va a ganar a argentina\", ngram_data_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6a2df57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.891865  ->  rateros\n",
      "-11.700338  ->  ratores\n",
      "-11.899188  ->  rreatos\n",
      "-11.925676  ->  rroetas\n",
      "-11.952572  ->  sratero\n",
      "--------------------------------------------------\n",
      "-53.344097  ->  seoatrr\n",
      "-53.891838  ->  trsroea\n",
      "-54.5099  ->  trsreoa\n",
      "-55.720245  ->  trsroae\n",
      "-56.395157  ->  trsraoe\n"
     ]
    }
   ],
   "source": [
    "char_list = \"arresto\"\n",
    "perms = [\"\".join(perm) for perm in list(set(permutations(list(char_list))))]\n",
    "\n",
    "likelihood_word = [(log_likelihood(best_model_chars, text, ngram_data_chars), text) for text in perms]\n",
    "likelihood_word = sorted(likelihood_word, reverse=True)\n",
    "\n",
    "for likelihood, permutation in likelihood_word[:5]:\n",
    "\tprint(likelihood, \" -> \", permutation)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for likelihood, permutation in likelihood_word[-5:]:\n",
    "\tprint(likelihood, \" -> \", permutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ccdac9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P = 0\n"
     ]
    }
   ],
   "source": [
    "# Tengo un problema de underflow con la perplejidad, no lo puede solucionar, pero creo que lo hace bien\n",
    "print(\"P = {}\".format(perplexity(best_model_chars, \" \".join(x_val), ngram_data_chars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e1950e",
   "metadata": {},
   "source": [
    "### 2. Con base en la implementación mostrada en clase, construya un modelo de lenguaje neuronal a nivel de palabra, pero preinicializado con los embeddings proporcionados. Tomé en cuenta secuencias de tamaño 4 para el modelo, es decir hasta 3 palabras en el contexto. Después de haber entrenado el modelo, recupere las 10 palabras más similares a tres palabras de su gusto dadas. Ponga al modelo a generar texto a partir de tres secuencias de inicio de su gusto. Escriba 5 ejemplos de oraciones y mídales el likelihood. Proponga un ejemplo para ver estructuras sintácticas (permutaciones de palabras de alguna oración) buenas usando el likelihood a partir de una oración que usted proponga. Calcule la perplejidad del modelo sobre los datos val. Compárelo con la perplejidad del modelo de lenguaje sin embeddings preentrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c2d3532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "26b5ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos los embeddings del archivo\n",
    "embedding_words  = set()\n",
    "word_to_emb_id   = {} \n",
    "vector_embeddings = []\n",
    "\n",
    "with open('word2vec_col.txt', 'r') as f:\n",
    "\tnext(f) # ignoramos la primer línea \n",
    "\tfor index, line in enumerate(f):\n",
    "\t\ttokens = line.split()\n",
    "\t\tword = tokens[0]\n",
    "\t\tembedding_words.add(word)\n",
    "\t\tword_to_emb_id[word] = index\n",
    "\t\tword_embedding = np.array(tokens[1:]).astype(float)\n",
    "\t\tvector_embeddings.append(word_embedding)\n",
    "\n",
    "vector_embeddings = np.array(vector_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d24b38ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "args2 = Namespace()\n",
    "args2.N = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b8e41159",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_data_w2v = NgramData(args2.N, \n",
    "\t\t\t\t\t\t\t5000, \n",
    "\t\t\t\t\t\t\ttokenizer.tokenize, \n",
    "\t\t\t\t\t\t\tembeddings_model=vector_embeddings, \n",
    "\t\t\t\t\t\t\tembedding_words=embedding_words,\n",
    "\t\t\t\t\t\t\tword_to_emb_id=word_to_emb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dc524a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_data_w2v.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d84dbaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ngram_train, y_ngram_train = ngram_data_w2v.transform(x_train)\n",
    "x_ngram_val  , y_ngram_val   = ngram_data_w2v.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "030b70ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "args2.batch_size = 64\n",
    "args2.num_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d9955fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_w2v = TensorDataset(torch.tensor(x_ngram_train, dtype=torch.int64), torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "train_loader_w2v = DataLoader(train_dataset_w2v, batch_size=args2.batch_size, num_workers=args2.num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a6e3f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_w2v = TensorDataset(torch.tensor(x_ngram_val, dtype=torch.int64), torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "val_loader_w2v = DataLoader(val_dataset_w2v, batch_size=args2.batch_size, num_workers=args2.num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d7ab67f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "05139c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters \n",
    "\n",
    "args2.vocabulary_size = ngram_data_w2v.get_vocabulary_size()\n",
    "args2.d = 100\n",
    "args2.d_h = 200\n",
    "args2.dropout = 0.1\n",
    "\n",
    "# Train hyperparameters\n",
    "\n",
    "args2.lr = 2.3e-1\n",
    "args2.num_epochs = 100\n",
    "args2.patience = 20\n",
    "\n",
    "args2.lr_patience = 10\n",
    "args2.lr_factor = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "42ff5d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args2.savedir = \"model_w2v\"\n",
    "os.makedirs(args2.savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0a4f822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = NeuralLM(args2, embedding_matrix=ngram_data_w2v.embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6f334879",
   "metadata": {},
   "outputs": [],
   "source": [
    "args2.use_gpu = torch.cuda.is_available()\n",
    "if args2.use_gpu:\n",
    "\tmodel_w2v.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_w2v.parameters(), lr=args2.lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=args2.lr_patience, verbose=True, factor=args2.lr_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66e15d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.11865841806220095\n",
      "Epoch [1/100], Loss: 5.9706 - Val accuracy: 0.1691 - Epoch time : 13.97\n",
      "Train acc: 0.13365916566985647\n",
      "Epoch [2/100], Loss: 5.5267 - Val accuracy: 0.1762 - Epoch time : 13.89\n",
      "Train acc: 0.14068667763157894\n",
      "Epoch [3/100], Loss: 5.2930 - Val accuracy: 0.1792 - Epoch time : 13.79\n",
      "Train acc: 0.1450956937799043\n",
      "Epoch [4/100], Loss: 5.1120 - Val accuracy: 0.1571 - Epoch time : 14.04\n",
      "Train acc: 0.14803566088516745\n",
      "Epoch [5/100], Loss: 4.9449 - Val accuracy: 0.1317 - Epoch time : 13.96\n",
      "Train acc: 0.15063733552631578\n",
      "Epoch [6/100], Loss: 4.8028 - Val accuracy: 0.1528 - Epoch time : 13.93\n",
      "Train acc: 0.15492673444976077\n",
      "Epoch [7/100], Loss: 4.6760 - Val accuracy: 0.1408 - Epoch time : 13.72\n",
      "Train acc: 0.1594011662679426\n",
      "Epoch [8/100], Loss: 4.5592 - Val accuracy: 0.1764 - Epoch time : 14.24\n",
      "Train acc: 0.16508298444976077\n",
      "Epoch [9/100], Loss: 4.4591 - Val accuracy: 0.1240 - Epoch time : 13.97\n",
      "Train acc: 0.1712301883971292\n",
      "Epoch [10/100], Loss: 4.3756 - Val accuracy: 0.1241 - Epoch time : 13.96\n",
      "Train acc: 0.17708582535885167\n",
      "Epoch [11/100], Loss: 4.2985 - Val accuracy: 0.1246 - Epoch time : 13.99\n",
      "Train acc: 0.18104627691387562\n",
      "Epoch [12/100], Loss: 4.2308 - Val accuracy: 0.1766 - Epoch time : 14.11\n",
      "Train acc: 0.1872271232057416\n",
      "Epoch [13/100], Loss: 4.1806 - Val accuracy: 0.1611 - Epoch time : 14.24\n",
      "Train acc: 0.19436677631578947\n",
      "Epoch [14/100], Loss: 4.1190 - Val accuracy: 0.1258 - Epoch time : 13.86\n",
      "Train acc: 0.2003102571770335\n",
      "Epoch [15/100], Loss: 4.0690 - Val accuracy: 0.1139 - Epoch time : 13.97\n",
      "Train acc: 0.20498841208133972\n",
      "Epoch [16/100], Loss: 4.0187 - Val accuracy: 0.1220 - Epoch time : 13.98\n",
      "Train acc: 0.20802183014354067\n",
      "Epoch [17/100], Loss: 3.9815 - Val accuracy: 0.1535 - Epoch time : 13.94\n",
      "Train acc: 0.2143634120813397\n",
      "Epoch [18/100], Loss: 3.9486 - Val accuracy: 0.1700 - Epoch time : 13.96\n",
      "Train acc: 0.21719871411483255\n",
      "Epoch [19/100], Loss: 3.9193 - Val accuracy: 0.1603 - Epoch time : 13.85\n",
      "Train acc: 0.21987328050239235\n",
      "Epoch [20/100], Loss: 3.8917 - Val accuracy: 0.1540 - Epoch time : 13.90\n",
      "Train acc: 0.22355898624401913\n",
      "Epoch [21/100], Loss: 3.8611 - Val accuracy: 0.1238 - Epoch time : 13.93\n",
      "Train acc: 0.2271306818181818\n",
      "Epoch [22/100], Loss: 3.8363 - Val accuracy: 0.1721 - Epoch time : 14.00\n",
      "Train acc: 0.2299416866028708\n",
      "Epoch [23/100], Loss: 3.8154 - Val accuracy: 0.1435 - Epoch time : 14.01\n",
      "Train acc: 0.23345544258373205\n",
      "Epoch [24/100], Loss: 3.7925 - Val accuracy: 0.1741 - Epoch time : 13.95\n",
      "Train acc: 0.23202938098086123\n",
      "Epoch [25/100], Loss: 3.7850 - Val accuracy: 0.1223 - Epoch time : 14.24\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.1500e-01.\n",
      "Train acc: 0.23646456339712918\n",
      "Epoch [26/100], Loss: 3.7611 - Val accuracy: 0.1271 - Epoch time : 13.95\n",
      "Train acc: 0.2897727272727273\n",
      "Epoch [27/100], Loss: 3.3019 - Val accuracy: 0.1648 - Epoch time : 13.89\n",
      "Train acc: 0.2978076405502392\n",
      "Epoch [28/100], Loss: 3.2239 - Val accuracy: 0.1885 - Epoch time : 13.93\n",
      "Train acc: 0.3029885616028708\n",
      "Epoch [29/100], Loss: 3.2024 - Val accuracy: 0.1594 - Epoch time : 14.00\n",
      "Train acc: 0.3015251196172249\n",
      "Epoch [30/100], Loss: 3.1948 - Val accuracy: 0.1614 - Epoch time : 14.00\n",
      "Train acc: 0.30486692583732056\n",
      "Epoch [31/100], Loss: 3.1755 - Val accuracy: 0.1762 - Epoch time : 14.06\n",
      "Train acc: 0.30565004485645936\n",
      "Epoch [32/100], Loss: 3.1709 - Val accuracy: 0.1676 - Epoch time : 13.98\n",
      "Train acc: 0.3071377840909091\n",
      "Epoch [33/100], Loss: 3.1627 - Val accuracy: 0.1659 - Epoch time : 13.99\n",
      "Train acc: 0.3056201405502392\n",
      "Epoch [34/100], Loss: 3.1550 - Val accuracy: 0.1741 - Epoch time : 14.02\n",
      "Train acc: 0.3068275269138756\n",
      "Epoch [35/100], Loss: 3.1447 - Val accuracy: 0.1407 - Epoch time : 14.05\n",
      "Train acc: 0.30753588516746416\n",
      "Epoch [36/100], Loss: 3.1358 - Val accuracy: 0.1343 - Epoch time : 13.97\n",
      "Epoch 00037: reducing learning rate of group 0 to 5.7500e-02.\n",
      "Train acc: 0.31066836124401914\n",
      "Epoch [37/100], Loss: 3.1281 - Val accuracy: 0.1691 - Epoch time : 13.90\n",
      "Train acc: 0.3428173594497607\n",
      "Epoch [38/100], Loss: 2.9117 - Val accuracy: 0.1656 - Epoch time : 13.96\n",
      "Train acc: 0.3457834928229665\n",
      "Epoch [39/100], Loss: 2.8842 - Val accuracy: 0.1674 - Epoch time : 14.07\n",
      "Train acc: 0.34592180023923447\n",
      "Epoch [40/100], Loss: 2.8760 - Val accuracy: 0.1599 - Epoch time : 14.09\n",
      "Train acc: 0.3460731907894737\n",
      "Epoch [41/100], Loss: 2.8696 - Val accuracy: 0.1782 - Epoch time : 14.09\n",
      "Train acc: 0.3472936602870813\n",
      "Epoch [42/100], Loss: 2.8648 - Val accuracy: 0.1770 - Epoch time : 14.28\n",
      "Train acc: 0.3486823415071771\n",
      "Epoch [43/100], Loss: 2.8609 - Val accuracy: 0.1790 - Epoch time : 13.92\n",
      "Train acc: 0.34939069976076553\n",
      "Epoch [44/100], Loss: 2.8538 - Val accuracy: 0.1705 - Epoch time : 14.12\n",
      "Train acc: 0.349379485645933\n",
      "Epoch [45/100], Loss: 2.8508 - Val accuracy: 0.1649 - Epoch time : 14.06\n",
      "Train acc: 0.34875336423444975\n",
      "Epoch [46/100], Loss: 2.8517 - Val accuracy: 0.1771 - Epoch time : 13.93\n",
      "Train acc: 0.3491907147129186\n",
      "Epoch [47/100], Loss: 2.8488 - Val accuracy: 0.1888 - Epoch time : 14.21\n",
      "Epoch 00048: reducing learning rate of group 0 to 2.8750e-02.\n",
      "Train acc: 0.350390625\n",
      "Epoch [48/100], Loss: 2.8438 - Val accuracy: 0.1609 - Epoch time : 14.02\n",
      "Train acc: 0.3699928977272727\n",
      "Epoch [49/100], Loss: 2.7333 - Val accuracy: 0.1777 - Epoch time : 14.03\n",
      "Train acc: 0.37118906997607654\n",
      "Epoch [50/100], Loss: 2.7227 - Val accuracy: 0.1777 - Epoch time : 14.08\n",
      "Train acc: 0.3695275119617225\n",
      "Epoch [51/100], Loss: 2.7171 - Val accuracy: 0.1815 - Epoch time : 14.09\n",
      "Train acc: 0.37149932715311007\n",
      "Epoch [52/100], Loss: 2.7133 - Val accuracy: 0.1742 - Epoch time : 14.28\n",
      "Train acc: 0.37150867224880385\n",
      "Epoch [53/100], Loss: 2.7083 - Val accuracy: 0.1893 - Epoch time : 13.96\n",
      "Train acc: 0.371783418062201\n",
      "Epoch [54/100], Loss: 2.7094 - Val accuracy: 0.1850 - Epoch time : 14.15\n",
      "Train acc: 0.37432902212918656\n",
      "Epoch [55/100], Loss: 2.7005 - Val accuracy: 0.1911 - Epoch time : 14.20\n",
      "Train acc: 0.3715049342105263\n",
      "Epoch [56/100], Loss: 2.7066 - Val accuracy: 0.1914 - Epoch time : 14.23\n",
      "Train acc: 0.37185630980861245\n",
      "Epoch [57/100], Loss: 2.7028 - Val accuracy: 0.1789 - Epoch time : 14.01\n",
      "Train acc: 0.3741608104066985\n",
      "Epoch [58/100], Loss: 2.6936 - Val accuracy: 0.1804 - Epoch time : 14.00\n",
      "Epoch 00059: reducing learning rate of group 0 to 1.4375e-02.\n",
      "Train acc: 0.3743963068181818\n",
      "Epoch [59/100], Loss: 2.6968 - Val accuracy: 0.1830 - Epoch time : 14.15\n",
      "Train acc: 0.38431332236842103\n",
      "Epoch [60/100], Loss: 2.6402 - Val accuracy: 0.1870 - Epoch time : 13.96\n",
      "Train acc: 0.3862627093301435\n",
      "Epoch [61/100], Loss: 2.6317 - Val accuracy: 0.1822 - Epoch time : 14.04\n",
      "Train acc: 0.3852459629186603\n",
      "Epoch [62/100], Loss: 2.6355 - Val accuracy: 0.1843 - Epoch time : 14.02\n",
      "Train acc: 0.3856253738038278\n",
      "Epoch [63/100], Loss: 2.6313 - Val accuracy: 0.1803 - Epoch time : 14.01\n",
      "Train acc: 0.38529642643540674\n",
      "Epoch [64/100], Loss: 2.6306 - Val accuracy: 0.1812 - Epoch time : 14.03\n",
      "Train acc: 0.3862851375598086\n",
      "Epoch [65/100], Loss: 2.6296 - Val accuracy: 0.1779 - Epoch time : 14.04\n",
      "Train acc: 0.38675799940191385\n",
      "Epoch [66/100], Loss: 2.6264 - Val accuracy: 0.1811 - Epoch time : 14.11\n",
      "Train acc: 0.3858776913875598\n",
      "Epoch [67/100], Loss: 2.6270 - Val accuracy: 0.1824 - Epoch time : 14.00\n",
      "Train acc: 0.38727384868421055\n",
      "Epoch [68/100], Loss: 2.6264 - Val accuracy: 0.1838 - Epoch time : 13.98\n",
      "Train acc: 0.3853263307416268\n",
      "Epoch [69/100], Loss: 2.6305 - Val accuracy: 0.1769 - Epoch time : 13.91\n",
      "Epoch 00070: reducing learning rate of group 0 to 7.1875e-03.\n",
      "Train acc: 0.3867991178229665\n",
      "Epoch [70/100], Loss: 2.6211 - Val accuracy: 0.1791 - Epoch time : 14.17\n",
      "Train acc: 0.3929893092105263\n",
      "Epoch [71/100], Loss: 2.5932 - Val accuracy: 0.1836 - Epoch time : 14.16\n",
      "Train acc: 0.3926827900717703\n",
      "Epoch [72/100], Loss: 2.5923 - Val accuracy: 0.1882 - Epoch time : 14.11\n",
      "Train acc: 0.39292389354066987\n",
      "Epoch [73/100], Loss: 2.5910 - Val accuracy: 0.1885 - Epoch time : 13.86\n",
      "Train acc: 0.3927538127990431\n",
      "Epoch [74/100], Loss: 2.5896 - Val accuracy: 0.1912 - Epoch time : 14.08\n",
      "Train acc: 0.394761139354067\n",
      "Epoch [75/100], Loss: 2.5829 - Val accuracy: 0.1862 - Epoch time : 13.99\n",
      "Train acc: 0.3930435107655503\n",
      "Epoch [76/100], Loss: 2.5912 - Val accuracy: 0.1883 - Epoch time : 14.07\n",
      "Train acc: 0.394293884569378\n",
      "Epoch [77/100], Loss: 2.5879 - Val accuracy: 0.1859 - Epoch time : 14.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.3948938397129187\n",
      "Epoch [78/100], Loss: 2.5832 - Val accuracy: 0.1887 - Epoch time : 12.76\n",
      "Train acc: 0.39453872607655505\n",
      "Epoch [79/100], Loss: 2.5830 - Val accuracy: 0.1880 - Epoch time : 12.65\n",
      "Train acc: 0.39366589413875597\n",
      "Epoch [80/100], Loss: 2.5854 - Val accuracy: 0.1838 - Epoch time : 12.72\n",
      "Epoch 00081: reducing learning rate of group 0 to 3.5938e-03.\n",
      "Train acc: 0.3936434659090909\n",
      "Epoch [81/100], Loss: 2.5821 - Val accuracy: 0.1870 - Epoch time : 12.83\n",
      "Train acc: 0.3968376196172249\n",
      "Epoch [82/100], Loss: 2.5698 - Val accuracy: 0.1825 - Epoch time : 12.70\n",
      "Train acc: 0.397517942583732\n",
      "Epoch [83/100], Loss: 2.5692 - Val accuracy: 0.1843 - Epoch time : 12.76\n",
      "Train acc: 0.3983216208133971\n",
      "Epoch [84/100], Loss: 2.5663 - Val accuracy: 0.1839 - Epoch time : 12.81\n",
      "Train acc: 0.3977515699760765\n",
      "Epoch [85/100], Loss: 2.5689 - Val accuracy: 0.1891 - Epoch time : 12.70\n",
      "Train acc: 0.39917576255980863\n",
      "Epoch [86/100], Loss: 2.5668 - Val accuracy: 0.1856 - Epoch time : 12.89\n",
      "Train acc: 0.3970095693779904\n",
      "Epoch [87/100], Loss: 2.5687 - Val accuracy: 0.1868 - Epoch time : 12.71\n",
      "Train acc: 0.39764877392344494\n",
      "Epoch [88/100], Loss: 2.5637 - Val accuracy: 0.1869 - Epoch time : 13.12\n",
      "Train acc: 0.3975908343301435\n",
      "Epoch [89/100], Loss: 2.5689 - Val accuracy: 0.1872 - Epoch time : 12.75\n",
      "Train acc: 0.398123504784689\n",
      "Epoch [90/100], Loss: 2.5619 - Val accuracy: 0.1882 - Epoch time : 12.87\n",
      "Train acc: 0.39786931818181814\n",
      "Epoch [91/100], Loss: 2.5659 - Val accuracy: 0.1848 - Epoch time : 12.82\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.7969e-03.\n",
      "Train acc: 0.3977366178229665\n",
      "Epoch [92/100], Loss: 2.5645 - Val accuracy: 0.1885 - Epoch time : 12.80\n",
      "Train acc: 0.4004840759569378\n",
      "Epoch [93/100], Loss: 2.5523 - Val accuracy: 0.1839 - Epoch time : 12.70\n",
      "Train acc: 0.3989757775119617\n",
      "Epoch [94/100], Loss: 2.5593 - Val accuracy: 0.1858 - Epoch time : 12.86\n",
      "Train acc: 0.3996710526315789\n",
      "Epoch [95/100], Loss: 2.5545 - Val accuracy: 0.1873 - Epoch time : 12.95\n",
      "Train acc: 0.4004728618421053\n",
      "Epoch [96/100], Loss: 2.5545 - Val accuracy: 0.1909 - Epoch time : 12.89\n",
      "Train acc: 0.39934584330143535\n",
      "Epoch [97/100], Loss: 2.5553 - Val accuracy: 0.1866 - Epoch time : 12.79\n",
      "Train acc: 0.4008204994019139\n",
      "Epoch [98/100], Loss: 2.5509 - Val accuracy: 0.1854 - Epoch time : 12.89\n",
      "Train acc: 0.40170828349282295\n",
      "Epoch [99/100], Loss: 2.5508 - Val accuracy: 0.1895 - Epoch time : 12.86\n",
      "Train acc: 0.399923370215311\n",
      "Epoch [100/100], Loss: 2.5550 - Val accuracy: 0.1855 - Epoch time : 12.63\n",
      "--- 1374.5437705516815 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "for epoch in range(args2.num_epochs):\n",
    "\tepoch_start_time =  time.time()\n",
    "\tloss_epoch = []\n",
    "\ttraining_metric = []\n",
    "\tmodel_w2v.train()\n",
    "\n",
    "\tfor window_words, labels in train_loader_w2v:\n",
    "\n",
    "\t\t# if gpu available\n",
    "\t\tif args2.use_gpu:\n",
    "\t\t\twindow_words = window_words.cuda()\n",
    "\t\t\tlabels = labels.cuda()\n",
    "\n",
    "\t\t# forward pass\n",
    "\t\toutputs = model_w2v(window_words)\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\tloss_epoch.append(loss.item())\n",
    "\n",
    "\t\t# get_training metrics\n",
    "\t\ty_pred = get_preds(outputs)\n",
    "\t\ttgt = labels.cpu().numpy()\n",
    "\t\ttraining_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "\t\t# posteriormente, hacemos el backward y optimizamos\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t# get metric n training dataset\n",
    "\tmean_epoch_metric = np.mean(training_metric)\n",
    "\ttrain_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "\t# get metric in validation dataset\n",
    "\tmodel_w2v.eval()\n",
    "\ttuning_metric = model_eval(val_loader_w2v, model_w2v, gpu=args2.use_gpu)\n",
    "\tmetric_history.append(mean_epoch_metric)\n",
    "\n",
    "\t# update scheduler\n",
    "\tscheduler.step(tuning_metric)\n",
    "\n",
    "\t# chech for metric improvement\n",
    "\tis_improvement = tuning_metric > best_metric\n",
    "\tif is_improvement:\n",
    "\t\tvest_metric = tuning_metric\n",
    "\t\tn_no_improve = 0\n",
    "\telse:\n",
    "\t\tn_no_improve += 1\n",
    "\n",
    "\tsave_checkpoint(\n",
    "\t\t{\n",
    "\t\t\"epoch\" : epoch + 1, \n",
    "\t\t\"state_dict\" : model_w2v.state_dict(), \n",
    "\t\t\"optimizer\" : optimizer.state_dict(),\n",
    "\t\t\"scheduler\" : scheduler.state_dict(), \n",
    "\t\t\"best_metric\" : best_metric\n",
    "\t\t}, \n",
    "\t\tis_improvement, \n",
    "\t\targs2.savedir\n",
    "\t)\n",
    "\n",
    "\t# detener el modelo si no hay mejora\n",
    "\tif n_no_improve >= args2.patience:\n",
    "\t\tprint(\"No improvement. Breaking out of loop\")\n",
    "\t\tbreak\n",
    "\n",
    "\tprint(\"Train acc: {}\".format(mean_epoch_metric))\n",
    "\tprint(\"Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time : {:.2f}\".format(epoch + 1, args2.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time() - epoch_start_time)))\n",
    "\n",
    "print(\"--- %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "76659ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(5000, 100)\n",
       "  (fc1): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=5000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_w2v = NeuralLM(args2)\n",
    "best_model_w2v.load_state_dict(torch.load(\"model_w2v/model_best.pt\", map_location=torch.device(\"cpu\"))[\"state_dict\"])\n",
    "best_model_w2v.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "302eb27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> <s> porque <unk> <unk> no valgo de <unk> los <unk> <unk> <unk> lo vuelvo a subir pinche <unk> acá </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"<s> <s> <s>\"\n",
    "\n",
    "# el tokenizer es la función \"list\"\n",
    "print(generate_sentence(best_model_w2v, initial_tokens, tokenizer.tokenize, ngram_data_w2v, max_gen_tokens=100, join_char=\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a79c2ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> hola cuando <unk> <unk> pero como hoy un <unk> <unk> ” <unk> que holanda no le <unk> no quiero pedo con <unk> un <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"<s> <s> hola\"\n",
    "\n",
    "print(generate_sentence(best_model_w2v, initial_tokens, tokenizer.tokenize, ngram_data_w2v, max_gen_tokens=100, join_char=\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0a805304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -5.535452\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model_w2v, \"clase de lenguage natural\", ngram_data_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "24767efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -29.040985\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model_w2v, \"messi es el mejor jugador\", ngram_data_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "38caa27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -30.505259\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model_w2v, \"ronaldo es el mejor jugador\", ngram_data_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4f0e7751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -31.871273\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model_w2v, \"amlo es un mal presidente\", ngram_data_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bcd1a06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -52.662636\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model_w2v, \"mexico le va a ganar a argentina\", ngram_data_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f0b3551a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-29.469181 ganar le va a argentina a mexico\n",
      "-29.469181 ganar le va a argentina a mexico\n",
      "-29.807985 argentina ganar le va a a mexico\n",
      "-29.807985 argentina ganar le va a a mexico\n",
      "-33.405617 le ganar va a argentina a mexico\n",
      "--------------------------------------------------\n",
      "-124.671486 le a va mexico ganar a argentina\n",
      "-126.85434 le a va mexico argentina ganar a\n",
      "-126.85434 le a va mexico argentina ganar a\n",
      "-128.51643 le a va mexico ganar argentina a\n",
      "-128.51643 le a va mexico ganar argentina a\n"
     ]
    }
   ],
   "source": [
    "word_list = \"mexico le va a ganar a argentina\"\n",
    "perms = [\" \".join(perm) for perm in permutations(word_list.split(\" \"))]\n",
    "\n",
    "for p, t in sorted([(log_likelihood(best_model_w2v, text, ngram_data_w2v), text) for text in perms], reverse=True)[:5]:\n",
    "\tprint(p, t)\n",
    "\n",
    "    \n",
    "print(\"-\" * 50)\n",
    "\n",
    "for p, t in sorted([(log_likelihood(best_model_w2v, text, ngram_data_w2v), text) for text in perms], reverse=True)[-5:]:\n",
    "\tprint(p, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d8ead2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P = 0\n"
     ]
    }
   ],
   "source": [
    "# Tengo un problema de underflow con la perplejidad, no lo puede solucionar, pero creo que lo hace bien\n",
    "print(\"P = {}\".format(perplexity(best_model_w2v, \" \".join(x_val), ngram_data_w2v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401e0ea2",
   "metadata": {},
   "source": [
    "### 3 A partir del modelo anterior haga un modelo de lenguaje que integre una conexión directa de la capa de embeddings hacía la salida, justo como lo proponía Bengio. Discuta sobre las diferencias en el proceso de entrenamiento y la perplejidad respecto al modelo anterior y el visto en clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0fab172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args3 = Namespace()\n",
    "\n",
    "# (Hyper)parámetros\n",
    "\n",
    "args3.N               = args2.N\n",
    "args3.batch_size      = args2.batch_size\n",
    "args3.num_workers     = args2.num_workers\n",
    "args3.vocabulary_size = args2.vocabulary_size\n",
    "args3.d               = args2.d\n",
    "args3.d_h             = args2.d_h\n",
    "args3.dropout         = args2.dropout\n",
    "args3.lr              = args2.lr\n",
    "args3.num_epochs      = args2.num_epochs\n",
    "args3.patience        = args2.patience\n",
    "args3.lr_patience     = args2.lr_patience\n",
    "args3.lr_factor       = args2.lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "50d04899",
   "metadata": {},
   "outputs": [],
   "source": [
    "args3.savedir = \"model_w2v_with_direct_conxn\"\n",
    "os.makedirs(args3.savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "67fc3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_2 = NeuralLM(args3, embedding_matrix=ngram_data_w2v.embedding_matrix, direct_emb_to_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "70a81d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args3.use_gpu = torch.cuda.is_available()\n",
    "if args3.use_gpu:\n",
    "\tmodel_w2v_2.cuda()\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_w2v_2.parameters(), lr=args3.lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=args3.lr_patience, verbose=True, factor=args3.lr_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4473ddaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.0852833433014354\n",
      "Epoch [1/100], Loss: 8.7656 - Val accuracy: 0.0685 - Epoch time : 24.51\n",
      "Train acc: 0.09734973086124403\n",
      "Epoch [2/100], Loss: 8.2779 - Val accuracy: 0.0698 - Epoch time : 24.52\n",
      "Train acc: 0.11349992523923445\n",
      "Epoch [3/100], Loss: 7.9537 - Val accuracy: 0.1039 - Epoch time : 24.58\n",
      "Train acc: 0.13167052930622009\n",
      "Epoch [4/100], Loss: 7.7698 - Val accuracy: 0.0935 - Epoch time : 24.99\n",
      "Train acc: 0.14615542763157893\n",
      "Epoch [5/100], Loss: 7.5895 - Val accuracy: 0.0772 - Epoch time : 24.62\n",
      "Train acc: 0.15960115131578945\n",
      "Epoch [6/100], Loss: 7.4453 - Val accuracy: 0.0841 - Epoch time : 24.73\n",
      "Train acc: 0.16911072069377991\n",
      "Epoch [7/100], Loss: 7.3871 - Val accuracy: 0.1472 - Epoch time : 24.95\n",
      "Train acc: 0.18089675538277514\n",
      "Epoch [8/100], Loss: 7.3039 - Val accuracy: 0.0573 - Epoch time : 24.87\n",
      "Train acc: 0.1897203947368421\n",
      "Epoch [9/100], Loss: 7.1717 - Val accuracy: 0.0934 - Epoch time : 24.64\n",
      "Train acc: 0.19794781698564592\n",
      "Epoch [10/100], Loss: 7.1044 - Val accuracy: 0.0863 - Epoch time : 24.70\n",
      "Train acc: 0.20416230562200957\n",
      "Epoch [11/100], Loss: 7.0396 - Val accuracy: 0.1133 - Epoch time : 24.70\n",
      "Train acc: 0.2125336423444976\n",
      "Epoch [12/100], Loss: 6.9383 - Val accuracy: 0.0666 - Epoch time : 24.48\n",
      "Train acc: 0.2172267494019139\n",
      "Epoch [13/100], Loss: 6.9790 - Val accuracy: 0.1017 - Epoch time : 24.88\n",
      "Train acc: 0.2205293062200957\n",
      "Epoch [14/100], Loss: 6.9659 - Val accuracy: 0.0675 - Epoch time : 25.05\n",
      "Train acc: 0.2269998504784689\n",
      "Epoch [15/100], Loss: 6.8483 - Val accuracy: 0.0793 - Epoch time : 24.93\n",
      "Train acc: 0.23214712918660285\n",
      "Epoch [16/100], Loss: 6.8234 - Val accuracy: 0.0582 - Epoch time : 24.67\n",
      "Train acc: 0.23567770633971294\n",
      "Epoch [17/100], Loss: 6.8163 - Val accuracy: 0.0587 - Epoch time : 24.64\n",
      "Train acc: 0.24093338815789472\n",
      "Epoch [18/100], Loss: 6.8023 - Val accuracy: 0.0925 - Epoch time : 24.69\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.1500e-01.\n",
      "Train acc: 0.24426584928229667\n",
      "Epoch [19/100], Loss: 6.7373 - Val accuracy: 0.0777 - Epoch time : 25.06\n",
      "Train acc: 0.31969946172248803\n",
      "Epoch [20/100], Loss: 4.0290 - Val accuracy: 0.1171 - Epoch time : 24.91\n",
      "Train acc: 0.33216955741626797\n",
      "Epoch [21/100], Loss: 3.8183 - Val accuracy: 0.1038 - Epoch time : 24.89\n",
      "Train acc: 0.3383074162679426\n",
      "Epoch [22/100], Loss: 3.7553 - Val accuracy: 0.0843 - Epoch time : 24.91\n",
      "Train acc: 0.3367804276315789\n",
      "Epoch [23/100], Loss: 3.7500 - Val accuracy: 0.1297 - Epoch time : 24.88\n",
      "Train acc: 0.3408567583732058\n",
      "Epoch [24/100], Loss: 3.7169 - Val accuracy: 0.0953 - Epoch time : 24.94\n",
      "Train acc: 0.34143054724880384\n",
      "Epoch [25/100], Loss: 3.6966 - Val accuracy: 0.1370 - Epoch time : 24.74\n",
      "Train acc: 0.3442265998803828\n",
      "Epoch [26/100], Loss: 3.6928 - Val accuracy: 0.1043 - Epoch time : 25.09\n",
      "Train acc: 0.34572181519138756\n",
      "Epoch [27/100], Loss: 3.6678 - Val accuracy: 0.0822 - Epoch time : 24.76\n",
      "Train acc: 0.3462862589712919\n",
      "Epoch [28/100], Loss: 3.6779 - Val accuracy: 0.0978 - Epoch time : 25.11\n",
      "Train acc: 0.3483309659090909\n",
      "Epoch [29/100], Loss: 3.6477 - Val accuracy: 0.1013 - Epoch time : 25.33\n",
      "Epoch 00030: reducing learning rate of group 0 to 5.7500e-02.\n",
      "Train acc: 0.35107842404306216\n",
      "Epoch [30/100], Loss: 3.6248 - Val accuracy: 0.0936 - Epoch time : 24.53\n",
      "Train acc: 0.4092722039473684\n",
      "Epoch [31/100], Loss: 2.7531 - Val accuracy: 0.1325 - Epoch time : 24.83\n",
      "Train acc: 0.4142624850478469\n",
      "Epoch [32/100], Loss: 2.6893 - Val accuracy: 0.0977 - Epoch time : 24.60\n",
      "Train acc: 0.41678752990430623\n",
      "Epoch [33/100], Loss: 2.6661 - Val accuracy: 0.1115 - Epoch time : 24.94\n",
      "Train acc: 0.4164567135167464\n",
      "Epoch [34/100], Loss: 2.6546 - Val accuracy: 0.1567 - Epoch time : 24.79\n",
      "Train acc: 0.41745290071770336\n",
      "Epoch [35/100], Loss: 2.6474 - Val accuracy: 0.1668 - Epoch time : 24.93\n",
      "Train acc: 0.41849768241626795\n",
      "Epoch [36/100], Loss: 2.6442 - Val accuracy: 0.1242 - Epoch time : 24.85\n",
      "Train acc: 0.4197013307416268\n",
      "Epoch [37/100], Loss: 2.6339 - Val accuracy: 0.1315 - Epoch time : 25.42\n",
      "Train acc: 0.4199648624401914\n",
      "Epoch [38/100], Loss: 2.6348 - Val accuracy: 0.1169 - Epoch time : 26.61\n",
      "Train acc: 0.4212114982057416\n",
      "Epoch [39/100], Loss: 2.6284 - Val accuracy: 0.1115 - Epoch time : 25.53\n",
      "Train acc: 0.42034614234449763\n",
      "Epoch [40/100], Loss: 2.6322 - Val accuracy: 0.1441 - Epoch time : 25.16\n",
      "Epoch 00041: reducing learning rate of group 0 to 2.8750e-02.\n",
      "Train acc: 0.4220861991626794\n",
      "Epoch [41/100], Loss: 2.6158 - Val accuracy: 0.1170 - Epoch time : 25.20\n",
      "Train acc: 0.4610272129186603\n",
      "Epoch [42/100], Loss: 2.3316 - Val accuracy: 0.1380 - Epoch time : 24.85\n",
      "Train acc: 0.46145521830143543\n",
      "Epoch [43/100], Loss: 2.3129 - Val accuracy: 0.1530 - Epoch time : 25.11\n",
      "Train acc: 0.46166641746411485\n",
      "Epoch [44/100], Loss: 2.3082 - Val accuracy: 0.1553 - Epoch time : 24.96\n",
      "Train acc: 0.46433911483253587\n",
      "Epoch [45/100], Loss: 2.3011 - Val accuracy: 0.1255 - Epoch time : 25.22\n",
      "Train acc: 0.46444191088516745\n",
      "Epoch [46/100], Loss: 2.2949 - Val accuracy: 0.1333 - Epoch time : 25.03\n",
      "Train acc: 0.46442135167464116\n",
      "Epoch [47/100], Loss: 2.2948 - Val accuracy: 0.1473 - Epoch time : 24.92\n",
      "Train acc: 0.4655577153110048\n",
      "Epoch [48/100], Loss: 2.2939 - Val accuracy: 0.1412 - Epoch time : 24.94\n",
      "Train acc: 0.46537642045454547\n",
      "Epoch [49/100], Loss: 2.2935 - Val accuracy: 0.1477 - Epoch time : 25.04\n",
      "Train acc: 0.465157745215311\n",
      "Epoch [50/100], Loss: 2.2900 - Val accuracy: 0.1615 - Epoch time : 24.91\n",
      "Train acc: 0.46583433014354064\n",
      "Epoch [51/100], Loss: 2.2899 - Val accuracy: 0.1553 - Epoch time : 25.07\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.4375e-02.\n",
      "Train acc: 0.4640232505980861\n",
      "Epoch [52/100], Loss: 2.2847 - Val accuracy: 0.1528 - Epoch time : 24.84\n",
      "Train acc: 0.4859711423444976\n",
      "Epoch [53/100], Loss: 2.1783 - Val accuracy: 0.1420 - Epoch time : 24.70\n",
      "Train acc: 0.48725889653110044\n",
      "Epoch [54/100], Loss: 2.1690 - Val accuracy: 0.1529 - Epoch time : 25.21\n",
      "Train acc: 0.4883784389952153\n",
      "Epoch [55/100], Loss: 2.1667 - Val accuracy: 0.1512 - Epoch time : 25.17\n",
      "Train acc: 0.4889223235645933\n",
      "Epoch [56/100], Loss: 2.1644 - Val accuracy: 0.1626 - Epoch time : 24.81\n",
      "Train acc: 0.4898867374401914\n",
      "Epoch [57/100], Loss: 2.1614 - Val accuracy: 0.1598 - Epoch time : 24.84\n",
      "Train acc: 0.4886531848086125\n",
      "Epoch [58/100], Loss: 2.1619 - Val accuracy: 0.1566 - Epoch time : 24.92\n",
      "Train acc: 0.4898942135167464\n",
      "Epoch [59/100], Loss: 2.1615 - Val accuracy: 0.1471 - Epoch time : 24.84\n",
      "Train acc: 0.4897559061004785\n",
      "Epoch [60/100], Loss: 2.1603 - Val accuracy: 0.1511 - Epoch time : 25.02\n",
      "Train acc: 0.48827564294258374\n",
      "Epoch [61/100], Loss: 2.1605 - Val accuracy: 0.1450 - Epoch time : 24.57\n",
      "Train acc: 0.49036707535885166\n",
      "Epoch [62/100], Loss: 2.1557 - Val accuracy: 0.1572 - Epoch time : 24.61\n",
      "Epoch 00063: reducing learning rate of group 0 to 7.1875e-03.\n",
      "Train acc: 0.488886812200957\n",
      "Epoch [63/100], Loss: 2.1576 - Val accuracy: 0.1602 - Epoch time : 24.99\n",
      "Train acc: 0.49977384868421054\n",
      "Epoch [64/100], Loss: 2.1080 - Val accuracy: 0.1654 - Epoch time : 24.63\n",
      "Train acc: 0.5008485346889953\n",
      "Epoch [65/100], Loss: 2.1058 - Val accuracy: 0.1676 - Epoch time : 24.88\n",
      "Train acc: 0.5017400568181818\n",
      "Epoch [66/100], Loss: 2.1017 - Val accuracy: 0.1645 - Epoch time : 24.71\n",
      "Train acc: 0.5024297248803827\n",
      "Epoch [67/100], Loss: 2.1001 - Val accuracy: 0.1562 - Epoch time : 24.91\n",
      "Train acc: 0.5015213815789474\n",
      "Epoch [68/100], Loss: 2.1007 - Val accuracy: 0.1606 - Epoch time : 25.19\n",
      "Train acc: 0.501814817583732\n",
      "Epoch [69/100], Loss: 2.0977 - Val accuracy: 0.1636 - Epoch time : 25.16\n",
      "Train acc: 0.5019923744019138\n",
      "Epoch [70/100], Loss: 2.0966 - Val accuracy: 0.1644 - Epoch time : 25.29\n",
      "Train acc: 0.5018951854066986\n",
      "Epoch [71/100], Loss: 2.1006 - Val accuracy: 0.1694 - Epoch time : 24.74\n",
      "Train acc: 0.5013998953349282\n",
      "Epoch [72/100], Loss: 2.0948 - Val accuracy: 0.1647 - Epoch time : 26.00\n",
      "Train acc: 0.5021026465311005\n",
      "Epoch [73/100], Loss: 2.0981 - Val accuracy: 0.1672 - Epoch time : 25.16\n",
      "Epoch 00074: reducing learning rate of group 0 to 3.5938e-03.\n",
      "Train acc: 0.5017475328947368\n",
      "Epoch [74/100], Loss: 2.0968 - Val accuracy: 0.1632 - Epoch time : 24.93\n",
      "Train acc: 0.507728394138756\n",
      "Epoch [75/100], Loss: 2.0716 - Val accuracy: 0.1658 - Epoch time : 24.71\n",
      "Train acc: 0.5073975777511962\n",
      "Epoch [76/100], Loss: 2.0719 - Val accuracy: 0.1695 - Epoch time : 24.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.5084741327751197\n",
      "Epoch [77/100], Loss: 2.0722 - Val accuracy: 0.1693 - Epoch time : 24.96\n",
      "Train acc: 0.5069882625598087\n",
      "Epoch [78/100], Loss: 2.0703 - Val accuracy: 0.1565 - Epoch time : 24.98\n",
      "Train acc: 0.5107562051435407\n",
      "Epoch [79/100], Loss: 2.0656 - Val accuracy: 0.1668 - Epoch time : 25.05\n",
      "Train acc: 0.5091918361244019\n",
      "Epoch [80/100], Loss: 2.0684 - Val accuracy: 0.1652 - Epoch time : 24.72\n",
      "Train acc: 0.5082834928229665\n",
      "Epoch [81/100], Loss: 2.0675 - Val accuracy: 0.1631 - Epoch time : 24.94\n",
      "Train acc: 0.5082984449760766\n",
      "Epoch [82/100], Loss: 2.0698 - Val accuracy: 0.1650 - Epoch time : 25.10\n",
      "Train acc: 0.5095020933014355\n",
      "Epoch [83/100], Loss: 2.0679 - Val accuracy: 0.1602 - Epoch time : 25.00\n",
      "Train acc: 0.5082292912679426\n",
      "Epoch [84/100], Loss: 2.0699 - Val accuracy: 0.1683 - Epoch time : 25.11\n",
      "Epoch 00085: reducing learning rate of group 0 to 1.7969e-03.\n",
      "Train acc: 0.5081208881578947\n",
      "Epoch [85/100], Loss: 2.0679 - Val accuracy: 0.1714 - Epoch time : 24.70\n",
      "Train acc: 0.5124551435406699\n",
      "Epoch [86/100], Loss: 2.0558 - Val accuracy: 0.1670 - Epoch time : 25.52\n",
      "Train acc: 0.5125747607655502\n",
      "Epoch [87/100], Loss: 2.0539 - Val accuracy: 0.1662 - Epoch time : 24.93\n",
      "Train acc: 0.5136457087320574\n",
      "Epoch [88/100], Loss: 2.0518 - Val accuracy: 0.1672 - Epoch time : 24.99\n",
      "Train acc: 0.5118963068181818\n",
      "Epoch [89/100], Loss: 2.0542 - Val accuracy: 0.1684 - Epoch time : 25.11\n",
      "Train acc: 0.5125953199760765\n",
      "Epoch [90/100], Loss: 2.0512 - Val accuracy: 0.1673 - Epoch time : 25.08\n",
      "Train acc: 0.5108963815789473\n",
      "Epoch [91/100], Loss: 2.0546 - Val accuracy: 0.1672 - Epoch time : 24.75\n",
      "Train acc: 0.5125093450956938\n",
      "Epoch [92/100], Loss: 2.0539 - Val accuracy: 0.1664 - Epoch time : 25.43\n",
      "Train acc: 0.5122159090909091\n",
      "Epoch [93/100], Loss: 2.0523 - Val accuracy: 0.1691 - Epoch time : 25.13\n",
      "Train acc: 0.5117411782296651\n",
      "Epoch [94/100], Loss: 2.0537 - Val accuracy: 0.1692 - Epoch time : 24.88\n",
      "Train acc: 0.5116122159090909\n",
      "Epoch [95/100], Loss: 2.0557 - Val accuracy: 0.1694 - Epoch time : 24.99\n",
      "Epoch 00096: reducing learning rate of group 0 to 8.9844e-04.\n",
      "Train acc: 0.5124327153110048\n",
      "Epoch [96/100], Loss: 2.0517 - Val accuracy: 0.1665 - Epoch time : 24.94\n",
      "Train acc: 0.5147502990430622\n",
      "Epoch [97/100], Loss: 2.0459 - Val accuracy: 0.1678 - Epoch time : 25.22\n",
      "Train acc: 0.5136382326555025\n",
      "Epoch [98/100], Loss: 2.0448 - Val accuracy: 0.1663 - Epoch time : 24.94\n",
      "Train acc: 0.5128270783492823\n",
      "Epoch [99/100], Loss: 2.0487 - Val accuracy: 0.1680 - Epoch time : 25.13\n",
      "Train acc: 0.5144830293062201\n",
      "Epoch [100/100], Loss: 2.0442 - Val accuracy: 0.1649 - Epoch time : 25.89\n",
      "--- 2496.4375262260437 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "for epoch in range(args3.num_epochs):\n",
    "\tepoch_start_time = time.time()\n",
    "\tloss_epoch = []\n",
    "\ttraining_metric = []\n",
    "\tmodel_w2v_2.train()\n",
    "\n",
    "\tfor window_words, labels in train_loader_w2v:\n",
    "\n",
    "\t\t# if gpu available\n",
    "\t\tif args3.use_gpu:\n",
    "\t\t\twindow_words = window_words.cuda()\n",
    "\t\t\tlabels = labels.cuda()\n",
    "\n",
    "\t\t# forward pass\n",
    "\t\toutputs = model_w2v_2(window_words)\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\tloss_epoch.append(loss.item())\n",
    "\n",
    "\t\t# get_training metrics\n",
    "\t\ty_pred = get_preds(outputs)\n",
    "\t\ttgt = labels.cpu().numpy()\n",
    "\t\ttraining_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "\t\t# posteriormente, hacemos el backward y optimizamos\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t# get metric n training dataset\n",
    "\tmean_epoch_metric = np.mean(training_metric)\n",
    "\ttrain_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "\t# get metric in validation dataset\n",
    "\tmodel_w2v_2.eval()\n",
    "\ttuning_metric = model_eval(val_loader_w2v, model_w2v_2, gpu=args3.use_gpu)\n",
    "\tmetric_history.append(mean_epoch_metric)\n",
    "\n",
    "\t# update scheduler\n",
    "\tscheduler.step(tuning_metric)\n",
    "\n",
    "\t# chech for metric improvement\n",
    "\tis_improvement = tuning_metric > best_metric\n",
    "\tif is_improvement:\n",
    "\t\tvest_metric = tuning_metric\n",
    "\t\tn_no_improve = 0\n",
    "\telse:\n",
    "\t\tn_no_improve += 1\n",
    "\n",
    "\tsave_checkpoint(\n",
    "\t\t{\n",
    "\t\t\"epoch\" : epoch + 1, \n",
    "\t\t\"state_dict\" : model_w2v_2.state_dict(), \n",
    "\t\t\"optimizer\" : optimizer.state_dict(),\n",
    "\t\t\"scheduler\" : scheduler.state_dict(), \n",
    "\t\t\"best_metric\" : best_metric\n",
    "\t\t}, \n",
    "\t\tis_improvement, \n",
    "\t\targs3.savedir\n",
    "\t)\n",
    "\n",
    "\t# detener el modelo si no hay mejora\n",
    "\tif n_no_improve >= args3.patience:\n",
    "\t\tprint(\"No improvement. Breaking out of loop\")\n",
    "\t\tbreak\n",
    "\n",
    "\tprint(\"Train acc: {}\".format(mean_epoch_metric))\n",
    "\tprint(\"Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time : {:.2f}\".format(epoch + 1, args3.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time() - epoch_start_time)))\n",
    "\n",
    "print(\"--- %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f940aa9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(5000, 100)\n",
       "  (fc1): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=500, out_features=5000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_w2v_2 = NeuralLM(args3, direct_emb_to_output=True)\n",
    "best_model_w2v_2.load_state_dict(torch.load(\"model_w2v_with_direct_conxn/model_best.pt\", map_location=torch.device(\"cpu\"))[\"state_dict\"])\n",
    "best_model_w2v_2.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3dd96a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amlo es un pendejo maricón <unk> quien vergas hace raúl mejor … vas a ir a dormir un <unk> <unk> por qué pala ver a este par de semana era como de sopa de letras <unk> las ratas y putas como <unk> ☹ ️ </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"amlo es un\"\n",
    "\n",
    "# el tokenizer es la función \"list\"\n",
    "print(generate_sentence(best_model_w2v_2, initial_tokens, tokenizer.tokenize, ngram_data_w2v, max_gen_tokens=100, join_char=\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "01444a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P = 0\n"
     ]
    }
   ],
   "source": [
    "# Tengo un problema de underflow con la perplejidad, no lo puede solucionar, pero creo que lo hace bien\n",
    "print(\"P = {}\".format(perplexity(best_model_w2v_2, \" \".join(x_val), ngram_data_w2v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bcf2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe6e7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (main, Nov 24 2022, 19:45:47) [GCC 12.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
